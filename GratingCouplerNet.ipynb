{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"features.csv\"\n",
    "labels_path = \"labels.csv\"\n",
    "dataset_path = \"data.csv\"\n",
    "new_dataset = \"output_frame.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.input = nn.Linear(3, 100)\n",
    "        self.first_hidden = nn.Linear(100, 100)\n",
    "        self.second_hidden = nn.Linear(100, 100)\n",
    "        self.third_hidden = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x:[])->[]:\n",
    "        # 3 -> 2\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.first_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.second_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.third_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 2 -> 2\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    \n",
    "    def get_dataset(path_to_dataset:str)->pandas.DataFrame:\n",
    "        return pandas.read_csv(path_to_dataset, header=None)\n",
    "    \n",
    "    \n",
    "    def get_dataset_no_zeroes(dataframe:pandas.DataFrame)->pandas.DataFrame:\n",
    "        return dataframe[dataframe[0] != 0]\n",
    "    \n",
    "    \n",
    "    def get_features(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.Tensor(dataframe.iloc[:,[0,1,2]].values)\n",
    "    \n",
    "    \n",
    "    def get_labels(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.Tensor(dataframe.iloc[:,[3,4]].values)\n",
    "    \n",
    "    \n",
    "    def get_features_no_zeroes(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.tensor(dataframe[dataframe[0] != 0].iloc[:,[0,1,2]].values)\n",
    "    \n",
    "    \n",
    "    def get_labels_no_zeroes(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.tensor(dataframe[dataframe[0] != 0].iloc[:,[3, 4]].values)\n",
    "    \n",
    "\n",
    "def transform_features(tensor:torch.tensor)->torch.tensor:\n",
    "    return np.log10(tensor)\n",
    "\n",
    "\n",
    "def transform_labels(tensor:torch.tensor)->torch.tensor:\n",
    "    tensor[:,0] = np.log10(tensor[:,0])\n",
    "    tensor[:,1] = np.log10(-1*tensor[:,1])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def norm(tensor:torch.tensor)->torch.tensor:\n",
    "    return torch.nn.functional.normalize(tensor).float()\n",
    "    \n",
    "    \n",
    "def magnitude(vector:np.array)->float:\n",
    "    return np.linalg.norm(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.019457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>8.000000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.890000e-07</td>\n",
       "      <td>8.000000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.016767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.010203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.006660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "0    2.200000e-07  5.000000e-07  5.714324e-02  0.000002 -0.019457\n",
       "1    3.140000e-08  8.000000e-07  7.142883e-02  0.000002 -0.000399\n",
       "2    1.890000e-07  8.000000e-07  9.000000e-07  0.000002 -0.003692\n",
       "3    3.140000e-08  6.710000e-07  7.142883e-02  0.000002 -0.016767\n",
       "4    6.290000e-08  7.140000e-07  2.857207e-02  0.000002 -0.008122\n",
       "..            ...           ...           ...       ...       ...\n",
       "507  1.570000e-07  6.290000e-07  2.857207e-02  0.000002 -0.010203\n",
       "508  1.570000e-07  5.430000e-07  2.857207e-02  0.000002 -0.009285\n",
       "509  1.260000e-07  5.000000e-07  5.714324e-02  0.000002 -0.005189\n",
       "510  3.140000e-08  7.570000e-07  2.857207e-02  0.000002 -0.000702\n",
       "511  1.570000e-07  6.710000e-07  1.428649e-02  0.000002 -0.006660\n",
       "\n",
       "[448 rows x 5 columns]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataset from PATH\n",
    "dataset = Dataset.get_dataset_no_zeroes(Dataset.get_dataset(dataset_path))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.410000e-10</td>\n",
       "      <td>4.010000e-07</td>\n",
       "      <td>0.400601</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.740849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.820000e-10</td>\n",
       "      <td>4.010000e-07</td>\n",
       "      <td>0.401202</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.744594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.320000e-09</td>\n",
       "      <td>4.020000e-07</td>\n",
       "      <td>0.401804</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.744642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.760000e-09</td>\n",
       "      <td>4.020000e-07</td>\n",
       "      <td>0.402405</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.749721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.200000e-09</td>\n",
       "      <td>4.030000e-07</td>\n",
       "      <td>0.403006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.750035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2.180000e-07</td>\n",
       "      <td>6.980000e-07</td>\n",
       "      <td>0.697595</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.646077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2.190000e-07</td>\n",
       "      <td>6.980000e-07</td>\n",
       "      <td>0.698196</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.646254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2.190000e-07</td>\n",
       "      <td>6.990000e-07</td>\n",
       "      <td>0.698798</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.646192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>6.990000e-07</td>\n",
       "      <td>0.699399</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.646147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>7.000000e-07</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.645984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1         2         3         4\n",
       "1    4.410000e-10  4.010000e-07  0.400601  0.000002 -0.740849\n",
       "2    8.820000e-10  4.010000e-07  0.401202  0.000002 -0.744594\n",
       "3    1.320000e-09  4.020000e-07  0.401804  0.000002 -0.744642\n",
       "4    1.760000e-09  4.020000e-07  0.402405  0.000002 -0.749721\n",
       "5    2.200000e-09  4.030000e-07  0.403006  0.000002 -0.750035\n",
       "..            ...           ...       ...       ...       ...\n",
       "495  2.180000e-07  6.980000e-07  0.697595  0.000002 -0.646077\n",
       "496  2.190000e-07  6.980000e-07  0.698196  0.000002 -0.646254\n",
       "497  2.190000e-07  6.990000e-07  0.698798  0.000002 -0.646192\n",
       "498  2.200000e-07  6.990000e-07  0.699399  0.000002 -0.646147\n",
       "499  2.200000e-07  7.000000e-07  0.700000  0.000002 -0.645984\n",
       "\n",
       "[499 rows x 5 columns]"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = Dataset.get_dataset_no_zeroes(Dataset.get_dataset(new_dataset))\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1.890000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.004594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.002086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.035842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>9.430000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>4.285766e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.006933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.010930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.018188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "211  1.890000e-07  5.860000e-07  1.000000e-01  0.000002 -0.004594\n",
       "490  6.290000e-08  6.710000e-07  9.000000e-07  0.000002 -0.002086\n",
       "24   6.290000e-08  7.140000e-07  8.571441e-02  0.000002 -0.035842\n",
       "255  1.570000e-07  5.860000e-07  7.142883e-02  0.000002 -0.003743\n",
       "160  9.430000e-08  6.290000e-07  4.285766e-02  0.000002 -0.006933\n",
       "..            ...           ...           ...       ...       ...\n",
       "21   1.570000e-07  5.860000e-07  1.428649e-02  0.000002 -0.010930\n",
       "18   6.290000e-08  7.140000e-07  1.428649e-02  0.000002 -0.002851\n",
       "317  2.200000e-07  7.570000e-07  5.714324e-02  0.000002 -0.009943\n",
       "325  1.570000e-07  5.430000e-07  8.571441e-02  0.000002 -0.005662\n",
       "503  1.260000e-07  7.140000e-07  1.000000e-01  0.000002 -0.018188\n",
       "\n",
       "[381 rows x 5 columns]"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET THE TRAINING SET\n",
    "training_set = dataset.sample(frac = 0.85)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transform_features(Dataset.get_features(training_set))\n",
    "X_normed = norm(X)\n",
    "y = transform_labels(Dataset.get_labels(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.019457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.016767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>4.285766e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>9.430000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.053611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "0    2.200000e-07  5.000000e-07  5.714324e-02  0.000002 -0.019457\n",
       "3    3.140000e-08  6.710000e-07  7.142883e-02  0.000002 -0.016767\n",
       "4    6.290000e-08  7.140000e-07  2.857207e-02  0.000002 -0.008122\n",
       "15   2.200000e-07  5.430000e-07  5.714324e-02  0.000002 -0.005241\n",
       "17   1.260000e-07  7.140000e-07  1.428649e-02  0.000002 -0.009594\n",
       "..            ...           ...           ...       ...       ...\n",
       "456  1.260000e-07  6.710000e-07  4.285766e-02  0.000002 -0.008698\n",
       "468  3.140000e-08  7.570000e-07  8.571441e-02  0.000002 -0.000279\n",
       "475  9.430000e-08  6.290000e-07  2.857207e-02  0.000002 -0.005730\n",
       "486  6.290000e-08  6.290000e-07  8.571441e-02  0.000002 -0.053611\n",
       "505  1.570000e-07  7.140000e-07  9.000000e-07  0.000002 -0.003704\n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set = dataset.drop(training_set.index)\n",
    "testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_features(Dataset.get_features(testing_set))\n",
    "X_test_normed = norm(X_test)\n",
    "y_test = transform_labels(Dataset.get_labels(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 19.940201, Validation Loss: 20.190847\n",
      "Epoch: 2, Training Loss: 18.334122, Validation Loss: 18.573755\n",
      "Epoch: 3, Training Loss: 15.173356, Validation Loss: 15.385673\n",
      "Epoch: 4, Training Loss: 9.181097, Validation Loss: 9.321869\n",
      "Epoch: 5, Training Loss: 1.910532, Validation Loss: 1.871902\n",
      "Epoch: 6, Training Loss: 7.776403, Validation Loss: 7.395719\n",
      "Epoch: 7, Training Loss: 3.247936, Validation Loss: 3.007770\n",
      "Epoch: 8, Training Loss: 0.244991, Validation Loss: 0.186116\n",
      "Epoch: 9, Training Loss: 1.136817, Validation Loss: 1.195092\n",
      "Epoch: 10, Training Loss: 2.467627, Validation Loss: 2.587449\n",
      "Epoch: 11, Training Loss: 2.974193, Validation Loss: 3.124432\n",
      "Epoch: 12, Training Loss: 2.626080, Validation Loss: 2.788476\n",
      "Epoch: 13, Training Loss: 1.765317, Validation Loss: 1.925752\n",
      "Epoch: 14, Training Loss: 0.988313, Validation Loss: 1.132378\n",
      "Epoch: 15, Training Loss: 0.969561, Validation Loss: 1.082263\n",
      "Epoch: 16, Training Loss: 1.609118, Validation Loss: 1.681586\n",
      "Epoch: 17, Training Loss: 1.674791, Validation Loss: 1.713837\n",
      "Epoch: 18, Training Loss: 0.881215, Validation Loss: 0.903663\n",
      "Epoch: 19, Training Loss: 0.227172, Validation Loss: 0.245854\n",
      "Epoch: 20, Training Loss: 0.195878, Validation Loss: 0.214427\n",
      "Epoch: 21, Training Loss: 0.498959, Validation Loss: 0.514176\n",
      "Epoch: 22, Training Loss: 0.747192, Validation Loss: 0.752454\n",
      "Epoch: 23, Training Loss: 0.773063, Validation Loss: 0.760541\n",
      "Epoch: 24, Training Loss: 0.623173, Validation Loss: 0.585427\n",
      "Epoch: 25, Training Loss: 0.467302, Validation Loss: 0.399494\n",
      "Epoch: 26, Training Loss: 0.463575, Validation Loss: 0.366891\n",
      "Epoch: 27, Training Loss: 0.588571, Validation Loss: 0.473508\n",
      "Epoch: 28, Training Loss: 0.627560, Validation Loss: 0.512996\n",
      "Epoch: 29, Training Loss: 0.464027, Validation Loss: 0.369761\n",
      "Epoch: 30, Training Loss: 0.247226, Validation Loss: 0.185482\n",
      "Epoch: 31, Training Loss: 0.155573, Validation Loss: 0.128403\n",
      "Epoch: 32, Training Loss: 0.193199, Validation Loss: 0.195415\n",
      "Epoch: 33, Training Loss: 0.255851, Validation Loss: 0.279155\n",
      "Epoch: 34, Training Loss: 0.265492, Validation Loss: 0.301274\n",
      "Epoch: 35, Training Loss: 0.225356, Validation Loss: 0.266077\n",
      "Epoch: 36, Training Loss: 0.195500, Validation Loss: 0.235444\n",
      "Epoch: 37, Training Loss: 0.223040, Validation Loss: 0.258979\n",
      "Epoch: 38, Training Loss: 0.280751, Validation Loss: 0.312144\n",
      "Epoch: 39, Training Loss: 0.292857, Validation Loss: 0.321323\n",
      "Epoch: 40, Training Loss: 0.239838, Validation Loss: 0.267362\n",
      "Epoch: 41, Training Loss: 0.178383, Validation Loss: 0.206177\n",
      "Epoch: 42, Training Loss: 0.157194, Validation Loss: 0.184334\n",
      "Epoch: 43, Training Loss: 0.166318, Validation Loss: 0.190092\n",
      "Epoch: 44, Training Loss: 0.168783, Validation Loss: 0.185481\n",
      "Epoch: 45, Training Loss: 0.149248, Validation Loss: 0.155185\n",
      "Epoch: 46, Training Loss: 0.126450, Validation Loss: 0.119083\n",
      "Epoch: 47, Training Loss: 0.127173, Validation Loss: 0.106215\n",
      "Epoch: 48, Training Loss: 0.151221, Validation Loss: 0.119324\n",
      "Epoch: 49, Training Loss: 0.169639, Validation Loss: 0.131993\n",
      "Epoch: 50, Training Loss: 0.163268, Validation Loss: 0.125927\n",
      "Epoch: 51, Training Loss: 0.146189, Validation Loss: 0.113957\n",
      "Epoch: 52, Training Loss: 0.140136, Validation Loss: 0.115278\n",
      "Epoch: 53, Training Loss: 0.145016, Validation Loss: 0.127268\n",
      "Epoch: 54, Training Loss: 0.145339, Validation Loss: 0.132820\n",
      "Epoch: 55, Training Loss: 0.131631, Validation Loss: 0.121699\n",
      "Epoch: 56, Training Loss: 0.117658, Validation Loss: 0.107884\n",
      "Epoch: 57, Training Loss: 0.124476, Validation Loss: 0.113822\n",
      "Epoch: 58, Training Loss: 0.123919, Validation Loss: 0.117947\n",
      "Epoch: 59, Training Loss: 0.118994, Validation Loss: 0.119226\n",
      "Epoch: 60, Training Loss: 0.119476, Validation Loss: 0.124844\n",
      "Epoch: 61, Training Loss: 0.122286, Validation Loss: 0.130680\n",
      "Epoch: 62, Training Loss: 0.123785, Validation Loss: 0.133493\n",
      "Epoch: 63, Training Loss: 0.122995, Validation Loss: 0.132281\n",
      "Epoch: 64, Training Loss: 0.121246, Validation Loss: 0.128741\n",
      "Epoch: 65, Training Loss: 0.119971, Validation Loss: 0.124923\n",
      "Epoch: 66, Training Loss: 0.119026, Validation Loss: 0.121265\n",
      "Epoch: 67, Training Loss: 0.117520, Validation Loss: 0.117320\n",
      "Epoch: 68, Training Loss: 0.115730, Validation Loss: 0.113573\n",
      "Epoch: 69, Training Loss: 0.114979, Validation Loss: 0.111209\n",
      "Epoch: 70, Training Loss: 0.115551, Validation Loss: 0.110231\n",
      "Epoch: 71, Training Loss: 0.116388, Validation Loss: 0.109385\n",
      "Epoch: 72, Training Loss: 0.116718, Validation Loss: 0.107901\n",
      "Epoch: 73, Training Loss: 0.116862, Validation Loss: 0.106344\n",
      "Epoch: 74, Training Loss: 0.117250, Validation Loss: 0.105548\n",
      "Epoch: 75, Training Loss: 0.117498, Validation Loss: 0.105498\n",
      "Epoch: 76, Training Loss: 0.117060, Validation Loss: 0.105771\n",
      "Epoch: 77, Training Loss: 0.116231, Validation Loss: 0.106478\n",
      "Epoch: 78, Training Loss: 0.115686, Validation Loss: 0.107873\n",
      "Epoch: 79, Training Loss: 0.115490, Validation Loss: 0.109572\n",
      "Epoch: 80, Training Loss: 0.115289, Validation Loss: 0.110908\n",
      "Epoch: 81, Training Loss: 0.115068, Validation Loss: 0.111778\n",
      "Epoch: 82, Training Loss: 0.115132, Validation Loss: 0.112595\n",
      "Epoch: 83, Training Loss: 0.115478, Validation Loss: 0.113544\n",
      "Epoch: 84, Training Loss: 0.115727, Validation Loss: 0.114381\n",
      "Epoch: 85, Training Loss: 0.115711, Validation Loss: 0.114930\n",
      "Epoch: 86, Training Loss: 0.115633, Validation Loss: 0.115247\n",
      "Epoch: 87, Training Loss: 0.115606, Validation Loss: 0.115252\n",
      "Epoch: 88, Training Loss: 0.115493, Validation Loss: 0.114685\n",
      "Epoch: 89, Training Loss: 0.115242, Validation Loss: 0.113527\n",
      "Epoch: 90, Training Loss: 0.115032, Validation Loss: 0.112133\n",
      "Epoch: 91, Training Loss: 0.114991, Validation Loss: 0.110893\n",
      "Epoch: 92, Training Loss: 0.115027, Validation Loss: 0.109949\n",
      "Epoch: 93, Training Loss: 0.115038, Validation Loss: 0.109313\n",
      "Epoch: 94, Training Loss: 0.115067, Validation Loss: 0.108998\n",
      "Epoch: 95, Training Loss: 0.115148, Validation Loss: 0.108937\n",
      "Epoch: 96, Training Loss: 0.115210, Validation Loss: 0.108947\n",
      "Epoch: 97, Training Loss: 0.115181, Validation Loss: 0.108892\n",
      "Epoch: 98, Training Loss: 0.115100, Validation Loss: 0.108836\n",
      "Epoch: 99, Training Loss: 0.115049, Validation Loss: 0.108930\n",
      "Epoch: 100, Training Loss: 0.115015, Validation Loss: 0.109246\n",
      "Epoch: 101, Training Loss: 0.114960, Validation Loss: 0.109758\n",
      "Epoch: 102, Training Loss: 0.114919, Validation Loss: 0.110406\n",
      "Epoch: 103, Training Loss: 0.114930, Validation Loss: 0.111079\n",
      "Epoch: 104, Training Loss: 0.114965, Validation Loss: 0.111606\n",
      "Epoch: 105, Training Loss: 0.114978, Validation Loss: 0.111871\n",
      "Epoch: 106, Training Loss: 0.114978, Validation Loss: 0.111903\n",
      "Epoch: 107, Training Loss: 0.114986, Validation Loss: 0.111807\n",
      "Epoch: 108, Training Loss: 0.114987, Validation Loss: 0.111658\n",
      "Epoch: 109, Training Loss: 0.114963, Validation Loss: 0.111489\n",
      "Epoch: 110, Training Loss: 0.114932, Validation Loss: 0.111309\n",
      "Epoch: 111, Training Loss: 0.114917, Validation Loss: 0.111100\n",
      "Epoch: 112, Training Loss: 0.114909, Validation Loss: 0.110822\n",
      "Epoch: 113, Training Loss: 0.114901, Validation Loss: 0.110472\n",
      "Epoch: 114, Training Loss: 0.114902, Validation Loss: 0.110116\n",
      "Epoch: 115, Training Loss: 0.114915, Validation Loss: 0.109840\n",
      "Epoch: 116, Training Loss: 0.114924, Validation Loss: 0.109693\n",
      "Epoch: 117, Training Loss: 0.114921, Validation Loss: 0.109684\n",
      "Epoch: 118, Training Loss: 0.114915, Validation Loss: 0.109783\n",
      "Epoch: 119, Training Loss: 0.114910, Validation Loss: 0.109932\n",
      "Epoch: 120, Training Loss: 0.114902, Validation Loss: 0.110073\n",
      "Epoch: 121, Training Loss: 0.114891, Validation Loss: 0.110189\n",
      "Epoch: 122, Training Loss: 0.114885, Validation Loss: 0.110301\n",
      "Epoch: 123, Training Loss: 0.114885, Validation Loss: 0.110440\n",
      "Epoch: 124, Training Loss: 0.114885, Validation Loss: 0.110609\n",
      "Epoch: 125, Training Loss: 0.114884, Validation Loss: 0.110787\n",
      "Epoch: 126, Training Loss: 0.114886, Validation Loss: 0.110932\n",
      "Epoch: 127, Training Loss: 0.114888, Validation Loss: 0.110998\n",
      "Epoch: 128, Training Loss: 0.114885, Validation Loss: 0.110963\n",
      "Epoch: 129, Training Loss: 0.114879, Validation Loss: 0.110851\n",
      "Epoch: 130, Training Loss: 0.114876, Validation Loss: 0.110707\n",
      "Epoch: 131, Training Loss: 0.114872, Validation Loss: 0.110573\n",
      "Epoch: 132, Training Loss: 0.114869, Validation Loss: 0.110467\n",
      "Epoch: 133, Training Loss: 0.114867, Validation Loss: 0.110389\n",
      "Epoch: 134, Training Loss: 0.114867, Validation Loss: 0.110321\n",
      "Epoch: 135, Training Loss: 0.114866, Validation Loss: 0.110251\n",
      "Epoch: 136, Training Loss: 0.114865, Validation Loss: 0.110183\n",
      "Epoch: 137, Training Loss: 0.114864, Validation Loss: 0.110138\n",
      "Epoch: 138, Training Loss: 0.114862, Validation Loss: 0.110136\n",
      "Epoch: 139, Training Loss: 0.114859, Validation Loss: 0.110181\n",
      "Epoch: 140, Training Loss: 0.114856, Validation Loss: 0.110259\n",
      "Epoch: 141, Training Loss: 0.114854, Validation Loss: 0.110346\n",
      "Epoch: 142, Training Loss: 0.114853, Validation Loss: 0.110418\n",
      "Epoch: 143, Training Loss: 0.114851, Validation Loss: 0.110465\n",
      "Epoch: 144, Training Loss: 0.114849, Validation Loss: 0.110496\n",
      "Epoch: 145, Training Loss: 0.114849, Validation Loss: 0.110523\n",
      "Epoch: 146, Training Loss: 0.114847, Validation Loss: 0.110549\n",
      "Epoch: 147, Training Loss: 0.114845, Validation Loss: 0.110566\n",
      "Epoch: 148, Training Loss: 0.114844, Validation Loss: 0.110562\n",
      "Epoch: 149, Training Loss: 0.114842, Validation Loss: 0.110529\n",
      "Epoch: 150, Training Loss: 0.114839, Validation Loss: 0.110471\n",
      "Epoch: 151, Training Loss: 0.114838, Validation Loss: 0.110406\n",
      "Epoch: 152, Training Loss: 0.114836, Validation Loss: 0.110350\n",
      "Epoch: 153, Training Loss: 0.114834, Validation Loss: 0.110312\n",
      "Epoch: 154, Training Loss: 0.114833, Validation Loss: 0.110291\n",
      "Epoch: 155, Training Loss: 0.114831, Validation Loss: 0.110281\n",
      "Epoch: 156, Training Loss: 0.114830, Validation Loss: 0.110276\n",
      "Epoch: 157, Training Loss: 0.114828, Validation Loss: 0.110272\n",
      "Epoch: 158, Training Loss: 0.114826, Validation Loss: 0.110275\n",
      "Epoch: 159, Training Loss: 0.114824, Validation Loss: 0.110288\n",
      "Epoch: 160, Training Loss: 0.114823, Validation Loss: 0.110313\n",
      "Epoch: 161, Training Loss: 0.114821, Validation Loss: 0.110341\n",
      "Epoch: 162, Training Loss: 0.114819, Validation Loss: 0.110366\n",
      "Epoch: 163, Training Loss: 0.114817, Validation Loss: 0.110385\n",
      "Epoch: 164, Training Loss: 0.114816, Validation Loss: 0.110395\n",
      "Epoch: 165, Training Loss: 0.114814, Validation Loss: 0.110396\n",
      "Epoch: 166, Training Loss: 0.114812, Validation Loss: 0.110391\n",
      "Epoch: 167, Training Loss: 0.114811, Validation Loss: 0.110379\n",
      "Epoch: 168, Training Loss: 0.114809, Validation Loss: 0.110361\n",
      "Epoch: 169, Training Loss: 0.114807, Validation Loss: 0.110339\n",
      "Epoch: 170, Training Loss: 0.114805, Validation Loss: 0.110316\n",
      "Epoch: 171, Training Loss: 0.114804, Validation Loss: 0.110294\n",
      "Epoch: 172, Training Loss: 0.114802, Validation Loss: 0.110278\n",
      "Epoch: 173, Training Loss: 0.114800, Validation Loss: 0.110269\n",
      "Epoch: 174, Training Loss: 0.114798, Validation Loss: 0.110265\n",
      "Epoch: 175, Training Loss: 0.114797, Validation Loss: 0.110262\n",
      "Epoch: 176, Training Loss: 0.114795, Validation Loss: 0.110259\n",
      "Epoch: 177, Training Loss: 0.114793, Validation Loss: 0.110256\n",
      "Epoch: 178, Training Loss: 0.114791, Validation Loss: 0.110261\n",
      "Epoch: 179, Training Loss: 0.114789, Validation Loss: 0.110272\n",
      "Epoch: 180, Training Loss: 0.114787, Validation Loss: 0.110283\n",
      "Epoch: 181, Training Loss: 0.114786, Validation Loss: 0.110292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182, Training Loss: 0.114784, Validation Loss: 0.110296\n",
      "Epoch: 183, Training Loss: 0.114782, Validation Loss: 0.110293\n",
      "Epoch: 184, Training Loss: 0.114780, Validation Loss: 0.110281\n",
      "Epoch: 185, Training Loss: 0.114778, Validation Loss: 0.110273\n",
      "Epoch: 186, Training Loss: 0.114776, Validation Loss: 0.110267\n",
      "Epoch: 187, Training Loss: 0.114775, Validation Loss: 0.110259\n",
      "Epoch: 188, Training Loss: 0.114773, Validation Loss: 0.110249\n",
      "Epoch: 189, Training Loss: 0.114771, Validation Loss: 0.110240\n",
      "Epoch: 190, Training Loss: 0.114769, Validation Loss: 0.110233\n",
      "Epoch: 191, Training Loss: 0.114767, Validation Loss: 0.110224\n",
      "Epoch: 192, Training Loss: 0.114765, Validation Loss: 0.110213\n",
      "Epoch: 193, Training Loss: 0.114763, Validation Loss: 0.110205\n",
      "Epoch: 194, Training Loss: 0.114762, Validation Loss: 0.110207\n",
      "Epoch: 195, Training Loss: 0.114760, Validation Loss: 0.110215\n",
      "Epoch: 196, Training Loss: 0.114758, Validation Loss: 0.110218\n",
      "Epoch: 197, Training Loss: 0.114756, Validation Loss: 0.110214\n",
      "Epoch: 198, Training Loss: 0.114754, Validation Loss: 0.110207\n",
      "Epoch: 199, Training Loss: 0.114752, Validation Loss: 0.110206\n",
      "Epoch: 200, Training Loss: 0.114750, Validation Loss: 0.110211\n",
      "Epoch: 201, Training Loss: 0.114749, Validation Loss: 0.110216\n",
      "Epoch: 202, Training Loss: 0.114747, Validation Loss: 0.110209\n",
      "Epoch: 203, Training Loss: 0.114745, Validation Loss: 0.110194\n",
      "Epoch: 204, Training Loss: 0.114743, Validation Loss: 0.110180\n",
      "Epoch: 205, Training Loss: 0.114741, Validation Loss: 0.110172\n",
      "Epoch: 206, Training Loss: 0.114739, Validation Loss: 0.110176\n",
      "Epoch: 207, Training Loss: 0.114737, Validation Loss: 0.110182\n",
      "Epoch: 208, Training Loss: 0.114735, Validation Loss: 0.110175\n",
      "Epoch: 209, Training Loss: 0.114733, Validation Loss: 0.110157\n",
      "Epoch: 210, Training Loss: 0.114732, Validation Loss: 0.110140\n",
      "Epoch: 211, Training Loss: 0.114730, Validation Loss: 0.110140\n",
      "Epoch: 212, Training Loss: 0.114728, Validation Loss: 0.110151\n",
      "Epoch: 213, Training Loss: 0.114726, Validation Loss: 0.110156\n",
      "Epoch: 214, Training Loss: 0.114724, Validation Loss: 0.110150\n",
      "Epoch: 215, Training Loss: 0.114722, Validation Loss: 0.110136\n",
      "Epoch: 216, Training Loss: 0.114720, Validation Loss: 0.110124\n",
      "Epoch: 217, Training Loss: 0.114718, Validation Loss: 0.110123\n",
      "Epoch: 218, Training Loss: 0.114716, Validation Loss: 0.110133\n",
      "Epoch: 219, Training Loss: 0.114714, Validation Loss: 0.110136\n",
      "Epoch: 220, Training Loss: 0.114712, Validation Loss: 0.110126\n",
      "Epoch: 221, Training Loss: 0.114710, Validation Loss: 0.110109\n",
      "Epoch: 222, Training Loss: 0.114708, Validation Loss: 0.110097\n",
      "Epoch: 223, Training Loss: 0.114706, Validation Loss: 0.110099\n",
      "Epoch: 224, Training Loss: 0.114704, Validation Loss: 0.110101\n",
      "Epoch: 225, Training Loss: 0.114702, Validation Loss: 0.110096\n",
      "Epoch: 226, Training Loss: 0.114700, Validation Loss: 0.110087\n",
      "Epoch: 227, Training Loss: 0.114698, Validation Loss: 0.110077\n",
      "Epoch: 228, Training Loss: 0.114696, Validation Loss: 0.110070\n",
      "Epoch: 229, Training Loss: 0.114694, Validation Loss: 0.110071\n",
      "Epoch: 230, Training Loss: 0.114692, Validation Loss: 0.110074\n",
      "Epoch: 231, Training Loss: 0.114690, Validation Loss: 0.110074\n",
      "Epoch: 232, Training Loss: 0.114688, Validation Loss: 0.110065\n",
      "Epoch: 233, Training Loss: 0.114686, Validation Loss: 0.110052\n",
      "Epoch: 234, Training Loss: 0.114684, Validation Loss: 0.110045\n",
      "Epoch: 235, Training Loss: 0.114682, Validation Loss: 0.110041\n",
      "Epoch: 236, Training Loss: 0.114680, Validation Loss: 0.110043\n",
      "Epoch: 237, Training Loss: 0.114678, Validation Loss: 0.110044\n",
      "Epoch: 238, Training Loss: 0.114676, Validation Loss: 0.110038\n",
      "Epoch: 239, Training Loss: 0.114674, Validation Loss: 0.110030\n",
      "Epoch: 240, Training Loss: 0.114672, Validation Loss: 0.110022\n",
      "Epoch: 241, Training Loss: 0.114670, Validation Loss: 0.110014\n",
      "Epoch: 242, Training Loss: 0.114668, Validation Loss: 0.110008\n",
      "Epoch: 243, Training Loss: 0.114666, Validation Loss: 0.110004\n",
      "Epoch: 244, Training Loss: 0.114664, Validation Loss: 0.110001\n",
      "Epoch: 245, Training Loss: 0.114661, Validation Loss: 0.109997\n",
      "Epoch: 246, Training Loss: 0.114659, Validation Loss: 0.109995\n",
      "Epoch: 247, Training Loss: 0.114657, Validation Loss: 0.109990\n",
      "Epoch: 248, Training Loss: 0.114655, Validation Loss: 0.109986\n",
      "Epoch: 249, Training Loss: 0.114653, Validation Loss: 0.109980\n",
      "Epoch: 250, Training Loss: 0.114651, Validation Loss: 0.109972\n",
      "Epoch: 251, Training Loss: 0.114649, Validation Loss: 0.109966\n",
      "Epoch: 252, Training Loss: 0.114647, Validation Loss: 0.109962\n",
      "Epoch: 253, Training Loss: 0.114644, Validation Loss: 0.109959\n",
      "Epoch: 254, Training Loss: 0.114642, Validation Loss: 0.109955\n",
      "Epoch: 255, Training Loss: 0.114640, Validation Loss: 0.109949\n",
      "Epoch: 256, Training Loss: 0.114638, Validation Loss: 0.109943\n",
      "Epoch: 257, Training Loss: 0.114636, Validation Loss: 0.109940\n",
      "Epoch: 258, Training Loss: 0.114634, Validation Loss: 0.109935\n",
      "Epoch: 259, Training Loss: 0.114631, Validation Loss: 0.109927\n",
      "Epoch: 260, Training Loss: 0.114629, Validation Loss: 0.109920\n",
      "Epoch: 261, Training Loss: 0.114627, Validation Loss: 0.109914\n",
      "Epoch: 262, Training Loss: 0.114625, Validation Loss: 0.109911\n",
      "Epoch: 263, Training Loss: 0.114623, Validation Loss: 0.109908\n",
      "Epoch: 264, Training Loss: 0.114620, Validation Loss: 0.109907\n",
      "Epoch: 265, Training Loss: 0.114618, Validation Loss: 0.109900\n",
      "Epoch: 266, Training Loss: 0.114616, Validation Loss: 0.109890\n",
      "Epoch: 267, Training Loss: 0.114614, Validation Loss: 0.109882\n",
      "Epoch: 268, Training Loss: 0.114611, Validation Loss: 0.109879\n",
      "Epoch: 269, Training Loss: 0.114609, Validation Loss: 0.109877\n",
      "Epoch: 270, Training Loss: 0.114607, Validation Loss: 0.109873\n",
      "Epoch: 271, Training Loss: 0.114605, Validation Loss: 0.109867\n",
      "Epoch: 272, Training Loss: 0.114602, Validation Loss: 0.109859\n",
      "Epoch: 273, Training Loss: 0.114600, Validation Loss: 0.109855\n",
      "Epoch: 274, Training Loss: 0.114598, Validation Loss: 0.109851\n",
      "Epoch: 275, Training Loss: 0.114595, Validation Loss: 0.109844\n",
      "Epoch: 276, Training Loss: 0.114593, Validation Loss: 0.109837\n",
      "Epoch: 277, Training Loss: 0.114591, Validation Loss: 0.109831\n",
      "Epoch: 278, Training Loss: 0.114589, Validation Loss: 0.109827\n",
      "Epoch: 279, Training Loss: 0.114586, Validation Loss: 0.109824\n",
      "Epoch: 280, Training Loss: 0.114584, Validation Loss: 0.109819\n",
      "Epoch: 281, Training Loss: 0.114582, Validation Loss: 0.109812\n",
      "Epoch: 282, Training Loss: 0.114579, Validation Loss: 0.109805\n",
      "Epoch: 283, Training Loss: 0.114577, Validation Loss: 0.109799\n",
      "Epoch: 284, Training Loss: 0.114575, Validation Loss: 0.109795\n",
      "Epoch: 285, Training Loss: 0.114572, Validation Loss: 0.109791\n",
      "Epoch: 286, Training Loss: 0.114570, Validation Loss: 0.109786\n",
      "Epoch: 287, Training Loss: 0.114568, Validation Loss: 0.109780\n",
      "Epoch: 288, Training Loss: 0.114565, Validation Loss: 0.109774\n",
      "Epoch: 289, Training Loss: 0.114563, Validation Loss: 0.109768\n",
      "Epoch: 290, Training Loss: 0.114560, Validation Loss: 0.109757\n",
      "Epoch: 291, Training Loss: 0.114558, Validation Loss: 0.109753\n",
      "Epoch: 292, Training Loss: 0.114556, Validation Loss: 0.109753\n",
      "Epoch: 293, Training Loss: 0.114553, Validation Loss: 0.109752\n",
      "Epoch: 294, Training Loss: 0.114551, Validation Loss: 0.109745\n",
      "Epoch: 295, Training Loss: 0.114549, Validation Loss: 0.109728\n",
      "Epoch: 296, Training Loss: 0.114546, Validation Loss: 0.109720\n",
      "Epoch: 297, Training Loss: 0.114544, Validation Loss: 0.109721\n",
      "Epoch: 298, Training Loss: 0.114541, Validation Loss: 0.109724\n",
      "Epoch: 299, Training Loss: 0.114539, Validation Loss: 0.109720\n",
      "Epoch: 300, Training Loss: 0.114536, Validation Loss: 0.109708\n",
      "Epoch: 301, Training Loss: 0.114534, Validation Loss: 0.109695\n",
      "Epoch: 302, Training Loss: 0.114532, Validation Loss: 0.109687\n",
      "Epoch: 303, Training Loss: 0.114529, Validation Loss: 0.109686\n",
      "Epoch: 304, Training Loss: 0.114527, Validation Loss: 0.109683\n",
      "Epoch: 305, Training Loss: 0.114524, Validation Loss: 0.109678\n",
      "Epoch: 306, Training Loss: 0.114522, Validation Loss: 0.109672\n",
      "Epoch: 307, Training Loss: 0.114519, Validation Loss: 0.109669\n",
      "Epoch: 308, Training Loss: 0.114517, Validation Loss: 0.109661\n",
      "Epoch: 309, Training Loss: 0.114514, Validation Loss: 0.109652\n",
      "Epoch: 310, Training Loss: 0.114512, Validation Loss: 0.109644\n",
      "Epoch: 311, Training Loss: 0.114509, Validation Loss: 0.109636\n",
      "Epoch: 312, Training Loss: 0.114507, Validation Loss: 0.109634\n",
      "Epoch: 313, Training Loss: 0.114504, Validation Loss: 0.109633\n",
      "Epoch: 314, Training Loss: 0.114502, Validation Loss: 0.109629\n",
      "Epoch: 315, Training Loss: 0.114499, Validation Loss: 0.109619\n",
      "Epoch: 316, Training Loss: 0.114497, Validation Loss: 0.109609\n",
      "Epoch: 317, Training Loss: 0.114494, Validation Loss: 0.109598\n",
      "Epoch: 318, Training Loss: 0.114492, Validation Loss: 0.109596\n",
      "Epoch: 319, Training Loss: 0.114489, Validation Loss: 0.109598\n",
      "Epoch: 320, Training Loss: 0.114487, Validation Loss: 0.109595\n",
      "Epoch: 321, Training Loss: 0.114484, Validation Loss: 0.109579\n",
      "Epoch: 322, Training Loss: 0.114482, Validation Loss: 0.109567\n",
      "Epoch: 323, Training Loss: 0.114479, Validation Loss: 0.109570\n",
      "Epoch: 324, Training Loss: 0.114476, Validation Loss: 0.109565\n",
      "Epoch: 325, Training Loss: 0.114474, Validation Loss: 0.109553\n",
      "Epoch: 326, Training Loss: 0.114471, Validation Loss: 0.109551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327, Training Loss: 0.114469, Validation Loss: 0.109543\n",
      "Epoch: 328, Training Loss: 0.114466, Validation Loss: 0.109542\n",
      "Epoch: 329, Training Loss: 0.114463, Validation Loss: 0.109532\n",
      "Epoch: 330, Training Loss: 0.114461, Validation Loss: 0.109517\n",
      "Epoch: 331, Training Loss: 0.114458, Validation Loss: 0.109522\n",
      "Epoch: 332, Training Loss: 0.114456, Validation Loss: 0.109518\n",
      "Epoch: 333, Training Loss: 0.114453, Validation Loss: 0.109505\n",
      "Epoch: 334, Training Loss: 0.114450, Validation Loss: 0.109499\n",
      "Epoch: 335, Training Loss: 0.114448, Validation Loss: 0.109491\n",
      "Epoch: 336, Training Loss: 0.114445, Validation Loss: 0.109485\n",
      "Epoch: 337, Training Loss: 0.114442, Validation Loss: 0.109483\n",
      "Epoch: 338, Training Loss: 0.114440, Validation Loss: 0.109477\n",
      "Epoch: 339, Training Loss: 0.114437, Validation Loss: 0.109472\n",
      "Epoch: 340, Training Loss: 0.114434, Validation Loss: 0.109461\n",
      "Epoch: 341, Training Loss: 0.114432, Validation Loss: 0.109451\n",
      "Epoch: 342, Training Loss: 0.114429, Validation Loss: 0.109446\n",
      "Epoch: 343, Training Loss: 0.114426, Validation Loss: 0.109448\n",
      "Epoch: 344, Training Loss: 0.114423, Validation Loss: 0.109442\n",
      "Epoch: 345, Training Loss: 0.114421, Validation Loss: 0.109427\n",
      "Epoch: 346, Training Loss: 0.114418, Validation Loss: 0.109415\n",
      "Epoch: 347, Training Loss: 0.114415, Validation Loss: 0.109412\n",
      "Epoch: 348, Training Loss: 0.114412, Validation Loss: 0.109414\n",
      "Epoch: 349, Training Loss: 0.114410, Validation Loss: 0.109416\n",
      "Epoch: 350, Training Loss: 0.114407, Validation Loss: 0.109403\n",
      "Epoch: 351, Training Loss: 0.114404, Validation Loss: 0.109384\n",
      "Epoch: 352, Training Loss: 0.114401, Validation Loss: 0.109377\n",
      "Epoch: 353, Training Loss: 0.114398, Validation Loss: 0.109368\n",
      "Epoch: 354, Training Loss: 0.114395, Validation Loss: 0.109373\n",
      "Epoch: 355, Training Loss: 0.114393, Validation Loss: 0.109377\n",
      "Epoch: 356, Training Loss: 0.114390, Validation Loss: 0.109368\n",
      "Epoch: 357, Training Loss: 0.114387, Validation Loss: 0.109350\n",
      "Epoch: 358, Training Loss: 0.114384, Validation Loss: 0.109331\n",
      "Epoch: 359, Training Loss: 0.114381, Validation Loss: 0.109325\n",
      "Epoch: 360, Training Loss: 0.114378, Validation Loss: 0.109337\n",
      "Epoch: 361, Training Loss: 0.114375, Validation Loss: 0.109343\n",
      "Epoch: 362, Training Loss: 0.114372, Validation Loss: 0.109317\n",
      "Epoch: 363, Training Loss: 0.114369, Validation Loss: 0.109297\n",
      "Epoch: 364, Training Loss: 0.114366, Validation Loss: 0.109303\n",
      "Epoch: 365, Training Loss: 0.114363, Validation Loss: 0.109307\n",
      "Epoch: 366, Training Loss: 0.114360, Validation Loss: 0.109290\n",
      "Epoch: 367, Training Loss: 0.114357, Validation Loss: 0.109272\n",
      "Epoch: 368, Training Loss: 0.114354, Validation Loss: 0.109272\n",
      "Epoch: 369, Training Loss: 0.114351, Validation Loss: 0.109281\n",
      "Epoch: 370, Training Loss: 0.114348, Validation Loss: 0.109275\n",
      "Epoch: 371, Training Loss: 0.114345, Validation Loss: 0.109247\n",
      "Epoch: 372, Training Loss: 0.114342, Validation Loss: 0.109232\n",
      "Epoch: 373, Training Loss: 0.114338, Validation Loss: 0.109251\n",
      "Epoch: 374, Training Loss: 0.114335, Validation Loss: 0.109250\n",
      "Epoch: 375, Training Loss: 0.114332, Validation Loss: 0.109224\n",
      "Epoch: 376, Training Loss: 0.114328, Validation Loss: 0.109214\n",
      "Epoch: 377, Training Loss: 0.114325, Validation Loss: 0.109222\n",
      "Epoch: 378, Training Loss: 0.114321, Validation Loss: 0.109215\n",
      "Epoch: 379, Training Loss: 0.114318, Validation Loss: 0.109203\n",
      "Epoch: 380, Training Loss: 0.114314, Validation Loss: 0.109194\n",
      "Epoch: 381, Training Loss: 0.114310, Validation Loss: 0.109192\n",
      "Epoch: 382, Training Loss: 0.114306, Validation Loss: 0.109193\n",
      "Epoch: 383, Training Loss: 0.114303, Validation Loss: 0.109189\n",
      "Epoch: 384, Training Loss: 0.114299, Validation Loss: 0.109178\n",
      "Epoch: 385, Training Loss: 0.114295, Validation Loss: 0.109168\n",
      "Epoch: 386, Training Loss: 0.114291, Validation Loss: 0.109166\n",
      "Epoch: 387, Training Loss: 0.114287, Validation Loss: 0.109166\n",
      "Epoch: 388, Training Loss: 0.114283, Validation Loss: 0.109158\n",
      "Epoch: 389, Training Loss: 0.114279, Validation Loss: 0.109160\n",
      "Epoch: 390, Training Loss: 0.114275, Validation Loss: 0.109150\n",
      "Epoch: 391, Training Loss: 0.114272, Validation Loss: 0.109136\n",
      "Epoch: 392, Training Loss: 0.114268, Validation Loss: 0.109149\n",
      "Epoch: 393, Training Loss: 0.114265, Validation Loss: 0.109133\n",
      "Epoch: 394, Training Loss: 0.114261, Validation Loss: 0.109120\n",
      "Epoch: 395, Training Loss: 0.114258, Validation Loss: 0.109120\n",
      "Epoch: 396, Training Loss: 0.114254, Validation Loss: 0.109122\n",
      "Epoch: 397, Training Loss: 0.114251, Validation Loss: 0.109107\n",
      "Epoch: 398, Training Loss: 0.114247, Validation Loss: 0.109088\n",
      "Epoch: 399, Training Loss: 0.114243, Validation Loss: 0.109085\n",
      "Epoch: 400, Training Loss: 0.114240, Validation Loss: 0.109091\n",
      "Epoch: 401, Training Loss: 0.114236, Validation Loss: 0.109086\n",
      "Epoch: 402, Training Loss: 0.114232, Validation Loss: 0.109068\n",
      "Epoch: 403, Training Loss: 0.114228, Validation Loss: 0.109051\n",
      "Epoch: 404, Training Loss: 0.114225, Validation Loss: 0.109047\n",
      "Epoch: 405, Training Loss: 0.114221, Validation Loss: 0.109049\n",
      "Epoch: 406, Training Loss: 0.114217, Validation Loss: 0.109044\n",
      "Epoch: 407, Training Loss: 0.114213, Validation Loss: 0.109030\n",
      "Epoch: 408, Training Loss: 0.114209, Validation Loss: 0.109008\n",
      "Epoch: 409, Training Loss: 0.114205, Validation Loss: 0.109004\n",
      "Epoch: 410, Training Loss: 0.114201, Validation Loss: 0.109011\n",
      "Epoch: 411, Training Loss: 0.114198, Validation Loss: 0.108992\n",
      "Epoch: 412, Training Loss: 0.114194, Validation Loss: 0.108976\n",
      "Epoch: 413, Training Loss: 0.114190, Validation Loss: 0.108972\n",
      "Epoch: 414, Training Loss: 0.114186, Validation Loss: 0.108982\n",
      "Epoch: 415, Training Loss: 0.114182, Validation Loss: 0.108972\n",
      "Epoch: 416, Training Loss: 0.114178, Validation Loss: 0.108932\n",
      "Epoch: 417, Training Loss: 0.114174, Validation Loss: 0.108925\n",
      "Epoch: 418, Training Loss: 0.114170, Validation Loss: 0.108937\n",
      "Epoch: 419, Training Loss: 0.114166, Validation Loss: 0.108930\n",
      "Epoch: 420, Training Loss: 0.114162, Validation Loss: 0.108920\n",
      "Epoch: 421, Training Loss: 0.114159, Validation Loss: 0.108911\n",
      "Epoch: 422, Training Loss: 0.114154, Validation Loss: 0.108912\n",
      "Epoch: 423, Training Loss: 0.114151, Validation Loss: 0.108906\n",
      "Epoch: 424, Training Loss: 0.114147, Validation Loss: 0.108859\n",
      "Epoch: 425, Training Loss: 0.114143, Validation Loss: 0.108859\n",
      "Epoch: 426, Training Loss: 0.114138, Validation Loss: 0.108894\n",
      "Epoch: 427, Training Loss: 0.114135, Validation Loss: 0.108902\n",
      "Epoch: 428, Training Loss: 0.114130, Validation Loss: 0.108867\n",
      "Epoch: 429, Training Loss: 0.114127, Validation Loss: 0.108832\n",
      "Epoch: 430, Training Loss: 0.114123, Validation Loss: 0.108802\n",
      "Epoch: 431, Training Loss: 0.114118, Validation Loss: 0.108836\n",
      "Epoch: 432, Training Loss: 0.114115, Validation Loss: 0.108874\n",
      "Epoch: 433, Training Loss: 0.114110, Validation Loss: 0.108821\n",
      "Epoch: 434, Training Loss: 0.114106, Validation Loss: 0.108786\n",
      "Epoch: 435, Training Loss: 0.114101, Validation Loss: 0.108805\n",
      "Epoch: 436, Training Loss: 0.114100, Validation Loss: 0.108822\n",
      "Epoch: 437, Training Loss: 0.114093, Validation Loss: 0.108763\n",
      "Epoch: 438, Training Loss: 0.114092, Validation Loss: 0.108721\n",
      "Epoch: 439, Training Loss: 0.114085, Validation Loss: 0.108797\n",
      "Epoch: 440, Training Loss: 0.114085, Validation Loss: 0.108838\n",
      "Epoch: 441, Training Loss: 0.114075, Validation Loss: 0.108733\n",
      "Epoch: 442, Training Loss: 0.114075, Validation Loss: 0.108667\n",
      "Epoch: 443, Training Loss: 0.114067, Validation Loss: 0.108713\n",
      "Epoch: 444, Training Loss: 0.114065, Validation Loss: 0.108780\n",
      "Epoch: 445, Training Loss: 0.114059, Validation Loss: 0.108726\n",
      "Epoch: 446, Training Loss: 0.114060, Validation Loss: 0.108663\n",
      "Epoch: 447, Training Loss: 0.114050, Validation Loss: 0.108702\n",
      "Epoch: 448, Training Loss: 0.114051, Validation Loss: 0.108737\n",
      "Epoch: 449, Training Loss: 0.114043, Validation Loss: 0.108656\n",
      "Epoch: 450, Training Loss: 0.114041, Validation Loss: 0.108595\n",
      "Epoch: 451, Training Loss: 0.114033, Validation Loss: 0.108667\n",
      "Epoch: 452, Training Loss: 0.114032, Validation Loss: 0.108739\n",
      "Epoch: 453, Training Loss: 0.114024, Validation Loss: 0.108652\n",
      "Epoch: 454, Training Loss: 0.114022, Validation Loss: 0.108576\n",
      "Epoch: 455, Training Loss: 0.114015, Validation Loss: 0.108598\n",
      "Epoch: 456, Training Loss: 0.114013, Validation Loss: 0.108664\n",
      "Epoch: 457, Training Loss: 0.114006, Validation Loss: 0.108619\n",
      "Epoch: 458, Training Loss: 0.114002, Validation Loss: 0.108571\n",
      "Epoch: 459, Training Loss: 0.113997, Validation Loss: 0.108580\n",
      "Epoch: 460, Training Loss: 0.113992, Validation Loss: 0.108606\n",
      "Epoch: 461, Training Loss: 0.113988, Validation Loss: 0.108569\n",
      "Epoch: 462, Training Loss: 0.113983, Validation Loss: 0.108548\n",
      "Epoch: 463, Training Loss: 0.113978, Validation Loss: 0.108560\n",
      "Epoch: 464, Training Loss: 0.113976, Validation Loss: 0.108575\n",
      "Epoch: 465, Training Loss: 0.113969, Validation Loss: 0.108518\n",
      "Epoch: 466, Training Loss: 0.113966, Validation Loss: 0.108492\n",
      "Epoch: 467, Training Loss: 0.113960, Validation Loss: 0.108530\n",
      "Epoch: 468, Training Loss: 0.113957, Validation Loss: 0.108559\n",
      "Epoch: 469, Training Loss: 0.113950, Validation Loss: 0.108492\n",
      "Epoch: 470, Training Loss: 0.113946, Validation Loss: 0.108457\n",
      "Epoch: 471, Training Loss: 0.113943, Validation Loss: 0.108488\n",
      "Epoch: 472, Training Loss: 0.113937, Validation Loss: 0.108482\n",
      "Epoch: 473, Training Loss: 0.113933, Validation Loss: 0.108428\n",
      "Epoch: 474, Training Loss: 0.113928, Validation Loss: 0.108451\n",
      "Epoch: 475, Training Loss: 0.113923, Validation Loss: 0.108495\n",
      "Epoch: 476, Training Loss: 0.113919, Validation Loss: 0.108462\n",
      "Epoch: 477, Training Loss: 0.113914, Validation Loss: 0.108373\n",
      "Epoch: 478, Training Loss: 0.113911, Validation Loss: 0.108342\n",
      "Epoch: 479, Training Loss: 0.113902, Validation Loss: 0.108440\n",
      "Epoch: 480, Training Loss: 0.113900, Validation Loss: 0.108472\n",
      "Epoch: 481, Training Loss: 0.113892, Validation Loss: 0.108391\n",
      "Epoch: 482, Training Loss: 0.113888, Validation Loss: 0.108334\n",
      "Epoch: 483, Training Loss: 0.113884, Validation Loss: 0.108343\n",
      "Epoch: 484, Training Loss: 0.113879, Validation Loss: 0.108372\n",
      "Epoch: 485, Training Loss: 0.113872, Validation Loss: 0.108359\n",
      "Epoch: 486, Training Loss: 0.113871, Validation Loss: 0.108306\n",
      "Epoch: 487, Training Loss: 0.113862, Validation Loss: 0.108369\n",
      "Epoch: 488, Training Loss: 0.113859, Validation Loss: 0.108355\n",
      "Epoch: 489, Training Loss: 0.113853, Validation Loss: 0.108270\n",
      "Epoch: 490, Training Loss: 0.113850, Validation Loss: 0.108228\n",
      "Epoch: 491, Training Loss: 0.113843, Validation Loss: 0.108328\n",
      "Epoch: 492, Training Loss: 0.113837, Validation Loss: 0.108330\n",
      "Epoch: 493, Training Loss: 0.113833, Validation Loss: 0.108245\n",
      "Epoch: 494, Training Loss: 0.113826, Validation Loss: 0.108231\n",
      "Epoch: 495, Training Loss: 0.113826, Validation Loss: 0.108292\n",
      "Epoch: 496, Training Loss: 0.113816, Validation Loss: 0.108203\n",
      "Epoch: 497, Training Loss: 0.113816, Validation Loss: 0.108166\n",
      "Epoch: 498, Training Loss: 0.113809, Validation Loss: 0.108299\n",
      "Epoch: 499, Training Loss: 0.113802, Validation Loss: 0.108265\n",
      "Epoch: 500, Training Loss: 0.113799, Validation Loss: 0.108124\n",
      "Epoch: 501, Training Loss: 0.113790, Validation Loss: 0.108150\n",
      "Epoch: 502, Training Loss: 0.113786, Validation Loss: 0.108225\n",
      "Epoch: 503, Training Loss: 0.113778, Validation Loss: 0.108188\n",
      "Epoch: 504, Training Loss: 0.113779, Validation Loss: 0.108107\n",
      "Epoch: 505, Training Loss: 0.113768, Validation Loss: 0.108169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 506, Training Loss: 0.113764, Validation Loss: 0.108174\n",
      "Epoch: 507, Training Loss: 0.113758, Validation Loss: 0.108083\n",
      "Epoch: 508, Training Loss: 0.113752, Validation Loss: 0.108077\n",
      "Epoch: 509, Training Loss: 0.113753, Validation Loss: 0.108179\n",
      "Epoch: 510, Training Loss: 0.113741, Validation Loss: 0.108088\n",
      "Epoch: 511, Training Loss: 0.113742, Validation Loss: 0.108013\n",
      "Epoch: 512, Training Loss: 0.113731, Validation Loss: 0.108120\n",
      "Epoch: 513, Training Loss: 0.113726, Validation Loss: 0.108104\n",
      "Epoch: 514, Training Loss: 0.113726, Validation Loss: 0.107982\n",
      "Epoch: 515, Training Loss: 0.113713, Validation Loss: 0.108051\n",
      "Epoch: 516, Training Loss: 0.113713, Validation Loss: 0.108098\n",
      "Epoch: 517, Training Loss: 0.113705, Validation Loss: 0.107967\n",
      "Epoch: 518, Training Loss: 0.113697, Validation Loss: 0.107990\n",
      "Epoch: 519, Training Loss: 0.113694, Validation Loss: 0.108044\n",
      "Epoch: 520, Training Loss: 0.113686, Validation Loss: 0.107989\n",
      "Epoch: 521, Training Loss: 0.113680, Validation Loss: 0.107961\n",
      "Epoch: 522, Training Loss: 0.113676, Validation Loss: 0.107987\n",
      "Epoch: 523, Training Loss: 0.113670, Validation Loss: 0.107954\n",
      "Epoch: 524, Training Loss: 0.113668, Validation Loss: 0.107891\n",
      "Epoch: 525, Training Loss: 0.113659, Validation Loss: 0.107983\n",
      "Epoch: 526, Training Loss: 0.113653, Validation Loss: 0.107963\n",
      "Epoch: 527, Training Loss: 0.113650, Validation Loss: 0.107861\n",
      "Epoch: 528, Training Loss: 0.113643, Validation Loss: 0.107918\n",
      "Epoch: 529, Training Loss: 0.113641, Validation Loss: 0.107938\n",
      "Epoch: 530, Training Loss: 0.113631, Validation Loss: 0.107830\n",
      "Epoch: 531, Training Loss: 0.113625, Validation Loss: 0.107845\n",
      "Epoch: 532, Training Loss: 0.113623, Validation Loss: 0.107947\n",
      "Epoch: 533, Training Loss: 0.113614, Validation Loss: 0.107876\n",
      "Epoch: 534, Training Loss: 0.113614, Validation Loss: 0.107748\n",
      "Epoch: 535, Training Loss: 0.113604, Validation Loss: 0.107855\n",
      "Epoch: 536, Training Loss: 0.113599, Validation Loss: 0.107870\n",
      "Epoch: 537, Training Loss: 0.113593, Validation Loss: 0.107752\n",
      "Epoch: 538, Training Loss: 0.113584, Validation Loss: 0.107783\n",
      "Epoch: 539, Training Loss: 0.113582, Validation Loss: 0.107857\n",
      "Epoch: 540, Training Loss: 0.113573, Validation Loss: 0.107774\n",
      "Epoch: 541, Training Loss: 0.113570, Validation Loss: 0.107689\n",
      "Epoch: 542, Training Loss: 0.113561, Validation Loss: 0.107752\n",
      "Epoch: 543, Training Loss: 0.113557, Validation Loss: 0.107798\n",
      "Epoch: 544, Training Loss: 0.113549, Validation Loss: 0.107714\n",
      "Epoch: 545, Training Loss: 0.113545, Validation Loss: 0.107674\n",
      "Epoch: 546, Training Loss: 0.113538, Validation Loss: 0.107740\n",
      "Epoch: 547, Training Loss: 0.113533, Validation Loss: 0.107728\n",
      "Epoch: 548, Training Loss: 0.113527, Validation Loss: 0.107626\n",
      "Epoch: 549, Training Loss: 0.113520, Validation Loss: 0.107640\n",
      "Epoch: 550, Training Loss: 0.113517, Validation Loss: 0.107742\n",
      "Epoch: 551, Training Loss: 0.113507, Validation Loss: 0.107651\n",
      "Epoch: 552, Training Loss: 0.113505, Validation Loss: 0.107575\n",
      "Epoch: 553, Training Loss: 0.113495, Validation Loss: 0.107632\n",
      "Epoch: 554, Training Loss: 0.113492, Validation Loss: 0.107681\n",
      "Epoch: 555, Training Loss: 0.113485, Validation Loss: 0.107552\n",
      "Epoch: 556, Training Loss: 0.113477, Validation Loss: 0.107569\n",
      "Epoch: 557, Training Loss: 0.113473, Validation Loss: 0.107638\n",
      "Epoch: 558, Training Loss: 0.113465, Validation Loss: 0.107561\n",
      "Epoch: 559, Training Loss: 0.113459, Validation Loss: 0.107525\n",
      "Epoch: 560, Training Loss: 0.113453, Validation Loss: 0.107573\n",
      "Epoch: 561, Training Loss: 0.113446, Validation Loss: 0.107538\n",
      "Epoch: 562, Training Loss: 0.113441, Validation Loss: 0.107494\n",
      "Epoch: 563, Training Loss: 0.113434, Validation Loss: 0.107525\n",
      "Epoch: 564, Training Loss: 0.113429, Validation Loss: 0.107557\n",
      "Epoch: 565, Training Loss: 0.113422, Validation Loss: 0.107450\n",
      "Epoch: 566, Training Loss: 0.113416, Validation Loss: 0.107475\n",
      "Epoch: 567, Training Loss: 0.113409, Validation Loss: 0.107484\n",
      "Epoch: 568, Training Loss: 0.113403, Validation Loss: 0.107438\n",
      "Epoch: 569, Training Loss: 0.113397, Validation Loss: 0.107455\n",
      "Epoch: 570, Training Loss: 0.113390, Validation Loss: 0.107476\n",
      "Epoch: 571, Training Loss: 0.113384, Validation Loss: 0.107380\n",
      "Epoch: 572, Training Loss: 0.113377, Validation Loss: 0.107401\n",
      "Epoch: 573, Training Loss: 0.113370, Validation Loss: 0.107410\n",
      "Epoch: 574, Training Loss: 0.113365, Validation Loss: 0.107366\n",
      "Epoch: 575, Training Loss: 0.113358, Validation Loss: 0.107391\n",
      "Epoch: 576, Training Loss: 0.113351, Validation Loss: 0.107388\n",
      "Epoch: 577, Training Loss: 0.113346, Validation Loss: 0.107302\n",
      "Epoch: 578, Training Loss: 0.113338, Validation Loss: 0.107337\n",
      "Epoch: 579, Training Loss: 0.113332, Validation Loss: 0.107352\n",
      "Epoch: 580, Training Loss: 0.113325, Validation Loss: 0.107302\n",
      "Epoch: 581, Training Loss: 0.113318, Validation Loss: 0.107314\n",
      "Epoch: 582, Training Loss: 0.113312, Validation Loss: 0.107272\n",
      "Epoch: 583, Training Loss: 0.113306, Validation Loss: 0.107310\n",
      "Epoch: 584, Training Loss: 0.113299, Validation Loss: 0.107260\n",
      "Epoch: 585, Training Loss: 0.113294, Validation Loss: 0.107219\n",
      "Epoch: 586, Training Loss: 0.113287, Validation Loss: 0.107287\n",
      "Epoch: 587, Training Loss: 0.113279, Validation Loss: 0.107239\n",
      "Epoch: 588, Training Loss: 0.113273, Validation Loss: 0.107231\n",
      "Epoch: 589, Training Loss: 0.113266, Validation Loss: 0.107186\n",
      "Epoch: 590, Training Loss: 0.113261, Validation Loss: 0.107166\n",
      "Epoch: 591, Training Loss: 0.113255, Validation Loss: 0.107255\n",
      "Epoch: 592, Training Loss: 0.113246, Validation Loss: 0.107177\n",
      "Epoch: 593, Training Loss: 0.113240, Validation Loss: 0.107167\n",
      "Epoch: 594, Training Loss: 0.113234, Validation Loss: 0.107144\n",
      "Epoch: 595, Training Loss: 0.113226, Validation Loss: 0.107115\n",
      "Epoch: 596, Training Loss: 0.113223, Validation Loss: 0.107115\n",
      "Epoch: 597, Training Loss: 0.113217, Validation Loss: 0.107206\n",
      "Epoch: 598, Training Loss: 0.113206, Validation Loss: 0.107083\n",
      "Epoch: 599, Training Loss: 0.113201, Validation Loss: 0.107071\n",
      "Epoch: 600, Training Loss: 0.113193, Validation Loss: 0.107062\n",
      "Epoch: 601, Training Loss: 0.113185, Validation Loss: 0.107086\n",
      "Epoch: 602, Training Loss: 0.113183, Validation Loss: 0.107056\n",
      "Epoch: 603, Training Loss: 0.113174, Validation Loss: 0.107105\n",
      "Epoch: 604, Training Loss: 0.113165, Validation Loss: 0.107012\n",
      "Epoch: 605, Training Loss: 0.113160, Validation Loss: 0.106975\n",
      "Epoch: 606, Training Loss: 0.113152, Validation Loss: 0.107046\n",
      "Epoch: 607, Training Loss: 0.113145, Validation Loss: 0.107027\n",
      "Epoch: 608, Training Loss: 0.113138, Validation Loss: 0.107026\n",
      "Epoch: 609, Training Loss: 0.113133, Validation Loss: 0.106930\n",
      "Epoch: 610, Training Loss: 0.113130, Validation Loss: 0.107023\n",
      "Epoch: 611, Training Loss: 0.113122, Validation Loss: 0.106894\n",
      "Epoch: 612, Training Loss: 0.113112, Validation Loss: 0.106988\n",
      "Epoch: 613, Training Loss: 0.113104, Validation Loss: 0.106945\n",
      "Epoch: 614, Training Loss: 0.113098, Validation Loss: 0.106934\n",
      "Epoch: 615, Training Loss: 0.113092, Validation Loss: 0.106876\n",
      "Epoch: 616, Training Loss: 0.113084, Validation Loss: 0.106876\n",
      "Epoch: 617, Training Loss: 0.113077, Validation Loss: 0.106889\n",
      "Epoch: 618, Training Loss: 0.113074, Validation Loss: 0.106948\n",
      "Epoch: 619, Training Loss: 0.113070, Validation Loss: 0.106775\n",
      "Epoch: 620, Training Loss: 0.113060, Validation Loss: 0.106883\n",
      "Epoch: 621, Training Loss: 0.113050, Validation Loss: 0.106795\n",
      "Epoch: 622, Training Loss: 0.113043, Validation Loss: 0.106804\n",
      "Epoch: 623, Training Loss: 0.113043, Validation Loss: 0.106895\n",
      "Epoch: 624, Training Loss: 0.113043, Validation Loss: 0.106685\n",
      "Epoch: 625, Training Loss: 0.113042, Validation Loss: 0.106914\n",
      "Epoch: 626, Training Loss: 0.113018, Validation Loss: 0.106751\n",
      "Epoch: 627, Training Loss: 0.113041, Validation Loss: 0.106610\n",
      "Epoch: 628, Training Loss: 0.113073, Validation Loss: 0.107103\n",
      "Epoch: 629, Training Loss: 0.112997, Validation Loss: 0.106700\n",
      "Epoch: 630, Training Loss: 0.113047, Validation Loss: 0.106524\n",
      "Epoch: 631, Training Loss: 0.113096, Validation Loss: 0.107199\n",
      "Epoch: 632, Training Loss: 0.112991, Validation Loss: 0.106765\n",
      "Epoch: 633, Training Loss: 0.113152, Validation Loss: 0.106415\n",
      "Epoch: 634, Training Loss: 0.113114, Validation Loss: 0.107290\n",
      "Epoch: 635, Training Loss: 0.113007, Validation Loss: 0.106943\n",
      "Epoch: 636, Training Loss: 0.113195, Validation Loss: 0.106365\n",
      "Epoch: 637, Training Loss: 0.113025, Validation Loss: 0.107029\n",
      "Epoch: 638, Training Loss: 0.113044, Validation Loss: 0.107111\n",
      "Epoch: 639, Training Loss: 0.113048, Validation Loss: 0.106377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 640, Training Loss: 0.112928, Validation Loss: 0.106627\n",
      "Epoch: 641, Training Loss: 0.112985, Validation Loss: 0.106960\n",
      "Epoch: 642, Training Loss: 0.112937, Validation Loss: 0.106476\n",
      "Epoch: 643, Training Loss: 0.112910, Validation Loss: 0.106506\n",
      "Epoch: 644, Training Loss: 0.112926, Validation Loss: 0.106729\n",
      "Epoch: 645, Training Loss: 0.112895, Validation Loss: 0.106502\n",
      "Epoch: 646, Training Loss: 0.112904, Validation Loss: 0.106416\n",
      "Epoch: 647, Training Loss: 0.112917, Validation Loss: 0.106762\n",
      "Epoch: 648, Training Loss: 0.112874, Validation Loss: 0.106504\n",
      "Epoch: 649, Training Loss: 0.112901, Validation Loss: 0.106340\n",
      "Epoch: 650, Training Loss: 0.112901, Validation Loss: 0.106729\n",
      "Epoch: 651, Training Loss: 0.112862, Validation Loss: 0.106547\n",
      "Epoch: 652, Training Loss: 0.112914, Validation Loss: 0.106272\n",
      "Epoch: 653, Training Loss: 0.112893, Validation Loss: 0.106751\n",
      "Epoch: 654, Training Loss: 0.112851, Validation Loss: 0.106572\n",
      "Epoch: 655, Training Loss: 0.112894, Validation Loss: 0.106231\n",
      "Epoch: 656, Training Loss: 0.112847, Validation Loss: 0.106599\n",
      "Epoch: 657, Training Loss: 0.112833, Validation Loss: 0.106566\n",
      "Epoch: 658, Training Loss: 0.112862, Validation Loss: 0.106236\n",
      "Epoch: 659, Training Loss: 0.112824, Validation Loss: 0.106558\n",
      "Epoch: 660, Training Loss: 0.112812, Validation Loss: 0.106492\n",
      "Epoch: 661, Training Loss: 0.112828, Validation Loss: 0.106206\n",
      "Epoch: 662, Training Loss: 0.112787, Validation Loss: 0.106408\n",
      "Epoch: 663, Training Loss: 0.112792, Validation Loss: 0.106486\n",
      "Epoch: 664, Training Loss: 0.112801, Validation Loss: 0.106223\n",
      "Epoch: 665, Training Loss: 0.112775, Validation Loss: 0.106410\n",
      "Epoch: 666, Training Loss: 0.112776, Validation Loss: 0.106390\n",
      "Epoch: 667, Training Loss: 0.112773, Validation Loss: 0.106174\n",
      "Epoch: 668, Training Loss: 0.112747, Validation Loss: 0.106326\n",
      "Epoch: 669, Training Loss: 0.112758, Validation Loss: 0.106443\n",
      "Epoch: 670, Training Loss: 0.112759, Validation Loss: 0.106175\n",
      "Epoch: 671, Training Loss: 0.112733, Validation Loss: 0.106323\n",
      "Epoch: 672, Training Loss: 0.112738, Validation Loss: 0.106327\n",
      "Epoch: 673, Training Loss: 0.112729, Validation Loss: 0.106126\n",
      "Epoch: 674, Training Loss: 0.112709, Validation Loss: 0.106232\n",
      "Epoch: 675, Training Loss: 0.112715, Validation Loss: 0.106330\n",
      "Epoch: 676, Training Loss: 0.112700, Validation Loss: 0.106189\n",
      "Epoch: 677, Training Loss: 0.112694, Validation Loss: 0.106197\n",
      "Epoch: 678, Training Loss: 0.112696, Validation Loss: 0.106182\n",
      "Epoch: 679, Training Loss: 0.112680, Validation Loss: 0.106132\n",
      "Epoch: 680, Training Loss: 0.112671, Validation Loss: 0.106172\n",
      "Epoch: 681, Training Loss: 0.112672, Validation Loss: 0.106231\n",
      "Epoch: 682, Training Loss: 0.112657, Validation Loss: 0.106174\n",
      "Epoch: 683, Training Loss: 0.112660, Validation Loss: 0.106033\n",
      "Epoch: 684, Training Loss: 0.112650, Validation Loss: 0.106168\n",
      "Epoch: 685, Training Loss: 0.112640, Validation Loss: 0.106156\n",
      "Epoch: 686, Training Loss: 0.112637, Validation Loss: 0.106032\n",
      "Epoch: 687, Training Loss: 0.112627, Validation Loss: 0.106156\n",
      "Epoch: 688, Training Loss: 0.112616, Validation Loss: 0.106086\n",
      "Epoch: 689, Training Loss: 0.112611, Validation Loss: 0.106034\n",
      "Epoch: 690, Training Loss: 0.112604, Validation Loss: 0.106035\n",
      "Epoch: 691, Training Loss: 0.112597, Validation Loss: 0.106068\n",
      "Epoch: 692, Training Loss: 0.112590, Validation Loss: 0.106022\n",
      "Epoch: 693, Training Loss: 0.112582, Validation Loss: 0.106022\n",
      "Epoch: 694, Training Loss: 0.112576, Validation Loss: 0.106009\n",
      "Epoch: 695, Training Loss: 0.112569, Validation Loss: 0.105965\n",
      "Epoch: 696, Training Loss: 0.112562, Validation Loss: 0.105966\n",
      "Epoch: 697, Training Loss: 0.112559, Validation Loss: 0.106043\n",
      "Epoch: 698, Training Loss: 0.112549, Validation Loss: 0.105942\n",
      "Epoch: 699, Training Loss: 0.112546, Validation Loss: 0.105885\n",
      "Epoch: 700, Training Loss: 0.112542, Validation Loss: 0.106038\n",
      "Epoch: 701, Training Loss: 0.112528, Validation Loss: 0.105948\n",
      "Epoch: 702, Training Loss: 0.112528, Validation Loss: 0.105840\n",
      "Epoch: 703, Training Loss: 0.112518, Validation Loss: 0.105984\n",
      "Epoch: 704, Training Loss: 0.112508, Validation Loss: 0.105933\n",
      "Epoch: 705, Training Loss: 0.112505, Validation Loss: 0.105822\n",
      "Epoch: 706, Training Loss: 0.112496, Validation Loss: 0.105938\n",
      "Epoch: 707, Training Loss: 0.112487, Validation Loss: 0.105900\n",
      "Epoch: 708, Training Loss: 0.112485, Validation Loss: 0.105785\n",
      "Epoch: 709, Training Loss: 0.112475, Validation Loss: 0.105913\n",
      "Epoch: 710, Training Loss: 0.112465, Validation Loss: 0.105858\n",
      "Epoch: 711, Training Loss: 0.112460, Validation Loss: 0.105764\n",
      "Epoch: 712, Training Loss: 0.112450, Validation Loss: 0.105810\n",
      "Epoch: 713, Training Loss: 0.112445, Validation Loss: 0.105859\n",
      "Epoch: 714, Training Loss: 0.112436, Validation Loss: 0.105763\n",
      "Epoch: 715, Training Loss: 0.112430, Validation Loss: 0.105729\n",
      "Epoch: 716, Training Loss: 0.112422, Validation Loss: 0.105804\n",
      "Epoch: 717, Training Loss: 0.112414, Validation Loss: 0.105786\n",
      "Epoch: 718, Training Loss: 0.112408, Validation Loss: 0.105703\n",
      "Epoch: 719, Training Loss: 0.112399, Validation Loss: 0.105723\n",
      "Epoch: 720, Training Loss: 0.112394, Validation Loss: 0.105777\n",
      "Epoch: 721, Training Loss: 0.112384, Validation Loss: 0.105690\n",
      "Epoch: 722, Training Loss: 0.112377, Validation Loss: 0.105673\n",
      "Epoch: 723, Training Loss: 0.112371, Validation Loss: 0.105735\n",
      "Epoch: 724, Training Loss: 0.112361, Validation Loss: 0.105673\n",
      "Epoch: 725, Training Loss: 0.112354, Validation Loss: 0.105648\n",
      "Epoch: 726, Training Loss: 0.112347, Validation Loss: 0.105692\n",
      "Epoch: 727, Training Loss: 0.112338, Validation Loss: 0.105643\n",
      "Epoch: 728, Training Loss: 0.112333, Validation Loss: 0.105596\n",
      "Epoch: 729, Training Loss: 0.112324, Validation Loss: 0.105662\n",
      "Epoch: 730, Training Loss: 0.112315, Validation Loss: 0.105610\n",
      "Epoch: 731, Training Loss: 0.112308, Validation Loss: 0.105578\n",
      "Epoch: 732, Training Loss: 0.112301, Validation Loss: 0.105618\n",
      "Epoch: 733, Training Loss: 0.112292, Validation Loss: 0.105581\n",
      "Epoch: 734, Training Loss: 0.112284, Validation Loss: 0.105553\n",
      "Epoch: 735, Training Loss: 0.112276, Validation Loss: 0.105567\n",
      "Epoch: 736, Training Loss: 0.112269, Validation Loss: 0.105549\n",
      "Epoch: 737, Training Loss: 0.112260, Validation Loss: 0.105543\n",
      "Epoch: 738, Training Loss: 0.112252, Validation Loss: 0.105532\n",
      "Epoch: 739, Training Loss: 0.112244, Validation Loss: 0.105502\n",
      "Epoch: 740, Training Loss: 0.112237, Validation Loss: 0.105494\n",
      "Epoch: 741, Training Loss: 0.112227, Validation Loss: 0.105467\n",
      "Epoch: 742, Training Loss: 0.112220, Validation Loss: 0.105479\n",
      "Epoch: 743, Training Loss: 0.112213, Validation Loss: 0.105495\n",
      "Epoch: 744, Training Loss: 0.112202, Validation Loss: 0.105449\n",
      "Epoch: 745, Training Loss: 0.112196, Validation Loss: 0.105406\n",
      "Epoch: 746, Training Loss: 0.112189, Validation Loss: 0.105481\n",
      "Epoch: 747, Training Loss: 0.112179, Validation Loss: 0.105375\n",
      "Epoch: 748, Training Loss: 0.112169, Validation Loss: 0.105418\n",
      "Epoch: 749, Training Loss: 0.112161, Validation Loss: 0.105411\n",
      "Epoch: 750, Training Loss: 0.112153, Validation Loss: 0.105341\n",
      "Epoch: 751, Training Loss: 0.112143, Validation Loss: 0.105396\n",
      "Epoch: 752, Training Loss: 0.112134, Validation Loss: 0.105345\n",
      "Epoch: 753, Training Loss: 0.112126, Validation Loss: 0.105304\n",
      "Epoch: 754, Training Loss: 0.112117, Validation Loss: 0.105372\n",
      "Epoch: 755, Training Loss: 0.112108, Validation Loss: 0.105351\n",
      "Epoch: 756, Training Loss: 0.112099, Validation Loss: 0.105280\n",
      "Epoch: 757, Training Loss: 0.112089, Validation Loss: 0.105295\n",
      "Epoch: 758, Training Loss: 0.112085, Validation Loss: 0.105354\n",
      "Epoch: 759, Training Loss: 0.112075, Validation Loss: 0.105212\n",
      "Epoch: 760, Training Loss: 0.112062, Validation Loss: 0.105258\n",
      "Epoch: 761, Training Loss: 0.112056, Validation Loss: 0.105299\n",
      "Epoch: 762, Training Loss: 0.112043, Validation Loss: 0.105215\n",
      "Epoch: 763, Training Loss: 0.112035, Validation Loss: 0.105182\n",
      "Epoch: 764, Training Loss: 0.112031, Validation Loss: 0.105291\n",
      "Epoch: 765, Training Loss: 0.112016, Validation Loss: 0.105161\n",
      "Epoch: 766, Training Loss: 0.112004, Validation Loss: 0.105184\n",
      "Epoch: 767, Training Loss: 0.111999, Validation Loss: 0.105253\n",
      "Epoch: 768, Training Loss: 0.111993, Validation Loss: 0.105094\n",
      "Epoch: 769, Training Loss: 0.111976, Validation Loss: 0.105202\n",
      "Epoch: 770, Training Loss: 0.111965, Validation Loss: 0.105177\n",
      "Epoch: 771, Training Loss: 0.111962, Validation Loss: 0.105061\n",
      "Epoch: 772, Training Loss: 0.111945, Validation Loss: 0.105158\n",
      "Epoch: 773, Training Loss: 0.111935, Validation Loss: 0.105128\n",
      "Epoch: 774, Training Loss: 0.111925, Validation Loss: 0.105065\n",
      "Epoch: 775, Training Loss: 0.111916, Validation Loss: 0.105108\n",
      "Epoch: 776, Training Loss: 0.111904, Validation Loss: 0.105031\n",
      "Epoch: 777, Training Loss: 0.111893, Validation Loss: 0.105085\n",
      "Epoch: 778, Training Loss: 0.111884, Validation Loss: 0.105004\n",
      "Epoch: 779, Training Loss: 0.111873, Validation Loss: 0.105018\n",
      "Epoch: 780, Training Loss: 0.111862, Validation Loss: 0.105035\n",
      "Epoch: 781, Training Loss: 0.111849, Validation Loss: 0.104995\n",
      "Epoch: 782, Training Loss: 0.111841, Validation Loss: 0.104936\n",
      "Epoch: 783, Training Loss: 0.111829, Validation Loss: 0.104988\n",
      "Epoch: 784, Training Loss: 0.111815, Validation Loss: 0.104953\n",
      "Epoch: 785, Training Loss: 0.111805, Validation Loss: 0.104953\n",
      "Epoch: 786, Training Loss: 0.111793, Validation Loss: 0.104957\n",
      "Epoch: 787, Training Loss: 0.111785, Validation Loss: 0.104864\n",
      "Epoch: 788, Training Loss: 0.111776, Validation Loss: 0.104989\n",
      "Epoch: 789, Training Loss: 0.111758, Validation Loss: 0.104875\n",
      "Epoch: 790, Training Loss: 0.111752, Validation Loss: 0.104809\n",
      "Epoch: 791, Training Loss: 0.111744, Validation Loss: 0.104963\n",
      "Epoch: 792, Training Loss: 0.111724, Validation Loss: 0.104804\n",
      "Epoch: 793, Training Loss: 0.111709, Validation Loss: 0.104804\n",
      "Epoch: 794, Training Loss: 0.111702, Validation Loss: 0.104874\n",
      "Epoch: 795, Training Loss: 0.111686, Validation Loss: 0.104757\n",
      "Epoch: 796, Training Loss: 0.111673, Validation Loss: 0.104750\n",
      "Epoch: 797, Training Loss: 0.111670, Validation Loss: 0.104883\n",
      "Epoch: 798, Training Loss: 0.111654, Validation Loss: 0.104678\n",
      "Epoch: 799, Training Loss: 0.111633, Validation Loss: 0.104734\n",
      "Epoch: 800, Training Loss: 0.111622, Validation Loss: 0.104768\n",
      "Epoch: 801, Training Loss: 0.111609, Validation Loss: 0.104650\n",
      "Epoch: 802, Training Loss: 0.111595, Validation Loss: 0.104734\n",
      "Epoch: 803, Training Loss: 0.111579, Validation Loss: 0.104672\n",
      "Epoch: 804, Training Loss: 0.111567, Validation Loss: 0.104617\n",
      "Epoch: 805, Training Loss: 0.111558, Validation Loss: 0.104737\n",
      "Epoch: 806, Training Loss: 0.111539, Validation Loss: 0.104591\n",
      "Epoch: 807, Training Loss: 0.111526, Validation Loss: 0.104561\n",
      "Epoch: 808, Training Loss: 0.111526, Validation Loss: 0.104750\n",
      "Epoch: 809, Training Loss: 0.111501, Validation Loss: 0.104499\n",
      "Epoch: 810, Training Loss: 0.111480, Validation Loss: 0.104565\n",
      "Epoch: 811, Training Loss: 0.111469, Validation Loss: 0.104615\n",
      "Epoch: 812, Training Loss: 0.111454, Validation Loss: 0.104457\n",
      "Epoch: 813, Training Loss: 0.111437, Validation Loss: 0.104562\n",
      "Epoch: 814, Training Loss: 0.111419, Validation Loss: 0.104492\n",
      "Epoch: 815, Training Loss: 0.111405, Validation Loss: 0.104423\n",
      "Epoch: 816, Training Loss: 0.111388, Validation Loss: 0.104467\n",
      "Epoch: 817, Training Loss: 0.111372, Validation Loss: 0.104451\n",
      "Epoch: 818, Training Loss: 0.111356, Validation Loss: 0.104363\n",
      "Epoch: 819, Training Loss: 0.111338, Validation Loss: 0.104375\n",
      "Epoch: 820, Training Loss: 0.111323, Validation Loss: 0.104396\n",
      "Epoch: 821, Training Loss: 0.111306, Validation Loss: 0.104308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 822, Training Loss: 0.111288, Validation Loss: 0.104296\n",
      "Epoch: 823, Training Loss: 0.111274, Validation Loss: 0.104364\n",
      "Epoch: 824, Training Loss: 0.111254, Validation Loss: 0.104250\n",
      "Epoch: 825, Training Loss: 0.111237, Validation Loss: 0.104233\n",
      "Epoch: 826, Training Loss: 0.111220, Validation Loss: 0.104299\n",
      "Epoch: 827, Training Loss: 0.111200, Validation Loss: 0.104212\n",
      "Epoch: 828, Training Loss: 0.111184, Validation Loss: 0.104157\n",
      "Epoch: 829, Training Loss: 0.111175, Validation Loss: 0.104321\n",
      "Epoch: 830, Training Loss: 0.111158, Validation Loss: 0.104063\n",
      "Epoch: 831, Training Loss: 0.111130, Validation Loss: 0.104229\n",
      "Epoch: 832, Training Loss: 0.111106, Validation Loss: 0.104165\n",
      "Epoch: 833, Training Loss: 0.111095, Validation Loss: 0.104024\n",
      "Epoch: 834, Training Loss: 0.111081, Validation Loss: 0.104236\n",
      "Epoch: 835, Training Loss: 0.111053, Validation Loss: 0.103993\n",
      "Epoch: 836, Training Loss: 0.111026, Validation Loss: 0.104078\n",
      "Epoch: 837, Training Loss: 0.111006, Validation Loss: 0.104075\n",
      "Epoch: 838, Training Loss: 0.110988, Validation Loss: 0.103935\n",
      "Epoch: 839, Training Loss: 0.110963, Validation Loss: 0.104007\n",
      "Epoch: 840, Training Loss: 0.110941, Validation Loss: 0.103982\n",
      "Epoch: 841, Training Loss: 0.110923, Validation Loss: 0.103857\n",
      "Epoch: 842, Training Loss: 0.110897, Validation Loss: 0.103917\n",
      "Epoch: 843, Training Loss: 0.110875, Validation Loss: 0.103894\n",
      "Epoch: 844, Training Loss: 0.110854, Validation Loss: 0.103785\n",
      "Epoch: 845, Training Loss: 0.110829, Validation Loss: 0.103859\n",
      "Epoch: 846, Training Loss: 0.110805, Validation Loss: 0.103809\n",
      "Epoch: 847, Training Loss: 0.110783, Validation Loss: 0.103723\n",
      "Epoch: 848, Training Loss: 0.110758, Validation Loss: 0.103780\n",
      "Epoch: 849, Training Loss: 0.110732, Validation Loss: 0.103721\n",
      "Epoch: 850, Training Loss: 0.110709, Validation Loss: 0.103648\n",
      "Epoch: 851, Training Loss: 0.110683, Validation Loss: 0.103702\n",
      "Epoch: 852, Training Loss: 0.110656, Validation Loss: 0.103617\n",
      "Epoch: 853, Training Loss: 0.110631, Validation Loss: 0.103574\n",
      "Epoch: 854, Training Loss: 0.110606, Validation Loss: 0.103611\n",
      "Epoch: 855, Training Loss: 0.110583, Validation Loss: 0.103475\n",
      "Epoch: 856, Training Loss: 0.110555, Validation Loss: 0.103569\n",
      "Epoch: 857, Training Loss: 0.110523, Validation Loss: 0.103451\n",
      "Epoch: 858, Training Loss: 0.110496, Validation Loss: 0.103404\n",
      "Epoch: 859, Training Loss: 0.110473, Validation Loss: 0.103483\n",
      "Epoch: 860, Training Loss: 0.110446, Validation Loss: 0.103310\n",
      "Epoch: 861, Training Loss: 0.110414, Validation Loss: 0.103424\n",
      "Epoch: 862, Training Loss: 0.110383, Validation Loss: 0.103276\n",
      "Epoch: 863, Training Loss: 0.110352, Validation Loss: 0.103339\n",
      "Epoch: 864, Training Loss: 0.110322, Validation Loss: 0.103201\n",
      "Epoch: 865, Training Loss: 0.110289, Validation Loss: 0.103244\n",
      "Epoch: 866, Training Loss: 0.110257, Validation Loss: 0.103153\n",
      "Epoch: 867, Training Loss: 0.110225, Validation Loss: 0.103142\n",
      "Epoch: 868, Training Loss: 0.110196, Validation Loss: 0.103162\n",
      "Epoch: 869, Training Loss: 0.110174, Validation Loss: 0.102979\n",
      "Epoch: 870, Training Loss: 0.110146, Validation Loss: 0.103176\n",
      "Epoch: 871, Training Loss: 0.110100, Validation Loss: 0.102933\n",
      "Epoch: 872, Training Loss: 0.110061, Validation Loss: 0.103009\n",
      "Epoch: 873, Training Loss: 0.110023, Validation Loss: 0.102918\n",
      "Epoch: 874, Training Loss: 0.109988, Validation Loss: 0.102876\n",
      "Epoch: 875, Training Loss: 0.109954, Validation Loss: 0.102882\n",
      "Epoch: 876, Training Loss: 0.109921, Validation Loss: 0.102740\n",
      "Epoch: 877, Training Loss: 0.109899, Validation Loss: 0.102895\n",
      "Epoch: 878, Training Loss: 0.109860, Validation Loss: 0.102603\n",
      "Epoch: 879, Training Loss: 0.109824, Validation Loss: 0.102803\n",
      "Epoch: 880, Training Loss: 0.109771, Validation Loss: 0.102555\n",
      "Epoch: 881, Training Loss: 0.109733, Validation Loss: 0.102655\n",
      "Epoch: 882, Training Loss: 0.109688, Validation Loss: 0.102505\n",
      "Epoch: 883, Training Loss: 0.109648, Validation Loss: 0.102544\n",
      "Epoch: 884, Training Loss: 0.109607, Validation Loss: 0.102445\n",
      "Epoch: 885, Training Loss: 0.109567, Validation Loss: 0.102459\n",
      "Epoch: 886, Training Loss: 0.109528, Validation Loss: 0.102326\n",
      "Epoch: 887, Training Loss: 0.109505, Validation Loss: 0.102486\n",
      "Epoch: 888, Training Loss: 0.109465, Validation Loss: 0.102175\n",
      "Epoch: 889, Training Loss: 0.109437, Validation Loss: 0.102474\n",
      "Epoch: 890, Training Loss: 0.109394, Validation Loss: 0.102082\n",
      "Epoch: 891, Training Loss: 0.109357, Validation Loss: 0.102416\n",
      "Epoch: 892, Training Loss: 0.109294, Validation Loss: 0.102011\n",
      "Epoch: 893, Training Loss: 0.109250, Validation Loss: 0.102259\n",
      "Epoch: 894, Training Loss: 0.109196, Validation Loss: 0.101925\n",
      "Epoch: 895, Training Loss: 0.109148, Validation Loss: 0.102113\n",
      "Epoch: 896, Training Loss: 0.109099, Validation Loss: 0.101851\n",
      "Epoch: 897, Training Loss: 0.109053, Validation Loss: 0.101998\n",
      "Epoch: 898, Training Loss: 0.109007, Validation Loss: 0.101728\n",
      "Epoch: 899, Training Loss: 0.108963, Validation Loss: 0.101923\n",
      "Epoch: 900, Training Loss: 0.108906, Validation Loss: 0.101651\n",
      "Epoch: 901, Training Loss: 0.108853, Validation Loss: 0.101750\n",
      "Epoch: 902, Training Loss: 0.108803, Validation Loss: 0.101595\n",
      "Epoch: 903, Training Loss: 0.108752, Validation Loss: 0.101613\n",
      "Epoch: 904, Training Loss: 0.108703, Validation Loss: 0.101494\n",
      "Epoch: 905, Training Loss: 0.108655, Validation Loss: 0.101513\n",
      "Epoch: 906, Training Loss: 0.108605, Validation Loss: 0.101382\n",
      "Epoch: 907, Training Loss: 0.108559, Validation Loss: 0.101439\n",
      "Epoch: 908, Training Loss: 0.108514, Validation Loss: 0.101243\n",
      "Epoch: 909, Training Loss: 0.108459, Validation Loss: 0.101339\n",
      "Epoch: 910, Training Loss: 0.108408, Validation Loss: 0.101171\n",
      "Epoch: 911, Training Loss: 0.108355, Validation Loss: 0.101223\n",
      "Epoch: 912, Training Loss: 0.108306, Validation Loss: 0.101074\n",
      "Epoch: 913, Training Loss: 0.108262, Validation Loss: 0.101186\n",
      "Epoch: 914, Training Loss: 0.108215, Validation Loss: 0.100941\n",
      "Epoch: 915, Training Loss: 0.108156, Validation Loss: 0.101042\n",
      "Epoch: 916, Training Loss: 0.108105, Validation Loss: 0.100868\n",
      "Epoch: 917, Training Loss: 0.108053, Validation Loss: 0.100841\n",
      "Epoch: 918, Training Loss: 0.108003, Validation Loss: 0.100855\n",
      "Epoch: 919, Training Loss: 0.107953, Validation Loss: 0.100755\n",
      "Epoch: 920, Training Loss: 0.107907, Validation Loss: 0.100669\n",
      "Epoch: 921, Training Loss: 0.107884, Validation Loss: 0.100890\n",
      "Epoch: 922, Training Loss: 0.107898, Validation Loss: 0.100476\n",
      "Epoch: 923, Training Loss: 0.107907, Validation Loss: 0.101120\n",
      "Epoch: 924, Training Loss: 0.107863, Validation Loss: 0.100370\n",
      "Epoch: 925, Training Loss: 0.107819, Validation Loss: 0.101055\n",
      "Epoch: 926, Training Loss: 0.107759, Validation Loss: 0.100267\n",
      "Epoch: 927, Training Loss: 0.107652, Validation Loss: 0.100750\n",
      "Epoch: 928, Training Loss: 0.107547, Validation Loss: 0.100223\n",
      "Epoch: 929, Training Loss: 0.107477, Validation Loss: 0.100306\n",
      "Epoch: 930, Training Loss: 0.107446, Validation Loss: 0.100390\n",
      "Epoch: 931, Training Loss: 0.107445, Validation Loss: 0.100075\n",
      "Epoch: 932, Training Loss: 0.107459, Validation Loss: 0.100642\n",
      "Epoch: 933, Training Loss: 0.107393, Validation Loss: 0.099985\n",
      "Epoch: 934, Training Loss: 0.107292, Validation Loss: 0.100296\n",
      "Epoch: 935, Training Loss: 0.107224, Validation Loss: 0.100117\n",
      "Epoch: 936, Training Loss: 0.107224, Validation Loss: 0.099911\n",
      "Epoch: 937, Training Loss: 0.107187, Validation Loss: 0.100242\n",
      "Epoch: 938, Training Loss: 0.107142, Validation Loss: 0.099820\n",
      "Epoch: 939, Training Loss: 0.107090, Validation Loss: 0.100022\n",
      "Epoch: 940, Training Loss: 0.107037, Validation Loss: 0.099850\n",
      "Epoch: 941, Training Loss: 0.107011, Validation Loss: 0.099780\n",
      "Epoch: 942, Training Loss: 0.106987, Validation Loss: 0.099985\n",
      "Epoch: 943, Training Loss: 0.106944, Validation Loss: 0.099716\n",
      "Epoch: 944, Training Loss: 0.106904, Validation Loss: 0.099744\n",
      "Epoch: 945, Training Loss: 0.106882, Validation Loss: 0.099795\n",
      "Epoch: 946, Training Loss: 0.106861, Validation Loss: 0.099600\n",
      "Epoch: 947, Training Loss: 0.106856, Validation Loss: 0.099934\n",
      "Epoch: 948, Training Loss: 0.106839, Validation Loss: 0.099515\n",
      "Epoch: 949, Training Loss: 0.106785, Validation Loss: 0.099805\n",
      "Epoch: 950, Training Loss: 0.106734, Validation Loss: 0.099578\n",
      "Epoch: 951, Training Loss: 0.106740, Validation Loss: 0.099447\n",
      "Epoch: 952, Training Loss: 0.106772, Validation Loss: 0.099940\n",
      "Epoch: 953, Training Loss: 0.106786, Validation Loss: 0.099356\n",
      "Epoch: 954, Training Loss: 0.106718, Validation Loss: 0.099892\n",
      "Epoch: 955, Training Loss: 0.106632, Validation Loss: 0.099384\n",
      "Epoch: 956, Training Loss: 0.106593, Validation Loss: 0.099454\n",
      "Epoch: 957, Training Loss: 0.106582, Validation Loss: 0.099523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 958, Training Loss: 0.106565, Validation Loss: 0.099305\n",
      "Epoch: 959, Training Loss: 0.106552, Validation Loss: 0.099568\n",
      "Epoch: 960, Training Loss: 0.106520, Validation Loss: 0.099332\n",
      "Epoch: 961, Training Loss: 0.106496, Validation Loss: 0.099332\n",
      "Epoch: 962, Training Loss: 0.106485, Validation Loss: 0.099429\n",
      "Epoch: 963, Training Loss: 0.106687, Validation Loss: 0.099343\n",
      "Epoch: 964, Training Loss: 0.108089, Validation Loss: 0.102293\n",
      "Epoch: 965, Training Loss: 0.109027, Validation Loss: 0.100639\n",
      "Epoch: 966, Training Loss: 0.108145, Validation Loss: 0.102636\n",
      "Epoch: 967, Training Loss: 0.106832, Validation Loss: 0.100334\n",
      "Epoch: 968, Training Loss: 0.108439, Validation Loss: 0.100408\n",
      "Epoch: 969, Training Loss: 0.107553, Validation Loss: 0.101601\n",
      "Epoch: 970, Training Loss: 0.106603, Validation Loss: 0.099597\n",
      "Epoch: 971, Training Loss: 0.108233, Validation Loss: 0.100044\n",
      "Epoch: 972, Training Loss: 0.107767, Validation Loss: 0.101955\n",
      "Epoch: 973, Training Loss: 0.106569, Validation Loss: 0.099896\n",
      "Epoch: 974, Training Loss: 0.108085, Validation Loss: 0.099980\n",
      "Epoch: 975, Training Loss: 0.106983, Validation Loss: 0.100711\n",
      "Epoch: 976, Training Loss: 0.106569, Validation Loss: 0.099640\n",
      "Epoch: 977, Training Loss: 0.107861, Validation Loss: 0.099593\n",
      "Epoch: 978, Training Loss: 0.106862, Validation Loss: 0.100322\n",
      "Epoch: 979, Training Loss: 0.106479, Validation Loss: 0.099989\n",
      "Epoch: 980, Training Loss: 0.107449, Validation Loss: 0.099611\n",
      "Epoch: 981, Training Loss: 0.106368, Validation Loss: 0.099734\n",
      "Epoch: 982, Training Loss: 0.106675, Validation Loss: 0.100127\n",
      "Epoch: 983, Training Loss: 0.106846, Validation Loss: 0.098845\n",
      "Epoch: 984, Training Loss: 0.106214, Validation Loss: 0.098728\n",
      "Epoch: 985, Training Loss: 0.106635, Validation Loss: 0.099909\n",
      "Epoch: 986, Training Loss: 0.106359, Validation Loss: 0.098742\n",
      "Epoch: 987, Training Loss: 0.106201, Validation Loss: 0.098886\n",
      "Epoch: 988, Training Loss: 0.106525, Validation Loss: 0.100122\n",
      "Epoch: 989, Training Loss: 0.106102, Validation Loss: 0.098715\n",
      "Epoch: 990, Training Loss: 0.106220, Validation Loss: 0.098508\n",
      "Epoch: 991, Training Loss: 0.106314, Validation Loss: 0.099334\n",
      "Epoch: 992, Training Loss: 0.106022, Validation Loss: 0.098612\n",
      "Epoch: 993, Training Loss: 0.106216, Validation Loss: 0.098502\n",
      "Epoch: 994, Training Loss: 0.106103, Validation Loss: 0.099211\n",
      "Epoch: 995, Training Loss: 0.106007, Validation Loss: 0.098990\n",
      "Epoch: 996, Training Loss: 0.106147, Validation Loss: 0.098530\n",
      "Epoch: 997, Training Loss: 0.105950, Validation Loss: 0.098753\n",
      "Epoch: 998, Training Loss: 0.106005, Validation Loss: 0.098840\n",
      "Epoch: 999, Training Loss: 0.106035, Validation Loss: 0.098258\n",
      "Epoch: 1000, Training Loss: 0.105875, Validation Loss: 0.098471\n",
      "Epoch: 1001, Training Loss: 0.105940, Validation Loss: 0.098846\n",
      "Epoch: 1002, Training Loss: 0.105894, Validation Loss: 0.098364\n",
      "Epoch: 1003, Training Loss: 0.105832, Validation Loss: 0.098423\n",
      "Epoch: 1004, Training Loss: 0.105880, Validation Loss: 0.098785\n",
      "Epoch: 1005, Training Loss: 0.105802, Validation Loss: 0.098252\n",
      "Epoch: 1006, Training Loss: 0.105794, Validation Loss: 0.098170\n",
      "Epoch: 1007, Training Loss: 0.105804, Validation Loss: 0.098495\n",
      "Epoch: 1008, Training Loss: 0.105727, Validation Loss: 0.098228\n",
      "Epoch: 1009, Training Loss: 0.105739, Validation Loss: 0.098135\n",
      "Epoch: 1010, Training Loss: 0.105706, Validation Loss: 0.098444\n",
      "Epoch: 1011, Training Loss: 0.105667, Validation Loss: 0.098307\n",
      "Epoch: 1012, Training Loss: 0.105672, Validation Loss: 0.098048\n",
      "Epoch: 1013, Training Loss: 0.105622, Validation Loss: 0.098160\n",
      "Epoch: 1014, Training Loss: 0.105607, Validation Loss: 0.098118\n",
      "Epoch: 1015, Training Loss: 0.105602, Validation Loss: 0.097865\n",
      "Epoch: 1016, Training Loss: 0.105559, Validation Loss: 0.098041\n",
      "Epoch: 1017, Training Loss: 0.105548, Validation Loss: 0.098127\n",
      "Epoch: 1018, Training Loss: 0.105534, Validation Loss: 0.097954\n",
      "Epoch: 1019, Training Loss: 0.105503, Validation Loss: 0.098003\n",
      "Epoch: 1020, Training Loss: 0.105493, Validation Loss: 0.098033\n",
      "Epoch: 1021, Training Loss: 0.105467, Validation Loss: 0.097764\n",
      "Epoch: 1022, Training Loss: 0.105441, Validation Loss: 0.097723\n",
      "Epoch: 1023, Training Loss: 0.105427, Validation Loss: 0.097786\n",
      "Epoch: 1024, Training Loss: 0.105398, Validation Loss: 0.097695\n",
      "Epoch: 1025, Training Loss: 0.105378, Validation Loss: 0.097710\n",
      "Epoch: 1026, Training Loss: 0.105359, Validation Loss: 0.097809\n",
      "Epoch: 1027, Training Loss: 0.105333, Validation Loss: 0.097693\n",
      "Epoch: 1028, Training Loss: 0.105310, Validation Loss: 0.097569\n",
      "Epoch: 1029, Training Loss: 0.105290, Validation Loss: 0.097533\n",
      "Epoch: 1030, Training Loss: 0.105268, Validation Loss: 0.097488\n",
      "Epoch: 1031, Training Loss: 0.105243, Validation Loss: 0.097439\n",
      "Epoch: 1032, Training Loss: 0.105223, Validation Loss: 0.097504\n",
      "Epoch: 1033, Training Loss: 0.105202, Validation Loss: 0.097496\n",
      "Epoch: 1034, Training Loss: 0.105176, Validation Loss: 0.097392\n",
      "Epoch: 1035, Training Loss: 0.105154, Validation Loss: 0.097315\n",
      "Epoch: 1036, Training Loss: 0.105134, Validation Loss: 0.097352\n",
      "Epoch: 1037, Training Loss: 0.105107, Validation Loss: 0.097218\n",
      "Epoch: 1038, Training Loss: 0.105083, Validation Loss: 0.097228\n",
      "Epoch: 1039, Training Loss: 0.105058, Validation Loss: 0.097221\n",
      "Epoch: 1040, Training Loss: 0.105039, Validation Loss: 0.097074\n",
      "Epoch: 1041, Training Loss: 0.105010, Validation Loss: 0.097140\n",
      "Epoch: 1042, Training Loss: 0.104983, Validation Loss: 0.097069\n",
      "Epoch: 1043, Training Loss: 0.104961, Validation Loss: 0.097005\n",
      "Epoch: 1044, Training Loss: 0.104935, Validation Loss: 0.097030\n",
      "Epoch: 1045, Training Loss: 0.104909, Validation Loss: 0.096924\n",
      "Epoch: 1046, Training Loss: 0.104883, Validation Loss: 0.096911\n",
      "Epoch: 1047, Training Loss: 0.104858, Validation Loss: 0.096853\n",
      "Epoch: 1048, Training Loss: 0.104831, Validation Loss: 0.096810\n",
      "Epoch: 1049, Training Loss: 0.104806, Validation Loss: 0.096750\n",
      "Epoch: 1050, Training Loss: 0.104779, Validation Loss: 0.096751\n",
      "Epoch: 1051, Training Loss: 0.104754, Validation Loss: 0.096661\n",
      "Epoch: 1052, Training Loss: 0.104727, Validation Loss: 0.096699\n",
      "Epoch: 1053, Training Loss: 0.104699, Validation Loss: 0.096622\n",
      "Epoch: 1054, Training Loss: 0.104673, Validation Loss: 0.096519\n",
      "Epoch: 1055, Training Loss: 0.104647, Validation Loss: 0.096562\n",
      "Epoch: 1056, Training Loss: 0.104617, Validation Loss: 0.096431\n",
      "Epoch: 1057, Training Loss: 0.104588, Validation Loss: 0.096420\n",
      "Epoch: 1058, Training Loss: 0.104560, Validation Loss: 0.096401\n",
      "Epoch: 1059, Training Loss: 0.104532, Validation Loss: 0.096288\n",
      "Epoch: 1060, Training Loss: 0.104502, Validation Loss: 0.096285\n",
      "Epoch: 1061, Training Loss: 0.104472, Validation Loss: 0.096260\n",
      "Epoch: 1062, Training Loss: 0.104444, Validation Loss: 0.096193\n",
      "Epoch: 1063, Training Loss: 0.104413, Validation Loss: 0.096130\n",
      "Epoch: 1064, Training Loss: 0.104382, Validation Loss: 0.096058\n",
      "Epoch: 1065, Training Loss: 0.104354, Validation Loss: 0.095955\n",
      "Epoch: 1066, Training Loss: 0.104321, Validation Loss: 0.095962\n",
      "Epoch: 1067, Training Loss: 0.104289, Validation Loss: 0.095947\n",
      "Epoch: 1068, Training Loss: 0.104261, Validation Loss: 0.095934\n",
      "Epoch: 1069, Training Loss: 0.104226, Validation Loss: 0.095801\n",
      "Epoch: 1070, Training Loss: 0.104195, Validation Loss: 0.095748\n",
      "Epoch: 1071, Training Loss: 0.104162, Validation Loss: 0.095679\n",
      "Epoch: 1072, Training Loss: 0.104126, Validation Loss: 0.095619\n",
      "Epoch: 1073, Training Loss: 0.104094, Validation Loss: 0.095625\n",
      "Epoch: 1074, Training Loss: 0.104057, Validation Loss: 0.095541\n",
      "Epoch: 1075, Training Loss: 0.104024, Validation Loss: 0.095437\n",
      "Epoch: 1076, Training Loss: 0.103989, Validation Loss: 0.095403\n",
      "Epoch: 1077, Training Loss: 0.103950, Validation Loss: 0.095361\n",
      "Epoch: 1078, Training Loss: 0.103918, Validation Loss: 0.095348\n",
      "Epoch: 1079, Training Loss: 0.103878, Validation Loss: 0.095222\n",
      "Epoch: 1080, Training Loss: 0.103842, Validation Loss: 0.095169\n",
      "Epoch: 1081, Training Loss: 0.103806, Validation Loss: 0.095111\n",
      "Epoch: 1082, Training Loss: 0.103769, Validation Loss: 0.095034\n",
      "Epoch: 1083, Training Loss: 0.103729, Validation Loss: 0.095107\n",
      "Epoch: 1084, Training Loss: 0.103691, Validation Loss: 0.095039\n",
      "Epoch: 1085, Training Loss: 0.103655, Validation Loss: 0.094829\n",
      "Epoch: 1086, Training Loss: 0.103616, Validation Loss: 0.094847\n",
      "Epoch: 1087, Training Loss: 0.103570, Validation Loss: 0.094687\n",
      "Epoch: 1088, Training Loss: 0.103531, Validation Loss: 0.094592\n",
      "Epoch: 1089, Training Loss: 0.103494, Validation Loss: 0.094687\n",
      "Epoch: 1090, Training Loss: 0.103447, Validation Loss: 0.094512\n",
      "Epoch: 1091, Training Loss: 0.103415, Validation Loss: 0.094304\n",
      "Epoch: 1092, Training Loss: 0.103391, Validation Loss: 0.094566\n",
      "Epoch: 1093, Training Loss: 0.103338, Validation Loss: 0.094154\n",
      "Epoch: 1094, Training Loss: 0.103284, Validation Loss: 0.094371\n",
      "Epoch: 1095, Training Loss: 0.103241, Validation Loss: 0.094287\n",
      "Epoch: 1096, Training Loss: 0.103228, Validation Loss: 0.093807\n",
      "Epoch: 1097, Training Loss: 0.103178, Validation Loss: 0.094119\n",
      "Epoch: 1098, Training Loss: 0.103093, Validation Loss: 0.093901\n",
      "Epoch: 1099, Training Loss: 0.103068, Validation Loss: 0.093852\n",
      "Epoch: 1100, Training Loss: 0.103014, Validation Loss: 0.094009\n",
      "Epoch: 1101, Training Loss: 0.102950, Validation Loss: 0.093699\n",
      "Epoch: 1102, Training Loss: 0.102949, Validation Loss: 0.092971\n",
      "Epoch: 1103, Training Loss: 0.103031, Validation Loss: 0.094212\n",
      "Epoch: 1104, Training Loss: 0.103237, Validation Loss: 0.093414\n",
      "Epoch: 1105, Training Loss: 0.103142, Validation Loss: 0.094875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1106, Training Loss: 0.102723, Validation Loss: 0.093398\n",
      "Epoch: 1107, Training Loss: 0.102896, Validation Loss: 0.092614\n",
      "Epoch: 1108, Training Loss: 0.103072, Validation Loss: 0.093997\n",
      "Epoch: 1109, Training Loss: 0.102713, Validation Loss: 0.092399\n",
      "Epoch: 1110, Training Loss: 0.102507, Validation Loss: 0.092993\n",
      "Epoch: 1111, Training Loss: 0.102653, Validation Loss: 0.093946\n",
      "Epoch: 1112, Training Loss: 0.102575, Validation Loss: 0.092722\n",
      "Epoch: 1113, Training Loss: 0.102363, Validation Loss: 0.092843\n",
      "Epoch: 1114, Training Loss: 0.102355, Validation Loss: 0.092722\n",
      "Epoch: 1115, Training Loss: 0.102367, Validation Loss: 0.092035\n",
      "Epoch: 1116, Training Loss: 0.102279, Validation Loss: 0.092932\n",
      "Epoch: 1117, Training Loss: 0.102144, Validation Loss: 0.092435\n",
      "Epoch: 1118, Training Loss: 0.102151, Validation Loss: 0.091796\n",
      "Epoch: 1119, Training Loss: 0.102170, Validation Loss: 0.092558\n",
      "Epoch: 1120, Training Loss: 0.101976, Validation Loss: 0.091764\n",
      "Epoch: 1121, Training Loss: 0.101904, Validation Loss: 0.091977\n",
      "Epoch: 1122, Training Loss: 0.101946, Validation Loss: 0.092563\n",
      "Epoch: 1123, Training Loss: 0.101835, Validation Loss: 0.091380\n",
      "Epoch: 1124, Training Loss: 0.101713, Validation Loss: 0.091481\n",
      "Epoch: 1125, Training Loss: 0.101653, Validation Loss: 0.091349\n",
      "Epoch: 1126, Training Loss: 0.101638, Validation Loss: 0.090960\n",
      "Epoch: 1127, Training Loss: 0.101645, Validation Loss: 0.092059\n",
      "Epoch: 1128, Training Loss: 0.101493, Validation Loss: 0.091211\n",
      "Epoch: 1129, Training Loss: 0.101393, Validation Loss: 0.090749\n",
      "Epoch: 1130, Training Loss: 0.101450, Validation Loss: 0.091205\n",
      "Epoch: 1131, Training Loss: 0.101372, Validation Loss: 0.090174\n",
      "Epoch: 1132, Training Loss: 0.101205, Validation Loss: 0.091012\n",
      "Epoch: 1133, Training Loss: 0.101159, Validation Loss: 0.091021\n",
      "Epoch: 1134, Training Loss: 0.101143, Validation Loss: 0.090015\n",
      "Epoch: 1135, Training Loss: 0.101088, Validation Loss: 0.090639\n",
      "Epoch: 1136, Training Loss: 0.100918, Validation Loss: 0.089692\n",
      "Epoch: 1137, Training Loss: 0.100819, Validation Loss: 0.090026\n",
      "Epoch: 1138, Training Loss: 0.100781, Validation Loss: 0.090305\n",
      "Epoch: 1139, Training Loss: 0.100677, Validation Loss: 0.089573\n",
      "Epoch: 1140, Training Loss: 0.100590, Validation Loss: 0.089374\n",
      "Epoch: 1141, Training Loss: 0.100521, Validation Loss: 0.089438\n",
      "Epoch: 1142, Training Loss: 0.100451, Validation Loss: 0.089064\n",
      "Epoch: 1143, Training Loss: 0.100360, Validation Loss: 0.089403\n",
      "Epoch: 1144, Training Loss: 0.100263, Validation Loss: 0.088952\n",
      "Epoch: 1145, Training Loss: 0.100197, Validation Loss: 0.088601\n",
      "Epoch: 1146, Training Loss: 0.100702, Validation Loss: 0.087908\n",
      "Epoch: 1147, Training Loss: 0.105518, Validation Loss: 0.097965\n",
      "Epoch: 1148, Training Loss: 0.106676, Validation Loss: 0.092415\n",
      "Epoch: 1149, Training Loss: 0.101625, Validation Loss: 0.093213\n",
      "Epoch: 1150, Training Loss: 0.101588, Validation Loss: 0.092993\n",
      "Epoch: 1151, Training Loss: 0.103762, Validation Loss: 0.089803\n",
      "Epoch: 1152, Training Loss: 0.100473, Validation Loss: 0.088500\n",
      "Epoch: 1153, Training Loss: 0.100696, Validation Loss: 0.088123\n",
      "Epoch: 1154, Training Loss: 0.102273, Validation Loss: 0.087730\n",
      "Epoch: 1155, Training Loss: 0.101043, Validation Loss: 0.092764\n",
      "Epoch: 1156, Training Loss: 0.100270, Validation Loss: 0.091647\n",
      "Epoch: 1157, Training Loss: 0.100918, Validation Loss: 0.088304\n",
      "Epoch: 1158, Training Loss: 0.099876, Validation Loss: 0.086772\n",
      "Epoch: 1159, Training Loss: 0.099950, Validation Loss: 0.084553\n",
      "Epoch: 1160, Training Loss: 0.099883, Validation Loss: 0.084689\n",
      "Epoch: 1161, Training Loss: 0.099625, Validation Loss: 0.089892\n",
      "Epoch: 1162, Training Loss: 0.099543, Validation Loss: 0.090104\n",
      "Epoch: 1163, Training Loss: 0.099231, Validation Loss: 0.086784\n",
      "Epoch: 1164, Training Loss: 0.099026, Validation Loss: 0.085411\n",
      "Epoch: 1165, Training Loss: 0.098957, Validation Loss: 0.083713\n",
      "Epoch: 1166, Training Loss: 0.098672, Validation Loss: 0.084135\n",
      "Epoch: 1167, Training Loss: 0.098668, Validation Loss: 0.087623\n",
      "Epoch: 1168, Training Loss: 0.098495, Validation Loss: 0.087410\n",
      "Epoch: 1169, Training Loss: 0.098269, Validation Loss: 0.084567\n",
      "Epoch: 1170, Training Loss: 0.098307, Validation Loss: 0.084018\n",
      "Epoch: 1171, Training Loss: 0.097968, Validation Loss: 0.083562\n",
      "Epoch: 1172, Training Loss: 0.097873, Validation Loss: 0.084280\n",
      "Epoch: 1173, Training Loss: 0.097955, Validation Loss: 0.086188\n",
      "Epoch: 1174, Training Loss: 0.097559, Validation Loss: 0.084680\n",
      "Epoch: 1175, Training Loss: 0.097578, Validation Loss: 0.081953\n",
      "Epoch: 1176, Training Loss: 0.097478, Validation Loss: 0.082693\n",
      "Epoch: 1177, Training Loss: 0.097073, Validation Loss: 0.083307\n",
      "Epoch: 1178, Training Loss: 0.097286, Validation Loss: 0.083668\n",
      "Epoch: 1179, Training Loss: 0.097053, Validation Loss: 0.084308\n",
      "Epoch: 1180, Training Loss: 0.096702, Validation Loss: 0.081795\n",
      "Epoch: 1181, Training Loss: 0.096907, Validation Loss: 0.080279\n",
      "Epoch: 1182, Training Loss: 0.096669, Validation Loss: 0.082387\n",
      "Epoch: 1183, Training Loss: 0.096409, Validation Loss: 0.082350\n",
      "Epoch: 1184, Training Loss: 0.096355, Validation Loss: 0.081828\n",
      "Epoch: 1185, Training Loss: 0.096266, Validation Loss: 0.081929\n",
      "Epoch: 1186, Training Loss: 0.096027, Validation Loss: 0.079650\n",
      "Epoch: 1187, Training Loss: 0.095784, Validation Loss: 0.080055\n",
      "Epoch: 1188, Training Loss: 0.095765, Validation Loss: 0.081472\n",
      "Epoch: 1189, Training Loss: 0.095554, Validation Loss: 0.080118\n",
      "Epoch: 1190, Training Loss: 0.095339, Validation Loss: 0.079313\n",
      "Epoch: 1191, Training Loss: 0.095279, Validation Loss: 0.079142\n",
      "Epoch: 1192, Training Loss: 0.095088, Validation Loss: 0.078552\n",
      "Epoch: 1193, Training Loss: 0.094933, Validation Loss: 0.079561\n",
      "Epoch: 1194, Training Loss: 0.094799, Validation Loss: 0.079435\n",
      "Epoch: 1195, Training Loss: 0.094650, Validation Loss: 0.077588\n",
      "Epoch: 1196, Training Loss: 0.094519, Validation Loss: 0.077221\n",
      "Epoch: 1197, Training Loss: 0.094322, Validation Loss: 0.077468\n",
      "Epoch: 1198, Training Loss: 0.094211, Validation Loss: 0.077594\n",
      "Epoch: 1199, Training Loss: 0.094044, Validation Loss: 0.078070\n",
      "Epoch: 1200, Training Loss: 0.093838, Validation Loss: 0.076943\n",
      "Epoch: 1201, Training Loss: 0.093722, Validation Loss: 0.075623\n",
      "Epoch: 1202, Training Loss: 0.093528, Validation Loss: 0.075906\n",
      "Epoch: 1203, Training Loss: 0.093335, Validation Loss: 0.076255\n",
      "Epoch: 1204, Training Loss: 0.093198, Validation Loss: 0.075970\n",
      "Epoch: 1205, Training Loss: 0.092992, Validation Loss: 0.075771\n",
      "Epoch: 1206, Training Loss: 0.092799, Validation Loss: 0.074520\n",
      "Epoch: 1207, Training Loss: 0.092626, Validation Loss: 0.073939\n",
      "Epoch: 1208, Training Loss: 0.092420, Validation Loss: 0.074583\n",
      "Epoch: 1209, Training Loss: 0.092228, Validation Loss: 0.074427\n",
      "Epoch: 1210, Training Loss: 0.092025, Validation Loss: 0.073580\n",
      "Epoch: 1211, Training Loss: 0.091840, Validation Loss: 0.073158\n",
      "Epoch: 1212, Training Loss: 0.091622, Validation Loss: 0.072451\n",
      "Epoch: 1213, Training Loss: 0.091410, Validation Loss: 0.072573\n",
      "Epoch: 1214, Training Loss: 0.091207, Validation Loss: 0.072770\n",
      "Epoch: 1215, Training Loss: 0.091003, Validation Loss: 0.071178\n",
      "Epoch: 1216, Training Loss: 0.090782, Validation Loss: 0.071122\n",
      "Epoch: 1217, Training Loss: 0.090542, Validation Loss: 0.071207\n",
      "Epoch: 1218, Training Loss: 0.090350, Validation Loss: 0.070794\n",
      "Epoch: 1219, Training Loss: 0.090150, Validation Loss: 0.070894\n",
      "Epoch: 1220, Training Loss: 0.089956, Validation Loss: 0.068890\n",
      "Epoch: 1221, Training Loss: 0.089678, Validation Loss: 0.069608\n",
      "Epoch: 1222, Training Loss: 0.089492, Validation Loss: 0.069856\n",
      "Epoch: 1223, Training Loss: 0.089341, Validation Loss: 0.068194\n",
      "Epoch: 1224, Training Loss: 0.089217, Validation Loss: 0.069282\n",
      "Epoch: 1225, Training Loss: 0.088987, Validation Loss: 0.067069\n",
      "Epoch: 1226, Training Loss: 0.088718, Validation Loss: 0.068643\n",
      "Epoch: 1227, Training Loss: 0.088488, Validation Loss: 0.067776\n",
      "Epoch: 1228, Training Loss: 0.088474, Validation Loss: 0.065648\n",
      "Epoch: 1229, Training Loss: 0.088374, Validation Loss: 0.068136\n",
      "Epoch: 1230, Training Loss: 0.088075, Validation Loss: 0.065946\n",
      "Epoch: 1231, Training Loss: 0.087794, Validation Loss: 0.066701\n",
      "Epoch: 1232, Training Loss: 0.087618, Validation Loss: 0.065436\n",
      "Epoch: 1233, Training Loss: 0.087513, Validation Loss: 0.064382\n",
      "Epoch: 1234, Training Loss: 0.087633, Validation Loss: 0.067747\n",
      "Epoch: 1235, Training Loss: 0.087459, Validation Loss: 0.063705\n",
      "Epoch: 1236, Training Loss: 0.087237, Validation Loss: 0.066454\n",
      "Epoch: 1237, Training Loss: 0.086786, Validation Loss: 0.064003\n",
      "Epoch: 1238, Training Loss: 0.086582, Validation Loss: 0.064586\n",
      "Epoch: 1239, Training Loss: 0.086623, Validation Loss: 0.065571\n",
      "Epoch: 1240, Training Loss: 0.087201, Validation Loss: 0.061527\n",
      "Epoch: 1241, Training Loss: 0.087428, Validation Loss: 0.068259\n",
      "Epoch: 1242, Training Loss: 0.086534, Validation Loss: 0.062435\n",
      "Epoch: 1243, Training Loss: 0.085894, Validation Loss: 0.062198\n",
      "Epoch: 1244, Training Loss: 0.086270, Validation Loss: 0.064339\n",
      "Epoch: 1245, Training Loss: 0.087008, Validation Loss: 0.060555\n",
      "Epoch: 1246, Training Loss: 0.087536, Validation Loss: 0.069095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1247, Training Loss: 0.085496, Validation Loss: 0.061177\n",
      "Epoch: 1248, Training Loss: 0.085766, Validation Loss: 0.059939\n",
      "Epoch: 1249, Training Loss: 0.086883, Validation Loss: 0.067675\n",
      "Epoch: 1250, Training Loss: 0.085982, Validation Loss: 0.062370\n",
      "Epoch: 1251, Training Loss: 0.084952, Validation Loss: 0.060512\n",
      "Epoch: 1252, Training Loss: 0.085883, Validation Loss: 0.062696\n",
      "Epoch: 1253, Training Loss: 0.085943, Validation Loss: 0.059671\n",
      "Epoch: 1254, Training Loss: 0.086622, Validation Loss: 0.068523\n",
      "Epoch: 1255, Training Loss: 0.084855, Validation Loss: 0.061515\n",
      "Epoch: 1256, Training Loss: 0.086162, Validation Loss: 0.059836\n",
      "Epoch: 1257, Training Loss: 0.085536, Validation Loss: 0.066166\n",
      "Epoch: 1258, Training Loss: 0.085197, Validation Loss: 0.062353\n",
      "Epoch: 1259, Training Loss: 0.084991, Validation Loss: 0.057874\n",
      "Epoch: 1260, Training Loss: 0.086910, Validation Loss: 0.065946\n",
      "Epoch: 1261, Training Loss: 0.086205, Validation Loss: 0.059459\n",
      "Epoch: 1262, Training Loss: 0.084103, Validation Loss: 0.061528\n",
      "Epoch: 1263, Training Loss: 0.084844, Validation Loss: 0.061681\n",
      "Epoch: 1264, Training Loss: 0.086009, Validation Loss: 0.056847\n",
      "Epoch: 1265, Training Loss: 0.086506, Validation Loss: 0.067974\n",
      "Epoch: 1266, Training Loss: 0.083689, Validation Loss: 0.058250\n",
      "Epoch: 1267, Training Loss: 0.085025, Validation Loss: 0.056393\n",
      "Epoch: 1268, Training Loss: 0.085904, Validation Loss: 0.066685\n",
      "Epoch: 1269, Training Loss: 0.084677, Validation Loss: 0.060085\n",
      "Epoch: 1270, Training Loss: 0.083539, Validation Loss: 0.057781\n",
      "Epoch: 1271, Training Loss: 0.085150, Validation Loss: 0.062882\n",
      "Epoch: 1272, Training Loss: 0.084406, Validation Loss: 0.058730\n",
      "Epoch: 1273, Training Loss: 0.083277, Validation Loss: 0.060610\n",
      "Epoch: 1274, Training Loss: 0.083923, Validation Loss: 0.061511\n",
      "Epoch: 1275, Training Loss: 0.084109, Validation Loss: 0.057578\n",
      "Epoch: 1276, Training Loss: 0.083577, Validation Loss: 0.063279\n",
      "Epoch: 1277, Training Loss: 0.082844, Validation Loss: 0.058160\n",
      "Epoch: 1278, Training Loss: 0.083275, Validation Loss: 0.056591\n",
      "Epoch: 1279, Training Loss: 0.083331, Validation Loss: 0.061924\n",
      "Epoch: 1280, Training Loss: 0.082952, Validation Loss: 0.058867\n",
      "Epoch: 1281, Training Loss: 0.082713, Validation Loss: 0.058912\n",
      "Epoch: 1282, Training Loss: 0.082883, Validation Loss: 0.061556\n",
      "Epoch: 1283, Training Loss: 0.083284, Validation Loss: 0.057572\n",
      "Epoch: 1284, Training Loss: 0.083009, Validation Loss: 0.061497\n",
      "Epoch: 1285, Training Loss: 0.082514, Validation Loss: 0.057044\n",
      "Epoch: 1286, Training Loss: 0.082235, Validation Loss: 0.058423\n",
      "Epoch: 1287, Training Loss: 0.083027, Validation Loss: 0.063698\n",
      "Epoch: 1288, Training Loss: 0.084203, Validation Loss: 0.056568\n",
      "Epoch: 1289, Training Loss: 0.084006, Validation Loss: 0.063969\n",
      "Epoch: 1290, Training Loss: 0.082926, Validation Loss: 0.056951\n",
      "Epoch: 1291, Training Loss: 0.081980, Validation Loss: 0.057432\n",
      "Epoch: 1292, Training Loss: 0.083158, Validation Loss: 0.063229\n",
      "Epoch: 1293, Training Loss: 0.085226, Validation Loss: 0.056832\n",
      "Epoch: 1294, Training Loss: 0.085608, Validation Loss: 0.068100\n",
      "Epoch: 1295, Training Loss: 0.083985, Validation Loss: 0.054970\n",
      "Epoch: 1296, Training Loss: 0.082040, Validation Loss: 0.057494\n",
      "Epoch: 1297, Training Loss: 0.082366, Validation Loss: 0.061133\n",
      "Epoch: 1298, Training Loss: 0.084087, Validation Loss: 0.056346\n",
      "Epoch: 1299, Training Loss: 0.084312, Validation Loss: 0.065484\n",
      "Epoch: 1300, Training Loss: 0.082480, Validation Loss: 0.055343\n",
      "Epoch: 1301, Training Loss: 0.081671, Validation Loss: 0.056707\n",
      "Epoch: 1302, Training Loss: 0.082212, Validation Loss: 0.059416\n",
      "Epoch: 1303, Training Loss: 0.083052, Validation Loss: 0.055431\n",
      "Epoch: 1304, Training Loss: 0.083349, Validation Loss: 0.064626\n",
      "Epoch: 1305, Training Loss: 0.082073, Validation Loss: 0.055878\n",
      "Epoch: 1306, Training Loss: 0.081346, Validation Loss: 0.058051\n",
      "Epoch: 1307, Training Loss: 0.081265, Validation Loss: 0.058336\n",
      "Epoch: 1308, Training Loss: 0.081752, Validation Loss: 0.056231\n",
      "Epoch: 1309, Training Loss: 0.082628, Validation Loss: 0.063513\n",
      "Epoch: 1310, Training Loss: 0.083077, Validation Loss: 0.055929\n",
      "Epoch: 1311, Training Loss: 0.082936, Validation Loss: 0.063265\n",
      "Epoch: 1312, Training Loss: 0.081859, Validation Loss: 0.055352\n",
      "Epoch: 1313, Training Loss: 0.080999, Validation Loss: 0.058268\n",
      "Epoch: 1314, Training Loss: 0.081030, Validation Loss: 0.058981\n",
      "Epoch: 1315, Training Loss: 0.081429, Validation Loss: 0.055942\n",
      "Epoch: 1316, Training Loss: 0.081689, Validation Loss: 0.060332\n",
      "Epoch: 1317, Training Loss: 0.081360, Validation Loss: 0.055151\n",
      "Epoch: 1318, Training Loss: 0.080924, Validation Loss: 0.059240\n",
      "Epoch: 1319, Training Loss: 0.080708, Validation Loss: 0.057408\n",
      "Epoch: 1320, Training Loss: 0.080616, Validation Loss: 0.057768\n",
      "Epoch: 1321, Training Loss: 0.080648, Validation Loss: 0.058619\n",
      "Epoch: 1322, Training Loss: 0.080684, Validation Loss: 0.056356\n",
      "Epoch: 1323, Training Loss: 0.080699, Validation Loss: 0.058505\n",
      "Epoch: 1324, Training Loss: 0.080657, Validation Loss: 0.056526\n",
      "Epoch: 1325, Training Loss: 0.080549, Validation Loss: 0.058529\n",
      "Epoch: 1326, Training Loss: 0.080604, Validation Loss: 0.055975\n",
      "Epoch: 1327, Training Loss: 0.080571, Validation Loss: 0.058480\n",
      "Epoch: 1328, Training Loss: 0.080503, Validation Loss: 0.056879\n",
      "Epoch: 1329, Training Loss: 0.080512, Validation Loss: 0.058981\n",
      "Epoch: 1330, Training Loss: 0.080567, Validation Loss: 0.055936\n",
      "Epoch: 1331, Training Loss: 0.080818, Validation Loss: 0.059240\n",
      "Epoch: 1332, Training Loss: 0.081360, Validation Loss: 0.054937\n",
      "Epoch: 1333, Training Loss: 0.082406, Validation Loss: 0.063178\n",
      "Epoch: 1334, Training Loss: 0.084081, Validation Loss: 0.056323\n",
      "Epoch: 1335, Training Loss: 0.086504, Validation Loss: 0.070281\n",
      "Epoch: 1336, Training Loss: 0.085850, Validation Loss: 0.056428\n",
      "Epoch: 1337, Training Loss: 0.083597, Validation Loss: 0.064164\n",
      "Epoch: 1338, Training Loss: 0.080955, Validation Loss: 0.055005\n",
      "Epoch: 1339, Training Loss: 0.080589, Validation Loss: 0.054867\n",
      "Epoch: 1340, Training Loss: 0.082235, Validation Loss: 0.060999\n",
      "Epoch: 1341, Training Loss: 0.082963, Validation Loss: 0.054614\n",
      "Epoch: 1342, Training Loss: 0.082561, Validation Loss: 0.062804\n",
      "Epoch: 1343, Training Loss: 0.081266, Validation Loss: 0.054745\n",
      "Epoch: 1344, Training Loss: 0.080195, Validation Loss: 0.058092\n",
      "Epoch: 1345, Training Loss: 0.080221, Validation Loss: 0.059200\n",
      "Epoch: 1346, Training Loss: 0.081396, Validation Loss: 0.056745\n",
      "Epoch: 1347, Training Loss: 0.083169, Validation Loss: 0.065927\n",
      "Epoch: 1348, Training Loss: 0.084494, Validation Loss: 0.057580\n",
      "Epoch: 1349, Training Loss: 0.085176, Validation Loss: 0.068380\n",
      "Epoch: 1350, Training Loss: 0.083206, Validation Loss: 0.055932\n",
      "Epoch: 1351, Training Loss: 0.080879, Validation Loss: 0.059986\n",
      "Epoch: 1352, Training Loss: 0.080119, Validation Loss: 0.056792\n",
      "Epoch: 1353, Training Loss: 0.081127, Validation Loss: 0.053952\n",
      "Epoch: 1354, Training Loss: 0.082286, Validation Loss: 0.061296\n",
      "Epoch: 1355, Training Loss: 0.081629, Validation Loss: 0.054391\n",
      "Epoch: 1356, Training Loss: 0.080461, Validation Loss: 0.058544\n",
      "Epoch: 1357, Training Loss: 0.079923, Validation Loss: 0.056337\n",
      "Epoch: 1358, Training Loss: 0.080265, Validation Loss: 0.055532\n",
      "Epoch: 1359, Training Loss: 0.080901, Validation Loss: 0.060455\n",
      "Epoch: 1360, Training Loss: 0.081622, Validation Loss: 0.056037\n",
      "Epoch: 1361, Training Loss: 0.081910, Validation Loss: 0.063457\n",
      "Epoch: 1362, Training Loss: 0.081701, Validation Loss: 0.056797\n",
      "Epoch: 1363, Training Loss: 0.081366, Validation Loss: 0.061932\n",
      "Epoch: 1364, Training Loss: 0.080798, Validation Loss: 0.055514\n",
      "Epoch: 1365, Training Loss: 0.080228, Validation Loss: 0.058335\n",
      "Epoch: 1366, Training Loss: 0.079834, Validation Loss: 0.055274\n",
      "Epoch: 1367, Training Loss: 0.079718, Validation Loss: 0.056184\n",
      "Epoch: 1368, Training Loss: 0.079702, Validation Loss: 0.057057\n",
      "Epoch: 1369, Training Loss: 0.079791, Validation Loss: 0.055943\n",
      "Epoch: 1370, Training Loss: 0.079944, Validation Loss: 0.058549\n",
      "Epoch: 1371, Training Loss: 0.080046, Validation Loss: 0.056187\n",
      "Epoch: 1372, Training Loss: 0.080122, Validation Loss: 0.059521\n",
      "Epoch: 1373, Training Loss: 0.080154, Validation Loss: 0.055818\n",
      "Epoch: 1374, Training Loss: 0.080272, Validation Loss: 0.059289\n",
      "Epoch: 1375, Training Loss: 0.080462, Validation Loss: 0.055263\n",
      "Epoch: 1376, Training Loss: 0.080515, Validation Loss: 0.059718\n",
      "Epoch: 1377, Training Loss: 0.080282, Validation Loss: 0.055452\n",
      "Epoch: 1378, Training Loss: 0.079864, Validation Loss: 0.057987\n",
      "Epoch: 1379, Training Loss: 0.079584, Validation Loss: 0.055478\n",
      "Epoch: 1380, Training Loss: 0.079469, Validation Loss: 0.055874\n",
      "Epoch: 1381, Training Loss: 0.079565, Validation Loss: 0.057039\n",
      "Epoch: 1382, Training Loss: 0.079786, Validation Loss: 0.055734\n",
      "Epoch: 1383, Training Loss: 0.080215, Validation Loss: 0.059766\n",
      "Epoch: 1384, Training Loss: 0.080839, Validation Loss: 0.055935\n",
      "Epoch: 1385, Training Loss: 0.081867, Validation Loss: 0.062895\n",
      "Epoch: 1386, Training Loss: 0.082951, Validation Loss: 0.056540\n",
      "Epoch: 1387, Training Loss: 0.083854, Validation Loss: 0.066418\n",
      "Epoch: 1388, Training Loss: 0.083013, Validation Loss: 0.056760\n",
      "Epoch: 1389, Training Loss: 0.081548, Validation Loss: 0.061863\n",
      "Epoch: 1390, Training Loss: 0.079933, Validation Loss: 0.055114\n",
      "Epoch: 1391, Training Loss: 0.079645, Validation Loss: 0.054793\n",
      "Epoch: 1392, Training Loss: 0.080569, Validation Loss: 0.058542\n",
      "Epoch: 1393, Training Loss: 0.081265, Validation Loss: 0.054160\n",
      "Epoch: 1394, Training Loss: 0.081275, Validation Loss: 0.060777\n",
      "Epoch: 1395, Training Loss: 0.080572, Validation Loss: 0.055159\n",
      "Epoch: 1396, Training Loss: 0.079894, Validation Loss: 0.059136\n",
      "Epoch: 1397, Training Loss: 0.079402, Validation Loss: 0.056486\n",
      "Epoch: 1398, Training Loss: 0.079244, Validation Loss: 0.058325\n",
      "Epoch: 1399, Training Loss: 0.079241, Validation Loss: 0.058686\n",
      "Epoch: 1400, Training Loss: 0.079261, Validation Loss: 0.057693\n",
      "Epoch: 1401, Training Loss: 0.079352, Validation Loss: 0.058582\n",
      "Epoch: 1402, Training Loss: 0.079494, Validation Loss: 0.056006\n",
      "Epoch: 1403, Training Loss: 0.079727, Validation Loss: 0.058820\n",
      "Epoch: 1404, Training Loss: 0.079917, Validation Loss: 0.055024\n",
      "Epoch: 1405, Training Loss: 0.080035, Validation Loss: 0.058928\n",
      "Epoch: 1406, Training Loss: 0.079965, Validation Loss: 0.054795\n",
      "Epoch: 1407, Training Loss: 0.079814, Validation Loss: 0.058808\n",
      "Epoch: 1408, Training Loss: 0.079612, Validation Loss: 0.055635\n",
      "Epoch: 1409, Training Loss: 0.079529, Validation Loss: 0.059149\n",
      "Epoch: 1410, Training Loss: 0.079420, Validation Loss: 0.056301\n",
      "Epoch: 1411, Training Loss: 0.079321, Validation Loss: 0.058396\n",
      "Epoch: 1412, Training Loss: 0.079403, Validation Loss: 0.055635\n",
      "Epoch: 1413, Training Loss: 0.079630, Validation Loss: 0.058957\n",
      "Epoch: 1414, Training Loss: 0.080051, Validation Loss: 0.056019\n",
      "Epoch: 1415, Training Loss: 0.081448, Validation Loss: 0.062897\n",
      "Epoch: 1416, Training Loss: 0.082498, Validation Loss: 0.056833\n",
      "Epoch: 1417, Training Loss: 0.083161, Validation Loss: 0.065205\n",
      "Epoch: 1418, Training Loss: 0.081895, Validation Loss: 0.056038\n",
      "Epoch: 1419, Training Loss: 0.080288, Validation Loss: 0.059649\n",
      "Epoch: 1420, Training Loss: 0.079182, Validation Loss: 0.055514\n",
      "Epoch: 1421, Training Loss: 0.079327, Validation Loss: 0.055155\n",
      "Epoch: 1422, Training Loss: 0.080235, Validation Loss: 0.059416\n",
      "Epoch: 1423, Training Loss: 0.081335, Validation Loss: 0.054768\n",
      "Epoch: 1424, Training Loss: 0.082338, Validation Loss: 0.063777\n",
      "Epoch: 1425, Training Loss: 0.082473, Validation Loss: 0.056834\n",
      "Epoch: 1426, Training Loss: 0.082553, Validation Loss: 0.065118\n",
      "Epoch: 1427, Training Loss: 0.081220, Validation Loss: 0.056542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1428, Training Loss: 0.079862, Validation Loss: 0.059917\n",
      "Epoch: 1429, Training Loss: 0.079007, Validation Loss: 0.056234\n",
      "Epoch: 1430, Training Loss: 0.079024, Validation Loss: 0.056107\n",
      "Epoch: 1431, Training Loss: 0.079673, Validation Loss: 0.059676\n",
      "Epoch: 1432, Training Loss: 0.080430, Validation Loss: 0.056048\n",
      "Epoch: 1433, Training Loss: 0.080939, Validation Loss: 0.062178\n",
      "Epoch: 1434, Training Loss: 0.080945, Validation Loss: 0.056198\n",
      "Epoch: 1435, Training Loss: 0.081018, Validation Loss: 0.062405\n",
      "Epoch: 1436, Training Loss: 0.080541, Validation Loss: 0.056199\n",
      "Epoch: 1437, Training Loss: 0.080008, Validation Loss: 0.060864\n",
      "Epoch: 1438, Training Loss: 0.079283, Validation Loss: 0.056145\n",
      "Epoch: 1439, Training Loss: 0.078852, Validation Loss: 0.057340\n",
      "Epoch: 1440, Training Loss: 0.078804, Validation Loss: 0.056743\n",
      "Epoch: 1441, Training Loss: 0.079015, Validation Loss: 0.055672\n",
      "Epoch: 1442, Training Loss: 0.079358, Validation Loss: 0.059170\n",
      "Epoch: 1443, Training Loss: 0.079537, Validation Loss: 0.055999\n",
      "Epoch: 1444, Training Loss: 0.079604, Validation Loss: 0.059883\n",
      "Epoch: 1445, Training Loss: 0.079546, Validation Loss: 0.055717\n",
      "Epoch: 1446, Training Loss: 0.079536, Validation Loss: 0.059371\n",
      "Epoch: 1447, Training Loss: 0.079462, Validation Loss: 0.055736\n",
      "Epoch: 1448, Training Loss: 0.079350, Validation Loss: 0.059608\n",
      "Epoch: 1449, Training Loss: 0.079073, Validation Loss: 0.056209\n",
      "Epoch: 1450, Training Loss: 0.078823, Validation Loss: 0.057989\n",
      "Epoch: 1451, Training Loss: 0.078609, Validation Loss: 0.055966\n",
      "Epoch: 1452, Training Loss: 0.078483, Validation Loss: 0.056618\n",
      "Epoch: 1453, Training Loss: 0.078508, Validation Loss: 0.057109\n",
      "Epoch: 1454, Training Loss: 0.078644, Validation Loss: 0.056230\n",
      "Epoch: 1455, Training Loss: 0.078904, Validation Loss: 0.058404\n",
      "Epoch: 1456, Training Loss: 0.079531, Validation Loss: 0.055837\n",
      "Epoch: 1457, Training Loss: 0.081082, Validation Loss: 0.062593\n",
      "Epoch: 1458, Training Loss: 0.082945, Validation Loss: 0.057642\n",
      "Epoch: 1459, Training Loss: 0.085393, Validation Loss: 0.069900\n",
      "Epoch: 1460, Training Loss: 0.084890, Validation Loss: 0.058733\n",
      "Epoch: 1461, Training Loss: 0.082762, Validation Loss: 0.064556\n",
      "Epoch: 1462, Training Loss: 0.079829, Validation Loss: 0.054545\n",
      "Epoch: 1463, Training Loss: 0.078663, Validation Loss: 0.055280\n",
      "Epoch: 1464, Training Loss: 0.079322, Validation Loss: 0.057495\n",
      "Epoch: 1465, Training Loss: 0.080867, Validation Loss: 0.054915\n",
      "Epoch: 1466, Training Loss: 0.082677, Validation Loss: 0.065201\n",
      "Epoch: 1467, Training Loss: 0.083483, Validation Loss: 0.058066\n",
      "Epoch: 1468, Training Loss: 0.084374, Validation Loss: 0.068421\n",
      "Epoch: 1469, Training Loss: 0.083504, Validation Loss: 0.058494\n",
      "Epoch: 1470, Training Loss: 0.082411, Validation Loss: 0.065420\n",
      "Epoch: 1471, Training Loss: 0.080321, Validation Loss: 0.056615\n",
      "Epoch: 1472, Training Loss: 0.078787, Validation Loss: 0.058562\n",
      "Epoch: 1473, Training Loss: 0.078471, Validation Loss: 0.056840\n",
      "Epoch: 1474, Training Loss: 0.079240, Validation Loss: 0.054738\n",
      "Epoch: 1475, Training Loss: 0.080258, Validation Loss: 0.060706\n",
      "Epoch: 1476, Training Loss: 0.080794, Validation Loss: 0.055428\n",
      "Epoch: 1477, Training Loss: 0.080935, Validation Loss: 0.062334\n",
      "Epoch: 1478, Training Loss: 0.080191, Validation Loss: 0.055763\n",
      "Epoch: 1479, Training Loss: 0.079345, Validation Loss: 0.059656\n",
      "Epoch: 1480, Training Loss: 0.078641, Validation Loss: 0.055766\n",
      "Epoch: 1481, Training Loss: 0.078308, Validation Loss: 0.057340\n",
      "Epoch: 1482, Training Loss: 0.078384, Validation Loss: 0.058070\n",
      "Epoch: 1483, Training Loss: 0.078785, Validation Loss: 0.056355\n",
      "Epoch: 1484, Training Loss: 0.079345, Validation Loss: 0.060252\n",
      "Epoch: 1485, Training Loss: 0.079818, Validation Loss: 0.055963\n",
      "Epoch: 1486, Training Loss: 0.080274, Validation Loss: 0.061707\n",
      "Epoch: 1487, Training Loss: 0.080401, Validation Loss: 0.055999\n",
      "Epoch: 1488, Training Loss: 0.080623, Validation Loss: 0.062133\n",
      "Epoch: 1489, Training Loss: 0.080288, Validation Loss: 0.055358\n",
      "Epoch: 1490, Training Loss: 0.079729, Validation Loss: 0.060198\n",
      "Epoch: 1491, Training Loss: 0.079104, Validation Loss: 0.055274\n",
      "Epoch: 1492, Training Loss: 0.078497, Validation Loss: 0.058119\n",
      "Epoch: 1493, Training Loss: 0.078134, Validation Loss: 0.056250\n",
      "Epoch: 1494, Training Loss: 0.078030, Validation Loss: 0.057162\n",
      "Epoch: 1495, Training Loss: 0.078066, Validation Loss: 0.057418\n",
      "Epoch: 1496, Training Loss: 0.078252, Validation Loss: 0.056288\n",
      "Epoch: 1497, Training Loss: 0.078702, Validation Loss: 0.059255\n",
      "Epoch: 1498, Training Loss: 0.079614, Validation Loss: 0.056939\n",
      "Epoch: 1499, Training Loss: 0.082145, Validation Loss: 0.065845\n",
      "Epoch: 1500, Training Loss: 0.085205, Validation Loss: 0.059947\n",
      "Epoch: 1501, Training Loss: 0.088139, Validation Loss: 0.074008\n",
      "Epoch: 1502, Training Loss: 0.086409, Validation Loss: 0.060224\n",
      "Epoch: 1503, Training Loss: 0.083184, Validation Loss: 0.065779\n",
      "Epoch: 1504, Training Loss: 0.079411, Validation Loss: 0.054661\n",
      "Epoch: 1505, Training Loss: 0.078208, Validation Loss: 0.055078\n",
      "Epoch: 1506, Training Loss: 0.079474, Validation Loss: 0.058614\n",
      "Epoch: 1507, Training Loss: 0.081512, Validation Loss: 0.055339\n",
      "Epoch: 1508, Training Loss: 0.083314, Validation Loss: 0.066834\n",
      "Epoch: 1509, Training Loss: 0.082604, Validation Loss: 0.058024\n",
      "Epoch: 1510, Training Loss: 0.081317, Validation Loss: 0.064469\n",
      "Epoch: 1511, Training Loss: 0.079502, Validation Loss: 0.056467\n",
      "Epoch: 1512, Training Loss: 0.078308, Validation Loss: 0.058525\n",
      "Epoch: 1513, Training Loss: 0.077960, Validation Loss: 0.057960\n",
      "Epoch: 1514, Training Loss: 0.078626, Validation Loss: 0.057380\n",
      "Epoch: 1515, Training Loss: 0.079989, Validation Loss: 0.063109\n",
      "Epoch: 1516, Training Loss: 0.081271, Validation Loss: 0.057603\n",
      "Epoch: 1517, Training Loss: 0.082820, Validation Loss: 0.066385\n",
      "Epoch: 1518, Training Loss: 0.082600, Validation Loss: 0.056419\n",
      "Epoch: 1519, Training Loss: 0.081245, Validation Loss: 0.063045\n",
      "Epoch: 1520, Training Loss: 0.079113, Validation Loss: 0.055154\n",
      "Epoch: 1521, Training Loss: 0.078033, Validation Loss: 0.055581\n",
      "Epoch: 1522, Training Loss: 0.078710, Validation Loss: 0.057573\n",
      "Epoch: 1523, Training Loss: 0.079933, Validation Loss: 0.054907\n",
      "Epoch: 1524, Training Loss: 0.080536, Validation Loss: 0.062485\n",
      "Epoch: 1525, Training Loss: 0.080331, Validation Loss: 0.056783\n",
      "Epoch: 1526, Training Loss: 0.079788, Validation Loss: 0.062653\n",
      "Epoch: 1527, Training Loss: 0.078769, Validation Loss: 0.056805\n",
      "Epoch: 1528, Training Loss: 0.078121, Validation Loss: 0.058880\n",
      "Epoch: 1529, Training Loss: 0.077774, Validation Loss: 0.057022\n",
      "Epoch: 1530, Training Loss: 0.077688, Validation Loss: 0.057334\n",
      "Epoch: 1531, Training Loss: 0.077820, Validation Loss: 0.058354\n",
      "Epoch: 1532, Training Loss: 0.078058, Validation Loss: 0.056703\n",
      "Epoch: 1533, Training Loss: 0.078509, Validation Loss: 0.060035\n",
      "Epoch: 1534, Training Loss: 0.078889, Validation Loss: 0.056279\n",
      "Epoch: 1535, Training Loss: 0.079376, Validation Loss: 0.061378\n",
      "Epoch: 1536, Training Loss: 0.079441, Validation Loss: 0.056092\n",
      "Epoch: 1537, Training Loss: 0.079258, Validation Loss: 0.060629\n",
      "Epoch: 1538, Training Loss: 0.078749, Validation Loss: 0.055179\n",
      "Epoch: 1539, Training Loss: 0.078206, Validation Loss: 0.058232\n",
      "Epoch: 1540, Training Loss: 0.077716, Validation Loss: 0.055648\n",
      "Epoch: 1541, Training Loss: 0.077560, Validation Loss: 0.056742\n",
      "Epoch: 1542, Training Loss: 0.077729, Validation Loss: 0.058273\n",
      "Epoch: 1543, Training Loss: 0.078094, Validation Loss: 0.056271\n",
      "Epoch: 1544, Training Loss: 0.078761, Validation Loss: 0.060542\n",
      "Epoch: 1545, Training Loss: 0.079825, Validation Loss: 0.056803\n",
      "Epoch: 1546, Training Loss: 0.081562, Validation Loss: 0.065389\n",
      "Epoch: 1547, Training Loss: 0.083269, Validation Loss: 0.058689\n",
      "Epoch: 1548, Training Loss: 0.085437, Validation Loss: 0.070963\n",
      "Epoch: 1549, Training Loss: 0.085371, Validation Loss: 0.059181\n",
      "Epoch: 1550, Training Loss: 0.084369, Validation Loss: 0.068067\n",
      "Epoch: 1551, Training Loss: 0.081399, Validation Loss: 0.055583\n",
      "Epoch: 1552, Training Loss: 0.078820, Validation Loss: 0.058733\n",
      "Epoch: 1553, Training Loss: 0.077598, Validation Loss: 0.055487\n",
      "Epoch: 1554, Training Loss: 0.078122, Validation Loss: 0.055534\n",
      "Epoch: 1555, Training Loss: 0.079747, Validation Loss: 0.062182\n",
      "Epoch: 1556, Training Loss: 0.081459, Validation Loss: 0.057617\n",
      "Epoch: 1557, Training Loss: 0.083491, Validation Loss: 0.068440\n",
      "Epoch: 1558, Training Loss: 0.084205, Validation Loss: 0.059701\n",
      "Epoch: 1559, Training Loss: 0.084705, Validation Loss: 0.070083\n",
      "Epoch: 1560, Training Loss: 0.082882, Validation Loss: 0.058362\n",
      "Epoch: 1561, Training Loss: 0.080540, Validation Loss: 0.063360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1562, Training Loss: 0.078236, Validation Loss: 0.055292\n",
      "Epoch: 1563, Training Loss: 0.077460, Validation Loss: 0.055560\n",
      "Epoch: 1564, Training Loss: 0.078217, Validation Loss: 0.058158\n",
      "Epoch: 1565, Training Loss: 0.079537, Validation Loss: 0.055727\n",
      "Epoch: 1566, Training Loss: 0.080796, Validation Loss: 0.064015\n",
      "Epoch: 1567, Training Loss: 0.080893, Validation Loss: 0.057012\n",
      "Epoch: 1568, Training Loss: 0.080180, Validation Loss: 0.063449\n",
      "Epoch: 1569, Training Loss: 0.078927, Validation Loss: 0.056381\n",
      "Epoch: 1570, Training Loss: 0.077926, Validation Loss: 0.059360\n",
      "Epoch: 1571, Training Loss: 0.077360, Validation Loss: 0.056969\n",
      "Epoch: 1572, Training Loss: 0.077272, Validation Loss: 0.057531\n",
      "Epoch: 1573, Training Loss: 0.077548, Validation Loss: 0.059254\n",
      "Epoch: 1574, Training Loss: 0.078060, Validation Loss: 0.056856\n",
      "Epoch: 1575, Training Loss: 0.078883, Validation Loss: 0.061457\n",
      "Epoch: 1576, Training Loss: 0.079518, Validation Loss: 0.056063\n",
      "Epoch: 1577, Training Loss: 0.079786, Validation Loss: 0.062286\n",
      "Epoch: 1578, Training Loss: 0.079270, Validation Loss: 0.055867\n",
      "Epoch: 1579, Training Loss: 0.078354, Validation Loss: 0.059491\n",
      "Epoch: 1580, Training Loss: 0.077467, Validation Loss: 0.055440\n",
      "Epoch: 1581, Training Loss: 0.077170, Validation Loss: 0.056360\n",
      "Epoch: 1582, Training Loss: 0.077344, Validation Loss: 0.057597\n",
      "Epoch: 1583, Training Loss: 0.077782, Validation Loss: 0.056325\n",
      "Epoch: 1584, Training Loss: 0.078490, Validation Loss: 0.061551\n",
      "Epoch: 1585, Training Loss: 0.079450, Validation Loss: 0.057642\n",
      "Epoch: 1586, Training Loss: 0.081364, Validation Loss: 0.066038\n",
      "Epoch: 1587, Training Loss: 0.083032, Validation Loss: 0.058882\n",
      "Epoch: 1588, Training Loss: 0.084563, Validation Loss: 0.069777\n",
      "Epoch: 1589, Training Loss: 0.083766, Validation Loss: 0.058592\n",
      "Epoch: 1590, Training Loss: 0.081949, Validation Loss: 0.065146\n",
      "Epoch: 1591, Training Loss: 0.079282, Validation Loss: 0.054838\n",
      "Epoch: 1592, Training Loss: 0.077497, Validation Loss: 0.056890\n",
      "Epoch: 1593, Training Loss: 0.077215, Validation Loss: 0.056229\n",
      "Epoch: 1594, Training Loss: 0.078191, Validation Loss: 0.055138\n",
      "Epoch: 1595, Training Loss: 0.079734, Validation Loss: 0.062996\n",
      "Epoch: 1596, Training Loss: 0.081253, Validation Loss: 0.058463\n",
      "Epoch: 1597, Training Loss: 0.083785, Validation Loss: 0.069932\n",
      "Epoch: 1598, Training Loss: 0.084745, Validation Loss: 0.060905\n",
      "Epoch: 1599, Training Loss: 0.084975, Validation Loss: 0.070889\n",
      "Epoch: 1600, Training Loss: 0.082582, Validation Loss: 0.058411\n",
      "Epoch: 1601, Training Loss: 0.079819, Validation Loss: 0.062341\n",
      "Epoch: 1602, Training Loss: 0.077527, Validation Loss: 0.055162\n",
      "Epoch: 1603, Training Loss: 0.077071, Validation Loss: 0.055541\n",
      "Epoch: 1604, Training Loss: 0.078183, Validation Loss: 0.059280\n",
      "Epoch: 1605, Training Loss: 0.079857, Validation Loss: 0.056188\n",
      "Epoch: 1606, Training Loss: 0.081700, Validation Loss: 0.066198\n",
      "Epoch: 1607, Training Loss: 0.082151, Validation Loss: 0.058100\n",
      "Epoch: 1608, Training Loss: 0.081722, Validation Loss: 0.066361\n",
      "Epoch: 1609, Training Loss: 0.079890, Validation Loss: 0.057209\n",
      "Epoch: 1610, Training Loss: 0.078156, Validation Loss: 0.060300\n",
      "Epoch: 1611, Training Loss: 0.076995, Validation Loss: 0.056541\n",
      "Epoch: 1612, Training Loss: 0.077023, Validation Loss: 0.056304\n",
      "Epoch: 1613, Training Loss: 0.077845, Validation Loss: 0.059743\n",
      "Epoch: 1614, Training Loss: 0.079124, Validation Loss: 0.056866\n",
      "Epoch: 1615, Training Loss: 0.080923, Validation Loss: 0.065561\n",
      "Epoch: 1616, Training Loss: 0.082097, Validation Loss: 0.057835\n",
      "Epoch: 1617, Training Loss: 0.082279, Validation Loss: 0.067058\n",
      "Epoch: 1618, Training Loss: 0.080761, Validation Loss: 0.056991\n",
      "Epoch: 1619, Training Loss: 0.078824, Validation Loss: 0.060782\n",
      "Epoch: 1620, Training Loss: 0.077143, Validation Loss: 0.055721\n",
      "Epoch: 1621, Training Loss: 0.076939, Validation Loss: 0.056108\n",
      "Epoch: 1622, Training Loss: 0.077825, Validation Loss: 0.059174\n",
      "Epoch: 1623, Training Loss: 0.078776, Validation Loss: 0.056030\n",
      "Epoch: 1624, Training Loss: 0.079438, Validation Loss: 0.063139\n",
      "Epoch: 1625, Training Loss: 0.079638, Validation Loss: 0.057388\n",
      "Epoch: 1626, Training Loss: 0.080172, Validation Loss: 0.065042\n",
      "Epoch: 1627, Training Loss: 0.079977, Validation Loss: 0.057606\n",
      "Epoch: 1628, Training Loss: 0.079223, Validation Loss: 0.062125\n",
      "Epoch: 1629, Training Loss: 0.078039, Validation Loss: 0.056082\n",
      "Epoch: 1630, Training Loss: 0.077060, Validation Loss: 0.057119\n",
      "Epoch: 1631, Training Loss: 0.076804, Validation Loss: 0.056453\n",
      "Epoch: 1632, Training Loss: 0.077221, Validation Loss: 0.055646\n",
      "Epoch: 1633, Training Loss: 0.077817, Validation Loss: 0.059604\n",
      "Epoch: 1634, Training Loss: 0.078443, Validation Loss: 0.055808\n",
      "Epoch: 1635, Training Loss: 0.079130, Validation Loss: 0.062778\n",
      "Epoch: 1636, Training Loss: 0.079516, Validation Loss: 0.056998\n",
      "Epoch: 1637, Training Loss: 0.080003, Validation Loss: 0.064728\n",
      "Epoch: 1638, Training Loss: 0.079994, Validation Loss: 0.058132\n",
      "Epoch: 1639, Training Loss: 0.079710, Validation Loss: 0.064019\n",
      "Epoch: 1640, Training Loss: 0.078926, Validation Loss: 0.056543\n",
      "Epoch: 1641, Training Loss: 0.078132, Validation Loss: 0.060829\n",
      "Epoch: 1642, Training Loss: 0.077299, Validation Loss: 0.055393\n",
      "Epoch: 1643, Training Loss: 0.076696, Validation Loss: 0.057656\n",
      "Epoch: 1644, Training Loss: 0.076476, Validation Loss: 0.056801\n",
      "Epoch: 1645, Training Loss: 0.076485, Validation Loss: 0.056501\n",
      "Epoch: 1646, Training Loss: 0.076729, Validation Loss: 0.058502\n",
      "Epoch: 1647, Training Loss: 0.077153, Validation Loss: 0.056577\n",
      "Epoch: 1648, Training Loss: 0.077679, Validation Loss: 0.060864\n",
      "Epoch: 1649, Training Loss: 0.078230, Validation Loss: 0.056883\n",
      "Epoch: 1650, Training Loss: 0.078894, Validation Loss: 0.063160\n",
      "Epoch: 1651, Training Loss: 0.079435, Validation Loss: 0.057081\n",
      "Epoch: 1652, Training Loss: 0.080160, Validation Loss: 0.064566\n",
      "Epoch: 1653, Training Loss: 0.080562, Validation Loss: 0.057092\n",
      "Epoch: 1654, Training Loss: 0.081261, Validation Loss: 0.065776\n",
      "Epoch: 1655, Training Loss: 0.081289, Validation Loss: 0.056792\n",
      "Epoch: 1656, Training Loss: 0.081315, Validation Loss: 0.065771\n",
      "Epoch: 1657, Training Loss: 0.080557, Validation Loss: 0.056485\n",
      "Epoch: 1658, Training Loss: 0.079681, Validation Loss: 0.063258\n",
      "Epoch: 1659, Training Loss: 0.078512, Validation Loss: 0.056050\n",
      "Epoch: 1660, Training Loss: 0.077496, Validation Loss: 0.060298\n",
      "Epoch: 1661, Training Loss: 0.076799, Validation Loss: 0.056736\n",
      "Epoch: 1662, Training Loss: 0.076443, Validation Loss: 0.059093\n",
      "Epoch: 1663, Training Loss: 0.076282, Validation Loss: 0.057549\n",
      "Epoch: 1664, Training Loss: 0.076180, Validation Loss: 0.057952\n",
      "Epoch: 1665, Training Loss: 0.076154, Validation Loss: 0.057319\n",
      "Epoch: 1666, Training Loss: 0.076156, Validation Loss: 0.056981\n",
      "Epoch: 1667, Training Loss: 0.076155, Validation Loss: 0.057408\n",
      "Epoch: 1668, Training Loss: 0.076185, Validation Loss: 0.056561\n",
      "Epoch: 1669, Training Loss: 0.076226, Validation Loss: 0.057822\n",
      "Epoch: 1670, Training Loss: 0.076303, Validation Loss: 0.056721\n",
      "Epoch: 1671, Training Loss: 0.076450, Validation Loss: 0.058879\n",
      "Epoch: 1672, Training Loss: 0.076772, Validation Loss: 0.056318\n",
      "Epoch: 1673, Training Loss: 0.077252, Validation Loss: 0.060420\n",
      "Epoch: 1674, Training Loss: 0.077977, Validation Loss: 0.056673\n",
      "Epoch: 1675, Training Loss: 0.079387, Validation Loss: 0.064151\n",
      "Epoch: 1676, Training Loss: 0.081588, Validation Loss: 0.058529\n",
      "Epoch: 1677, Training Loss: 0.085855, Validation Loss: 0.073034\n",
      "Epoch: 1678, Training Loss: 0.091359, Validation Loss: 0.064556\n",
      "Epoch: 1679, Training Loss: 0.101121, Validation Loss: 0.091705\n",
      "Epoch: 1680, Training Loss: 0.104647, Validation Loss: 0.075056\n",
      "Epoch: 1681, Training Loss: 0.105634, Validation Loss: 0.095663\n",
      "Epoch: 1682, Training Loss: 0.093069, Validation Loss: 0.064605\n",
      "Epoch: 1683, Training Loss: 0.081608, Validation Loss: 0.062881\n",
      "Epoch: 1684, Training Loss: 0.076911, Validation Loss: 0.054791\n",
      "Epoch: 1685, Training Loss: 0.081857, Validation Loss: 0.055924\n",
      "Epoch: 1686, Training Loss: 0.089326, Validation Loss: 0.075089\n",
      "Epoch: 1687, Training Loss: 0.088160, Validation Loss: 0.062191\n",
      "Epoch: 1688, Training Loss: 0.082594, Validation Loss: 0.068755\n",
      "Epoch: 1689, Training Loss: 0.076872, Validation Loss: 0.056336\n",
      "Epoch: 1690, Training Loss: 0.077468, Validation Loss: 0.056938\n",
      "Epoch: 1691, Training Loss: 0.082776, Validation Loss: 0.069183\n",
      "Epoch: 1692, Training Loss: 0.085604, Validation Loss: 0.061631\n",
      "Epoch: 1693, Training Loss: 0.084420, Validation Loss: 0.072692\n",
      "Epoch: 1694, Training Loss: 0.079041, Validation Loss: 0.058368\n",
      "Epoch: 1695, Training Loss: 0.076960, Validation Loss: 0.057473\n",
      "Epoch: 1696, Training Loss: 0.079828, Validation Loss: 0.063479\n",
      "Epoch: 1697, Training Loss: 0.081750, Validation Loss: 0.057356\n",
      "Epoch: 1698, Training Loss: 0.080254, Validation Loss: 0.062305\n",
      "Epoch: 1699, Training Loss: 0.077622, Validation Loss: 0.056287\n",
      "Epoch: 1700, Training Loss: 0.077700, Validation Loss: 0.055316\n",
      "Epoch: 1701, Training Loss: 0.079532, Validation Loss: 0.062182\n",
      "Epoch: 1702, Training Loss: 0.079573, Validation Loss: 0.057331\n",
      "Epoch: 1703, Training Loss: 0.077833, Validation Loss: 0.058950\n",
      "Epoch: 1704, Training Loss: 0.076803, Validation Loss: 0.057492\n",
      "Epoch: 1705, Training Loss: 0.077864, Validation Loss: 0.057034\n",
      "Epoch: 1706, Training Loss: 0.078750, Validation Loss: 0.061776\n",
      "Epoch: 1707, Training Loss: 0.077706, Validation Loss: 0.058278\n",
      "Epoch: 1708, Training Loss: 0.076711, Validation Loss: 0.060819\n",
      "Epoch: 1709, Training Loss: 0.076803, Validation Loss: 0.060166\n",
      "Epoch: 1710, Training Loss: 0.077221, Validation Loss: 0.058855\n",
      "Epoch: 1711, Training Loss: 0.077384, Validation Loss: 0.061205\n",
      "Epoch: 1712, Training Loss: 0.077173, Validation Loss: 0.056628\n",
      "Epoch: 1713, Training Loss: 0.076569, Validation Loss: 0.058562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1714, Training Loss: 0.076176, Validation Loss: 0.057276\n",
      "Epoch: 1715, Training Loss: 0.076481, Validation Loss: 0.056551\n",
      "Epoch: 1716, Training Loss: 0.076846, Validation Loss: 0.060138\n",
      "Epoch: 1717, Training Loss: 0.076704, Validation Loss: 0.056432\n",
      "Epoch: 1718, Training Loss: 0.076344, Validation Loss: 0.058390\n",
      "Epoch: 1719, Training Loss: 0.076117, Validation Loss: 0.056854\n",
      "Epoch: 1720, Training Loss: 0.076022, Validation Loss: 0.056666\n",
      "Epoch: 1721, Training Loss: 0.076095, Validation Loss: 0.058250\n",
      "Epoch: 1722, Training Loss: 0.076253, Validation Loss: 0.057309\n",
      "Epoch: 1723, Training Loss: 0.076369, Validation Loss: 0.060120\n",
      "Epoch: 1724, Training Loss: 0.076253, Validation Loss: 0.057478\n",
      "Epoch: 1725, Training Loss: 0.076145, Validation Loss: 0.059750\n",
      "Epoch: 1726, Training Loss: 0.076016, Validation Loss: 0.056911\n",
      "Epoch: 1727, Training Loss: 0.075848, Validation Loss: 0.058690\n",
      "Epoch: 1728, Training Loss: 0.075697, Validation Loss: 0.057545\n",
      "Epoch: 1729, Training Loss: 0.075671, Validation Loss: 0.058511\n",
      "Epoch: 1730, Training Loss: 0.075595, Validation Loss: 0.058111\n",
      "Epoch: 1731, Training Loss: 0.075500, Validation Loss: 0.057723\n",
      "Epoch: 1732, Training Loss: 0.075483, Validation Loss: 0.057424\n",
      "Epoch: 1733, Training Loss: 0.075470, Validation Loss: 0.056903\n",
      "Epoch: 1734, Training Loss: 0.075465, Validation Loss: 0.057821\n",
      "Epoch: 1735, Training Loss: 0.075612, Validation Loss: 0.056617\n",
      "Epoch: 1736, Training Loss: 0.076029, Validation Loss: 0.059790\n",
      "Epoch: 1737, Training Loss: 0.076905, Validation Loss: 0.056730\n",
      "Epoch: 1738, Training Loss: 0.079196, Validation Loss: 0.064765\n",
      "Epoch: 1739, Training Loss: 0.084908, Validation Loss: 0.062120\n",
      "Epoch: 1740, Training Loss: 0.101128, Validation Loss: 0.093618\n",
      "Epoch: 1741, Training Loss: 0.133637, Validation Loss: 0.103026\n",
      "Epoch: 1742, Training Loss: 0.196897, Validation Loss: 0.204348\n",
      "Epoch: 1743, Training Loss: 0.234220, Validation Loss: 0.191854\n",
      "Epoch: 1744, Training Loss: 0.301779, Validation Loss: 0.314400\n",
      "Epoch: 1745, Training Loss: 0.139339, Validation Loss: 0.107010\n",
      "Epoch: 1746, Training Loss: 0.116207, Validation Loss: 0.076590\n",
      "Epoch: 1747, Training Loss: 0.154326, Validation Loss: 0.147743\n",
      "Epoch: 1748, Training Loss: 0.142259, Validation Loss: 0.114271\n",
      "Epoch: 1749, Training Loss: 0.143059, Validation Loss: 0.104152\n",
      "Epoch: 1750, Training Loss: 0.085243, Validation Loss: 0.061264\n",
      "Epoch: 1751, Training Loss: 0.098237, Validation Loss: 0.071601\n",
      "Epoch: 1752, Training Loss: 0.093493, Validation Loss: 0.068332\n",
      "Epoch: 1753, Training Loss: 0.083355, Validation Loss: 0.053257\n",
      "Epoch: 1754, Training Loss: 0.086684, Validation Loss: 0.059998\n",
      "Epoch: 1755, Training Loss: 0.095390, Validation Loss: 0.075195\n",
      "Epoch: 1756, Training Loss: 0.085137, Validation Loss: 0.058644\n",
      "Epoch: 1757, Training Loss: 0.083390, Validation Loss: 0.055657\n",
      "Epoch: 1758, Training Loss: 0.089133, Validation Loss: 0.066858\n",
      "Epoch: 1759, Training Loss: 0.088693, Validation Loss: 0.065585\n",
      "Epoch: 1760, Training Loss: 0.082007, Validation Loss: 0.058035\n",
      "Epoch: 1761, Training Loss: 0.084294, Validation Loss: 0.058846\n",
      "Epoch: 1762, Training Loss: 0.086515, Validation Loss: 0.061083\n",
      "Epoch: 1763, Training Loss: 0.084447, Validation Loss: 0.063036\n",
      "Epoch: 1764, Training Loss: 0.081279, Validation Loss: 0.056520\n",
      "Epoch: 1765, Training Loss: 0.084393, Validation Loss: 0.056539\n",
      "Epoch: 1766, Training Loss: 0.084578, Validation Loss: 0.060699\n",
      "Epoch: 1767, Training Loss: 0.081802, Validation Loss: 0.057030\n",
      "Epoch: 1768, Training Loss: 0.081251, Validation Loss: 0.054268\n",
      "Epoch: 1769, Training Loss: 0.083531, Validation Loss: 0.056634\n",
      "Epoch: 1770, Training Loss: 0.081816, Validation Loss: 0.054407\n",
      "Epoch: 1771, Training Loss: 0.080645, Validation Loss: 0.055627\n",
      "Epoch: 1772, Training Loss: 0.081693, Validation Loss: 0.056560\n",
      "Epoch: 1773, Training Loss: 0.082309, Validation Loss: 0.055864\n",
      "Epoch: 1774, Training Loss: 0.080959, Validation Loss: 0.058267\n",
      "Epoch: 1775, Training Loss: 0.080534, Validation Loss: 0.057200\n",
      "Epoch: 1776, Training Loss: 0.081490, Validation Loss: 0.054843\n",
      "Epoch: 1777, Training Loss: 0.081027, Validation Loss: 0.056387\n",
      "Epoch: 1778, Training Loss: 0.080269, Validation Loss: 0.055957\n",
      "Epoch: 1779, Training Loss: 0.080316, Validation Loss: 0.055104\n",
      "Epoch: 1780, Training Loss: 0.080789, Validation Loss: 0.056210\n",
      "Epoch: 1781, Training Loss: 0.080006, Validation Loss: 0.055552\n",
      "Epoch: 1782, Training Loss: 0.079838, Validation Loss: 0.055902\n",
      "Epoch: 1783, Training Loss: 0.080066, Validation Loss: 0.055241\n",
      "Epoch: 1784, Training Loss: 0.079924, Validation Loss: 0.053252\n",
      "Epoch: 1785, Training Loss: 0.079543, Validation Loss: 0.053681\n",
      "Epoch: 1786, Training Loss: 0.079692, Validation Loss: 0.054057\n",
      "Epoch: 1787, Training Loss: 0.079762, Validation Loss: 0.052978\n",
      "Epoch: 1788, Training Loss: 0.079410, Validation Loss: 0.053840\n",
      "Epoch: 1789, Training Loss: 0.079239, Validation Loss: 0.054590\n",
      "Epoch: 1790, Training Loss: 0.079370, Validation Loss: 0.054326\n",
      "Epoch: 1791, Training Loss: 0.079261, Validation Loss: 0.054149\n",
      "Epoch: 1792, Training Loss: 0.079018, Validation Loss: 0.053510\n",
      "Epoch: 1793, Training Loss: 0.079015, Validation Loss: 0.054525\n",
      "Epoch: 1794, Training Loss: 0.079012, Validation Loss: 0.054883\n",
      "Epoch: 1795, Training Loss: 0.078967, Validation Loss: 0.054010\n",
      "Epoch: 1796, Training Loss: 0.078784, Validation Loss: 0.054791\n",
      "Epoch: 1797, Training Loss: 0.078859, Validation Loss: 0.055474\n",
      "Epoch: 1798, Training Loss: 0.078779, Validation Loss: 0.054324\n",
      "Epoch: 1799, Training Loss: 0.078681, Validation Loss: 0.054126\n",
      "Epoch: 1800, Training Loss: 0.078588, Validation Loss: 0.054638\n",
      "Epoch: 1801, Training Loss: 0.078592, Validation Loss: 0.054299\n",
      "Epoch: 1802, Training Loss: 0.078568, Validation Loss: 0.053931\n",
      "Epoch: 1803, Training Loss: 0.078386, Validation Loss: 0.054160\n",
      "Epoch: 1804, Training Loss: 0.078398, Validation Loss: 0.054278\n",
      "Epoch: 1805, Training Loss: 0.078379, Validation Loss: 0.053876\n",
      "Epoch: 1806, Training Loss: 0.078243, Validation Loss: 0.053577\n",
      "Epoch: 1807, Training Loss: 0.078189, Validation Loss: 0.053885\n",
      "Epoch: 1808, Training Loss: 0.078151, Validation Loss: 0.053745\n",
      "Epoch: 1809, Training Loss: 0.078096, Validation Loss: 0.053701\n",
      "Epoch: 1810, Training Loss: 0.078021, Validation Loss: 0.054306\n",
      "Epoch: 1811, Training Loss: 0.078016, Validation Loss: 0.054688\n",
      "Epoch: 1812, Training Loss: 0.077977, Validation Loss: 0.054382\n",
      "Epoch: 1813, Training Loss: 0.077911, Validation Loss: 0.054680\n",
      "Epoch: 1814, Training Loss: 0.077901, Validation Loss: 0.054701\n",
      "Epoch: 1815, Training Loss: 0.077861, Validation Loss: 0.054491\n",
      "Epoch: 1816, Training Loss: 0.077834, Validation Loss: 0.054373\n",
      "Epoch: 1817, Training Loss: 0.077782, Validation Loss: 0.054484\n",
      "Epoch: 1818, Training Loss: 0.077754, Validation Loss: 0.054676\n",
      "Epoch: 1819, Training Loss: 0.077741, Validation Loss: 0.054752\n",
      "Epoch: 1820, Training Loss: 0.077705, Validation Loss: 0.054501\n",
      "Epoch: 1821, Training Loss: 0.077671, Validation Loss: 0.054490\n",
      "Epoch: 1822, Training Loss: 0.077645, Validation Loss: 0.054343\n",
      "Epoch: 1823, Training Loss: 0.077624, Validation Loss: 0.054222\n",
      "Epoch: 1824, Training Loss: 0.077596, Validation Loss: 0.054569\n",
      "Epoch: 1825, Training Loss: 0.077573, Validation Loss: 0.054790\n",
      "Epoch: 1826, Training Loss: 0.077555, Validation Loss: 0.054434\n",
      "Epoch: 1827, Training Loss: 0.077536, Validation Loss: 0.054387\n",
      "Epoch: 1828, Training Loss: 0.077520, Validation Loss: 0.054366\n",
      "Epoch: 1829, Training Loss: 0.077487, Validation Loss: 0.054270\n",
      "Epoch: 1830, Training Loss: 0.077458, Validation Loss: 0.054581\n",
      "Epoch: 1831, Training Loss: 0.077446, Validation Loss: 0.054621\n",
      "Epoch: 1832, Training Loss: 0.077423, Validation Loss: 0.054454\n",
      "Epoch: 1833, Training Loss: 0.077400, Validation Loss: 0.054586\n",
      "Epoch: 1834, Training Loss: 0.077381, Validation Loss: 0.054537\n",
      "Epoch: 1835, Training Loss: 0.077365, Validation Loss: 0.054604\n",
      "Epoch: 1836, Training Loss: 0.077346, Validation Loss: 0.054396\n",
      "Epoch: 1837, Training Loss: 0.077336, Validation Loss: 0.054435\n",
      "Epoch: 1838, Training Loss: 0.077325, Validation Loss: 0.054533\n",
      "Epoch: 1839, Training Loss: 0.077301, Validation Loss: 0.054377\n",
      "Epoch: 1840, Training Loss: 0.077275, Validation Loss: 0.054683\n",
      "Epoch: 1841, Training Loss: 0.077247, Validation Loss: 0.054827\n",
      "Epoch: 1842, Training Loss: 0.077255, Validation Loss: 0.054655\n",
      "Epoch: 1843, Training Loss: 0.077217, Validation Loss: 0.054855\n",
      "Epoch: 1844, Training Loss: 0.077195, Validation Loss: 0.054494\n",
      "Epoch: 1845, Training Loss: 0.077182, Validation Loss: 0.054541\n",
      "Epoch: 1846, Training Loss: 0.077175, Validation Loss: 0.054891\n",
      "Epoch: 1847, Training Loss: 0.077149, Validation Loss: 0.054563\n",
      "Epoch: 1848, Training Loss: 0.077119, Validation Loss: 0.054671\n",
      "Epoch: 1849, Training Loss: 0.077130, Validation Loss: 0.054948\n",
      "Epoch: 1850, Training Loss: 0.077084, Validation Loss: 0.054702\n",
      "Epoch: 1851, Training Loss: 0.077071, Validation Loss: 0.054794\n",
      "Epoch: 1852, Training Loss: 0.077060, Validation Loss: 0.054710\n",
      "Epoch: 1853, Training Loss: 0.077041, Validation Loss: 0.054670\n",
      "Epoch: 1854, Training Loss: 0.077022, Validation Loss: 0.054916\n",
      "Epoch: 1855, Training Loss: 0.077004, Validation Loss: 0.054724\n",
      "Epoch: 1856, Training Loss: 0.076997, Validation Loss: 0.054841\n",
      "Epoch: 1857, Training Loss: 0.076970, Validation Loss: 0.054544\n",
      "Epoch: 1858, Training Loss: 0.076958, Validation Loss: 0.054277\n",
      "Epoch: 1859, Training Loss: 0.076966, Validation Loss: 0.054682\n",
      "Epoch: 1860, Training Loss: 0.076930, Validation Loss: 0.054198\n",
      "Epoch: 1861, Training Loss: 0.076898, Validation Loss: 0.053926\n",
      "Epoch: 1862, Training Loss: 0.076894, Validation Loss: 0.054082\n",
      "Epoch: 1863, Training Loss: 0.076868, Validation Loss: 0.053795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1864, Training Loss: 0.076841, Validation Loss: 0.054054\n",
      "Epoch: 1865, Training Loss: 0.076814, Validation Loss: 0.054115\n",
      "Epoch: 1866, Training Loss: 0.076807, Validation Loss: 0.054022\n",
      "Epoch: 1867, Training Loss: 0.076806, Validation Loss: 0.054590\n",
      "Epoch: 1868, Training Loss: 0.076780, Validation Loss: 0.054294\n",
      "Epoch: 1869, Training Loss: 0.076750, Validation Loss: 0.054524\n",
      "Epoch: 1870, Training Loss: 0.076753, Validation Loss: 0.054610\n",
      "Epoch: 1871, Training Loss: 0.076761, Validation Loss: 0.054053\n",
      "Epoch: 1872, Training Loss: 0.076716, Validation Loss: 0.054301\n",
      "Epoch: 1873, Training Loss: 0.076693, Validation Loss: 0.054419\n",
      "Epoch: 1874, Training Loss: 0.076682, Validation Loss: 0.054459\n",
      "Epoch: 1875, Training Loss: 0.076664, Validation Loss: 0.055026\n",
      "Epoch: 1876, Training Loss: 0.076637, Validation Loss: 0.054741\n",
      "Epoch: 1877, Training Loss: 0.076620, Validation Loss: 0.054221\n",
      "Epoch: 1878, Training Loss: 0.076606, Validation Loss: 0.054174\n",
      "Epoch: 1879, Training Loss: 0.076581, Validation Loss: 0.054220\n",
      "Epoch: 1880, Training Loss: 0.076558, Validation Loss: 0.054615\n",
      "Epoch: 1881, Training Loss: 0.076552, Validation Loss: 0.054963\n",
      "Epoch: 1882, Training Loss: 0.076521, Validation Loss: 0.054700\n",
      "Epoch: 1883, Training Loss: 0.076502, Validation Loss: 0.054393\n",
      "Epoch: 1884, Training Loss: 0.076491, Validation Loss: 0.054213\n",
      "Epoch: 1885, Training Loss: 0.076472, Validation Loss: 0.054737\n",
      "Epoch: 1886, Training Loss: 0.076466, Validation Loss: 0.054761\n",
      "Epoch: 1887, Training Loss: 0.076451, Validation Loss: 0.055010\n",
      "Epoch: 1888, Training Loss: 0.076422, Validation Loss: 0.054664\n",
      "Epoch: 1889, Training Loss: 0.076403, Validation Loss: 0.054508\n",
      "Epoch: 1890, Training Loss: 0.076389, Validation Loss: 0.054333\n",
      "Epoch: 1891, Training Loss: 0.076370, Validation Loss: 0.054477\n",
      "Epoch: 1892, Training Loss: 0.076352, Validation Loss: 0.054733\n",
      "Epoch: 1893, Training Loss: 0.076346, Validation Loss: 0.054857\n",
      "Epoch: 1894, Training Loss: 0.076319, Validation Loss: 0.054604\n",
      "Epoch: 1895, Training Loss: 0.076307, Validation Loss: 0.054483\n",
      "Epoch: 1896, Training Loss: 0.076301, Validation Loss: 0.054252\n",
      "Epoch: 1897, Training Loss: 0.076280, Validation Loss: 0.054800\n",
      "Epoch: 1898, Training Loss: 0.076267, Validation Loss: 0.054828\n",
      "Epoch: 1899, Training Loss: 0.076239, Validation Loss: 0.054957\n",
      "Epoch: 1900, Training Loss: 0.076215, Validation Loss: 0.054660\n",
      "Epoch: 1901, Training Loss: 0.076210, Validation Loss: 0.054351\n",
      "Epoch: 1902, Training Loss: 0.076198, Validation Loss: 0.054937\n",
      "Epoch: 1903, Training Loss: 0.076213, Validation Loss: 0.054878\n",
      "Epoch: 1904, Training Loss: 0.076205, Validation Loss: 0.055493\n",
      "Epoch: 1905, Training Loss: 0.076172, Validation Loss: 0.054688\n",
      "Epoch: 1906, Training Loss: 0.076135, Validation Loss: 0.054820\n",
      "Epoch: 1907, Training Loss: 0.076106, Validation Loss: 0.054712\n",
      "Epoch: 1908, Training Loss: 0.076127, Validation Loss: 0.054413\n",
      "Epoch: 1909, Training Loss: 0.076195, Validation Loss: 0.055606\n",
      "Epoch: 1910, Training Loss: 0.076196, Validation Loss: 0.054614\n",
      "Epoch: 1911, Training Loss: 0.076142, Validation Loss: 0.055629\n",
      "Epoch: 1912, Training Loss: 0.076109, Validation Loss: 0.054787\n",
      "Epoch: 1913, Training Loss: 0.076042, Validation Loss: 0.055057\n",
      "Epoch: 1914, Training Loss: 0.076020, Validation Loss: 0.055219\n",
      "Epoch: 1915, Training Loss: 0.076060, Validation Loss: 0.054513\n",
      "Epoch: 1916, Training Loss: 0.076132, Validation Loss: 0.055658\n",
      "Epoch: 1917, Training Loss: 0.076037, Validation Loss: 0.054383\n",
      "Epoch: 1918, Training Loss: 0.075961, Validation Loss: 0.055225\n",
      "Epoch: 1919, Training Loss: 0.075914, Validation Loss: 0.054950\n",
      "Epoch: 1920, Training Loss: 0.075919, Validation Loss: 0.055276\n",
      "Epoch: 1921, Training Loss: 0.075953, Validation Loss: 0.055913\n",
      "Epoch: 1922, Training Loss: 0.075979, Validation Loss: 0.054905\n",
      "Epoch: 1923, Training Loss: 0.076045, Validation Loss: 0.056108\n",
      "Epoch: 1924, Training Loss: 0.076111, Validation Loss: 0.054266\n",
      "Epoch: 1925, Training Loss: 0.076034, Validation Loss: 0.055803\n",
      "Epoch: 1926, Training Loss: 0.075873, Validation Loss: 0.054419\n",
      "Epoch: 1927, Training Loss: 0.075785, Validation Loss: 0.054993\n",
      "Epoch: 1928, Training Loss: 0.075822, Validation Loss: 0.055994\n",
      "Epoch: 1929, Training Loss: 0.075982, Validation Loss: 0.055169\n",
      "Epoch: 1930, Training Loss: 0.076145, Validation Loss: 0.057087\n",
      "Epoch: 1931, Training Loss: 0.076333, Validation Loss: 0.054660\n",
      "Epoch: 1932, Training Loss: 0.076413, Validation Loss: 0.057075\n",
      "Epoch: 1933, Training Loss: 0.076331, Validation Loss: 0.053969\n",
      "Epoch: 1934, Training Loss: 0.076068, Validation Loss: 0.056156\n",
      "Epoch: 1935, Training Loss: 0.075805, Validation Loss: 0.054346\n",
      "Epoch: 1936, Training Loss: 0.075697, Validation Loss: 0.054556\n",
      "Epoch: 1937, Training Loss: 0.075766, Validation Loss: 0.055675\n",
      "Epoch: 1938, Training Loss: 0.075983, Validation Loss: 0.054614\n",
      "Epoch: 1939, Training Loss: 0.076259, Validation Loss: 0.057934\n",
      "Epoch: 1940, Training Loss: 0.076738, Validation Loss: 0.055343\n",
      "Epoch: 1941, Training Loss: 0.077483, Validation Loss: 0.060083\n",
      "Epoch: 1942, Training Loss: 0.078371, Validation Loss: 0.054958\n",
      "Epoch: 1943, Training Loss: 0.078698, Validation Loss: 0.061412\n",
      "Epoch: 1944, Training Loss: 0.078715, Validation Loss: 0.054696\n",
      "Epoch: 1945, Training Loss: 0.077976, Validation Loss: 0.060307\n",
      "Epoch: 1946, Training Loss: 0.076949, Validation Loss: 0.054238\n",
      "Epoch: 1947, Training Loss: 0.075989, Validation Loss: 0.056983\n",
      "Epoch: 1948, Training Loss: 0.075495, Validation Loss: 0.054781\n",
      "Epoch: 1949, Training Loss: 0.075442, Validation Loss: 0.055281\n",
      "Epoch: 1950, Training Loss: 0.075818, Validation Loss: 0.057558\n",
      "Epoch: 1951, Training Loss: 0.076743, Validation Loss: 0.055669\n",
      "Epoch: 1952, Training Loss: 0.079028, Validation Loss: 0.063187\n",
      "Epoch: 1953, Training Loss: 0.082024, Validation Loss: 0.057506\n",
      "Epoch: 1954, Training Loss: 0.083360, Validation Loss: 0.067650\n",
      "Epoch: 1955, Training Loss: 0.084244, Validation Loss: 0.057454\n",
      "Epoch: 1956, Training Loss: 0.081028, Validation Loss: 0.064414\n",
      "Epoch: 1957, Training Loss: 0.078088, Validation Loss: 0.054029\n",
      "Epoch: 1958, Training Loss: 0.076287, Validation Loss: 0.055467\n",
      "Epoch: 1959, Training Loss: 0.077425, Validation Loss: 0.059459\n",
      "Epoch: 1960, Training Loss: 0.079675, Validation Loss: 0.055202\n",
      "Epoch: 1961, Training Loss: 0.080333, Validation Loss: 0.065043\n",
      "Epoch: 1962, Training Loss: 0.080930, Validation Loss: 0.058772\n",
      "Epoch: 1963, Training Loss: 0.082261, Validation Loss: 0.068993\n",
      "Epoch: 1964, Training Loss: 0.082592, Validation Loss: 0.059305\n",
      "Epoch: 1965, Training Loss: 0.080749, Validation Loss: 0.064800\n",
      "Epoch: 1966, Training Loss: 0.078974, Validation Loss: 0.055058\n",
      "Epoch: 1967, Training Loss: 0.076980, Validation Loss: 0.057057\n",
      "Epoch: 1968, Training Loss: 0.076352, Validation Loss: 0.056689\n",
      "Epoch: 1969, Training Loss: 0.077281, Validation Loss: 0.054994\n",
      "Epoch: 1970, Training Loss: 0.077874, Validation Loss: 0.061050\n",
      "Epoch: 1971, Training Loss: 0.077614, Validation Loss: 0.054681\n",
      "Epoch: 1972, Training Loss: 0.076336, Validation Loss: 0.057376\n",
      "Epoch: 1973, Training Loss: 0.075651, Validation Loss: 0.055746\n",
      "Epoch: 1974, Training Loss: 0.075842, Validation Loss: 0.055679\n",
      "Epoch: 1975, Training Loss: 0.076398, Validation Loss: 0.059329\n",
      "Epoch: 1976, Training Loss: 0.076764, Validation Loss: 0.056006\n",
      "Epoch: 1977, Training Loss: 0.076877, Validation Loss: 0.060787\n",
      "Epoch: 1978, Training Loss: 0.076391, Validation Loss: 0.056101\n",
      "Epoch: 1979, Training Loss: 0.075982, Validation Loss: 0.058529\n",
      "Epoch: 1980, Training Loss: 0.075829, Validation Loss: 0.055119\n",
      "Epoch: 1981, Training Loss: 0.075714, Validation Loss: 0.056632\n",
      "Epoch: 1982, Training Loss: 0.075566, Validation Loss: 0.055530\n",
      "Epoch: 1983, Training Loss: 0.075507, Validation Loss: 0.055765\n",
      "Epoch: 1984, Training Loss: 0.075643, Validation Loss: 0.057169\n",
      "Epoch: 1985, Training Loss: 0.075804, Validation Loss: 0.055402\n",
      "Epoch: 1986, Training Loss: 0.075694, Validation Loss: 0.057128\n",
      "Epoch: 1987, Training Loss: 0.075742, Validation Loss: 0.053851\n",
      "Epoch: 1988, Training Loss: 0.075631, Validation Loss: 0.056729\n",
      "Epoch: 1989, Training Loss: 0.075550, Validation Loss: 0.055065\n",
      "Epoch: 1990, Training Loss: 0.075734, Validation Loss: 0.058750\n",
      "Epoch: 1991, Training Loss: 0.075581, Validation Loss: 0.055864\n",
      "Epoch: 1992, Training Loss: 0.075368, Validation Loss: 0.057279\n",
      "Epoch: 1993, Training Loss: 0.075373, Validation Loss: 0.054735\n",
      "Epoch: 1994, Training Loss: 0.075286, Validation Loss: 0.056539\n",
      "Epoch: 1995, Training Loss: 0.075273, Validation Loss: 0.054858\n",
      "Epoch: 1996, Training Loss: 0.075392, Validation Loss: 0.057505\n",
      "Epoch: 1997, Training Loss: 0.075369, Validation Loss: 0.054473\n",
      "Epoch: 1998, Training Loss: 0.075491, Validation Loss: 0.056876\n",
      "Epoch: 1999, Training Loss: 0.075695, Validation Loss: 0.053991\n",
      "Epoch: 2000, Training Loss: 0.076074, Validation Loss: 0.058933\n",
      "Epoch: 2001, Training Loss: 0.077102, Validation Loss: 0.056082\n",
      "Epoch: 2002, Training Loss: 0.079627, Validation Loss: 0.065878\n",
      "Epoch: 2003, Training Loss: 0.083558, Validation Loss: 0.058568\n",
      "Epoch: 2004, Training Loss: 0.087005, Validation Loss: 0.073691\n",
      "Epoch: 2005, Training Loss: 0.092905, Validation Loss: 0.063133\n",
      "Epoch: 2006, Training Loss: 0.086784, Validation Loss: 0.073487\n",
      "Epoch: 2007, Training Loss: 0.080269, Validation Loss: 0.056676\n",
      "Epoch: 2008, Training Loss: 0.081139, Validation Loss: 0.055982\n",
      "Epoch: 2009, Training Loss: 0.079332, Validation Loss: 0.060422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2010, Training Loss: 0.078067, Validation Loss: 0.054045\n",
      "Epoch: 2011, Training Loss: 0.082488, Validation Loss: 0.058699\n",
      "Epoch: 2012, Training Loss: 0.081380, Validation Loss: 0.060894\n",
      "Epoch: 2013, Training Loss: 0.077687, Validation Loss: 0.057983\n",
      "Epoch: 2014, Training Loss: 0.078287, Validation Loss: 0.055026\n",
      "Epoch: 2015, Training Loss: 0.078604, Validation Loss: 0.060364\n",
      "Epoch: 2016, Training Loss: 0.077266, Validation Loss: 0.056781\n",
      "Epoch: 2017, Training Loss: 0.079087, Validation Loss: 0.054758\n",
      "Epoch: 2018, Training Loss: 0.080134, Validation Loss: 0.063142\n",
      "Epoch: 2019, Training Loss: 0.077356, Validation Loss: 0.054212\n",
      "Epoch: 2020, Training Loss: 0.077306, Validation Loss: 0.054135\n",
      "Epoch: 2021, Training Loss: 0.078249, Validation Loss: 0.058796\n",
      "Epoch: 2022, Training Loss: 0.077130, Validation Loss: 0.053031\n",
      "Epoch: 2023, Training Loss: 0.077742, Validation Loss: 0.054465\n",
      "Epoch: 2024, Training Loss: 0.078248, Validation Loss: 0.057267\n",
      "Epoch: 2025, Training Loss: 0.076700, Validation Loss: 0.054546\n",
      "Epoch: 2026, Training Loss: 0.076541, Validation Loss: 0.054099\n",
      "Epoch: 2027, Training Loss: 0.077186, Validation Loss: 0.058116\n",
      "Epoch: 2028, Training Loss: 0.076593, Validation Loss: 0.055662\n",
      "Epoch: 2029, Training Loss: 0.076445, Validation Loss: 0.055339\n",
      "Epoch: 2030, Training Loss: 0.077129, Validation Loss: 0.059257\n",
      "Epoch: 2031, Training Loss: 0.076702, Validation Loss: 0.055454\n",
      "Epoch: 2032, Training Loss: 0.076128, Validation Loss: 0.056205\n",
      "Epoch: 2033, Training Loss: 0.076439, Validation Loss: 0.057046\n",
      "Epoch: 2034, Training Loss: 0.076492, Validation Loss: 0.054122\n",
      "Epoch: 2035, Training Loss: 0.076154, Validation Loss: 0.055448\n",
      "Epoch: 2036, Training Loss: 0.076321, Validation Loss: 0.055497\n",
      "Epoch: 2037, Training Loss: 0.076458, Validation Loss: 0.054511\n",
      "Epoch: 2038, Training Loss: 0.076066, Validation Loss: 0.055493\n",
      "Epoch: 2039, Training Loss: 0.075970, Validation Loss: 0.055545\n",
      "Epoch: 2040, Training Loss: 0.076273, Validation Loss: 0.055170\n",
      "Epoch: 2041, Training Loss: 0.076189, Validation Loss: 0.056648\n",
      "Epoch: 2042, Training Loss: 0.075942, Validation Loss: 0.055687\n",
      "Epoch: 2043, Training Loss: 0.075981, Validation Loss: 0.055347\n",
      "Epoch: 2044, Training Loss: 0.076133, Validation Loss: 0.056111\n",
      "Epoch: 2045, Training Loss: 0.076026, Validation Loss: 0.054418\n",
      "Epoch: 2046, Training Loss: 0.075889, Validation Loss: 0.054336\n",
      "Epoch: 2047, Training Loss: 0.075981, Validation Loss: 0.055484\n",
      "Epoch: 2048, Training Loss: 0.076013, Validation Loss: 0.054459\n",
      "Epoch: 2049, Training Loss: 0.075831, Validation Loss: 0.055823\n",
      "Epoch: 2050, Training Loss: 0.075754, Validation Loss: 0.055540\n",
      "Epoch: 2051, Training Loss: 0.075889, Validation Loss: 0.055428\n",
      "Epoch: 2052, Training Loss: 0.075907, Validation Loss: 0.056706\n",
      "Epoch: 2053, Training Loss: 0.075856, Validation Loss: 0.055561\n",
      "Epoch: 2054, Training Loss: 0.075840, Validation Loss: 0.056381\n",
      "Epoch: 2055, Training Loss: 0.075919, Validation Loss: 0.055288\n",
      "Epoch: 2056, Training Loss: 0.075971, Validation Loss: 0.055222\n",
      "Epoch: 2057, Training Loss: 0.075859, Validation Loss: 0.054812\n",
      "Epoch: 2058, Training Loss: 0.075718, Validation Loss: 0.055567\n",
      "Epoch: 2059, Training Loss: 0.075712, Validation Loss: 0.055355\n",
      "Epoch: 2060, Training Loss: 0.075729, Validation Loss: 0.056509\n",
      "Epoch: 2061, Training Loss: 0.075668, Validation Loss: 0.056039\n",
      "Epoch: 2062, Training Loss: 0.075599, Validation Loss: 0.055691\n",
      "Epoch: 2063, Training Loss: 0.075668, Validation Loss: 0.056134\n",
      "Epoch: 2064, Training Loss: 0.075653, Validation Loss: 0.054978\n",
      "Epoch: 2065, Training Loss: 0.075662, Validation Loss: 0.056021\n",
      "Epoch: 2066, Training Loss: 0.075627, Validation Loss: 0.055186\n",
      "Epoch: 2067, Training Loss: 0.075617, Validation Loss: 0.056007\n",
      "Epoch: 2068, Training Loss: 0.075617, Validation Loss: 0.055708\n",
      "Epoch: 2069, Training Loss: 0.075599, Validation Loss: 0.056248\n",
      "Epoch: 2070, Training Loss: 0.075550, Validation Loss: 0.055860\n",
      "Epoch: 2071, Training Loss: 0.075517, Validation Loss: 0.056206\n",
      "Epoch: 2072, Training Loss: 0.075506, Validation Loss: 0.055451\n",
      "Epoch: 2073, Training Loss: 0.075546, Validation Loss: 0.056291\n",
      "Epoch: 2074, Training Loss: 0.075540, Validation Loss: 0.055363\n",
      "Epoch: 2075, Training Loss: 0.075565, Validation Loss: 0.056671\n",
      "Epoch: 2076, Training Loss: 0.075646, Validation Loss: 0.055560\n",
      "Epoch: 2077, Training Loss: 0.075791, Validation Loss: 0.057192\n",
      "Epoch: 2078, Training Loss: 0.076001, Validation Loss: 0.055311\n",
      "Epoch: 2079, Training Loss: 0.076378, Validation Loss: 0.058114\n",
      "Epoch: 2080, Training Loss: 0.076953, Validation Loss: 0.055363\n",
      "Epoch: 2081, Training Loss: 0.078011, Validation Loss: 0.061120\n",
      "Epoch: 2082, Training Loss: 0.079353, Validation Loss: 0.057114\n",
      "Epoch: 2083, Training Loss: 0.081856, Validation Loss: 0.067262\n",
      "Epoch: 2084, Training Loss: 0.084282, Validation Loss: 0.060792\n",
      "Epoch: 2085, Training Loss: 0.088389, Validation Loss: 0.075305\n",
      "Epoch: 2086, Training Loss: 0.090670, Validation Loss: 0.065487\n",
      "Epoch: 2087, Training Loss: 0.094712, Validation Loss: 0.082804\n",
      "Epoch: 2088, Training Loss: 0.093070, Validation Loss: 0.067248\n",
      "Epoch: 2089, Training Loss: 0.091863, Validation Loss: 0.079205\n",
      "Epoch: 2090, Training Loss: 0.085221, Validation Loss: 0.060706\n",
      "Epoch: 2091, Training Loss: 0.079875, Validation Loss: 0.062516\n",
      "Epoch: 2092, Training Loss: 0.076223, Validation Loss: 0.055030\n",
      "Epoch: 2093, Training Loss: 0.075782, Validation Loss: 0.054494\n",
      "Epoch: 2094, Training Loss: 0.077980, Validation Loss: 0.061372\n",
      "Epoch: 2095, Training Loss: 0.080736, Validation Loss: 0.058703\n",
      "Epoch: 2096, Training Loss: 0.083001, Validation Loss: 0.068425\n",
      "Epoch: 2097, Training Loss: 0.081858, Validation Loss: 0.059436\n",
      "Epoch: 2098, Training Loss: 0.079669, Validation Loss: 0.062500\n",
      "Epoch: 2099, Training Loss: 0.076907, Validation Loss: 0.055585\n",
      "Epoch: 2100, Training Loss: 0.075656, Validation Loss: 0.055104\n",
      "Epoch: 2101, Training Loss: 0.076171, Validation Loss: 0.057418\n",
      "Epoch: 2102, Training Loss: 0.077507, Validation Loss: 0.055745\n",
      "Epoch: 2103, Training Loss: 0.078490, Validation Loss: 0.060764\n",
      "Epoch: 2104, Training Loss: 0.078110, Validation Loss: 0.056790\n",
      "Epoch: 2105, Training Loss: 0.077098, Validation Loss: 0.058617\n",
      "Epoch: 2106, Training Loss: 0.075967, Validation Loss: 0.056434\n",
      "Epoch: 2107, Training Loss: 0.075348, Validation Loss: 0.056442\n",
      "Epoch: 2108, Training Loss: 0.075630, Validation Loss: 0.057067\n",
      "Epoch: 2109, Training Loss: 0.076245, Validation Loss: 0.056885\n",
      "Epoch: 2110, Training Loss: 0.076650, Validation Loss: 0.058411\n",
      "Epoch: 2111, Training Loss: 0.076513, Validation Loss: 0.056000\n",
      "Epoch: 2112, Training Loss: 0.076126, Validation Loss: 0.057635\n",
      "Epoch: 2113, Training Loss: 0.075731, Validation Loss: 0.054805\n",
      "Epoch: 2114, Training Loss: 0.075536, Validation Loss: 0.056899\n",
      "Epoch: 2115, Training Loss: 0.075457, Validation Loss: 0.055352\n",
      "Epoch: 2116, Training Loss: 0.075453, Validation Loss: 0.056120\n",
      "Epoch: 2117, Training Loss: 0.075522, Validation Loss: 0.056834\n",
      "Epoch: 2118, Training Loss: 0.075622, Validation Loss: 0.055535\n",
      "Epoch: 2119, Training Loss: 0.075827, Validation Loss: 0.058267\n",
      "Epoch: 2120, Training Loss: 0.075946, Validation Loss: 0.055365\n",
      "Epoch: 2121, Training Loss: 0.075940, Validation Loss: 0.058716\n",
      "Epoch: 2122, Training Loss: 0.075719, Validation Loss: 0.055253\n",
      "Epoch: 2123, Training Loss: 0.075541, Validation Loss: 0.057747\n",
      "Epoch: 2124, Training Loss: 0.075353, Validation Loss: 0.055598\n",
      "Epoch: 2125, Training Loss: 0.075246, Validation Loss: 0.056859\n",
      "Epoch: 2126, Training Loss: 0.075207, Validation Loss: 0.056048\n",
      "Epoch: 2127, Training Loss: 0.075122, Validation Loss: 0.056042\n",
      "Epoch: 2128, Training Loss: 0.075087, Validation Loss: 0.055812\n",
      "Epoch: 2129, Training Loss: 0.075064, Validation Loss: 0.055928\n",
      "Epoch: 2130, Training Loss: 0.075060, Validation Loss: 0.056048\n",
      "Epoch: 2131, Training Loss: 0.075070, Validation Loss: 0.056133\n",
      "Epoch: 2132, Training Loss: 0.075071, Validation Loss: 0.056343\n",
      "Epoch: 2133, Training Loss: 0.075058, Validation Loss: 0.056008\n",
      "Epoch: 2134, Training Loss: 0.075046, Validation Loss: 0.056450\n",
      "Epoch: 2135, Training Loss: 0.075047, Validation Loss: 0.055631\n",
      "Epoch: 2136, Training Loss: 0.075111, Validation Loss: 0.057037\n",
      "Epoch: 2137, Training Loss: 0.075280, Validation Loss: 0.055797\n",
      "Epoch: 2138, Training Loss: 0.075675, Validation Loss: 0.058795\n",
      "Epoch: 2139, Training Loss: 0.076439, Validation Loss: 0.055891\n",
      "Epoch: 2140, Training Loss: 0.078158, Validation Loss: 0.062983\n",
      "Epoch: 2141, Training Loss: 0.081046, Validation Loss: 0.058227\n",
      "Epoch: 2142, Training Loss: 0.087313, Validation Loss: 0.075798\n",
      "Epoch: 2143, Training Loss: 0.096041, Validation Loss: 0.069480\n",
      "Epoch: 2144, Training Loss: 0.112598, Validation Loss: 0.106273\n",
      "Epoch: 2145, Training Loss: 0.122952, Validation Loss: 0.091751\n",
      "Epoch: 2146, Training Loss: 0.135690, Validation Loss: 0.132730\n",
      "Epoch: 2147, Training Loss: 0.111843, Validation Loss: 0.080977\n",
      "Epoch: 2148, Training Loss: 0.090482, Validation Loss: 0.075274\n",
      "Epoch: 2149, Training Loss: 0.083358, Validation Loss: 0.063338\n",
      "Epoch: 2150, Training Loss: 0.092745, Validation Loss: 0.062389\n",
      "Epoch: 2151, Training Loss: 0.090631, Validation Loss: 0.075269\n",
      "Epoch: 2152, Training Loss: 0.079314, Validation Loss: 0.053672\n",
      "Epoch: 2153, Training Loss: 0.085329, Validation Loss: 0.059994\n",
      "Epoch: 2154, Training Loss: 0.092287, Validation Loss: 0.077754\n",
      "Epoch: 2155, Training Loss: 0.084337, Validation Loss: 0.061046\n",
      "Epoch: 2156, Training Loss: 0.081022, Validation Loss: 0.056388\n",
      "Epoch: 2157, Training Loss: 0.084762, Validation Loss: 0.070408\n",
      "Epoch: 2158, Training Loss: 0.078253, Validation Loss: 0.054396\n",
      "Epoch: 2159, Training Loss: 0.076584, Validation Loss: 0.054734\n",
      "Epoch: 2160, Training Loss: 0.081037, Validation Loss: 0.066859\n",
      "Epoch: 2161, Training Loss: 0.079196, Validation Loss: 0.057428\n",
      "Epoch: 2162, Training Loss: 0.078799, Validation Loss: 0.058446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2163, Training Loss: 0.079467, Validation Loss: 0.062151\n",
      "Epoch: 2164, Training Loss: 0.076688, Validation Loss: 0.055239\n",
      "Epoch: 2165, Training Loss: 0.076816, Validation Loss: 0.054210\n",
      "Epoch: 2166, Training Loss: 0.077659, Validation Loss: 0.059514\n",
      "Epoch: 2167, Training Loss: 0.076590, Validation Loss: 0.056473\n",
      "Epoch: 2168, Training Loss: 0.077786, Validation Loss: 0.055632\n",
      "Epoch: 2169, Training Loss: 0.077739, Validation Loss: 0.060891\n",
      "Epoch: 2170, Training Loss: 0.076417, Validation Loss: 0.056807\n",
      "Epoch: 2171, Training Loss: 0.076888, Validation Loss: 0.055419\n",
      "Epoch: 2172, Training Loss: 0.076216, Validation Loss: 0.057776\n",
      "Epoch: 2173, Training Loss: 0.075947, Validation Loss: 0.056706\n",
      "Epoch: 2174, Training Loss: 0.076812, Validation Loss: 0.055948\n",
      "Epoch: 2175, Training Loss: 0.076387, Validation Loss: 0.058046\n",
      "Epoch: 2176, Training Loss: 0.076185, Validation Loss: 0.058503\n",
      "Epoch: 2177, Training Loss: 0.076353, Validation Loss: 0.055748\n",
      "Epoch: 2178, Training Loss: 0.075745, Validation Loss: 0.057784\n",
      "Epoch: 2179, Training Loss: 0.075561, Validation Loss: 0.057094\n",
      "Epoch: 2180, Training Loss: 0.075964, Validation Loss: 0.055556\n",
      "Epoch: 2181, Training Loss: 0.075837, Validation Loss: 0.058271\n",
      "Epoch: 2182, Training Loss: 0.075671, Validation Loss: 0.056999\n",
      "Epoch: 2183, Training Loss: 0.075907, Validation Loss: 0.056889\n",
      "Epoch: 2184, Training Loss: 0.075764, Validation Loss: 0.058240\n",
      "Epoch: 2185, Training Loss: 0.075346, Validation Loss: 0.056891\n",
      "Epoch: 2186, Training Loss: 0.075337, Validation Loss: 0.056214\n",
      "Epoch: 2187, Training Loss: 0.075458, Validation Loss: 0.057677\n",
      "Epoch: 2188, Training Loss: 0.075332, Validation Loss: 0.056288\n",
      "Epoch: 2189, Training Loss: 0.075274, Validation Loss: 0.056277\n",
      "Epoch: 2190, Training Loss: 0.075421, Validation Loss: 0.057835\n",
      "Epoch: 2191, Training Loss: 0.075358, Validation Loss: 0.055901\n",
      "Epoch: 2192, Training Loss: 0.075158, Validation Loss: 0.057035\n",
      "Epoch: 2193, Training Loss: 0.075073, Validation Loss: 0.056540\n",
      "Epoch: 2194, Training Loss: 0.075143, Validation Loss: 0.055492\n",
      "Epoch: 2195, Training Loss: 0.075105, Validation Loss: 0.056825\n",
      "Epoch: 2196, Training Loss: 0.074999, Validation Loss: 0.055994\n",
      "Epoch: 2197, Training Loss: 0.075039, Validation Loss: 0.056523\n",
      "Epoch: 2198, Training Loss: 0.075075, Validation Loss: 0.057203\n",
      "Epoch: 2199, Training Loss: 0.074997, Validation Loss: 0.056635\n",
      "Epoch: 2200, Training Loss: 0.074886, Validation Loss: 0.056543\n",
      "Epoch: 2201, Training Loss: 0.074847, Validation Loss: 0.056546\n",
      "Epoch: 2202, Training Loss: 0.074870, Validation Loss: 0.055841\n",
      "Epoch: 2203, Training Loss: 0.074850, Validation Loss: 0.056656\n",
      "Epoch: 2204, Training Loss: 0.074800, Validation Loss: 0.056374\n",
      "Epoch: 2205, Training Loss: 0.074791, Validation Loss: 0.056272\n",
      "Epoch: 2206, Training Loss: 0.074803, Validation Loss: 0.057030\n",
      "Epoch: 2207, Training Loss: 0.074786, Validation Loss: 0.056066\n",
      "Epoch: 2208, Training Loss: 0.074723, Validation Loss: 0.056761\n",
      "Epoch: 2209, Training Loss: 0.074695, Validation Loss: 0.056733\n",
      "Epoch: 2210, Training Loss: 0.074711, Validation Loss: 0.056333\n",
      "Epoch: 2211, Training Loss: 0.074705, Validation Loss: 0.057160\n",
      "Epoch: 2212, Training Loss: 0.074668, Validation Loss: 0.056351\n",
      "Epoch: 2213, Training Loss: 0.074651, Validation Loss: 0.056941\n",
      "Epoch: 2214, Training Loss: 0.074640, Validation Loss: 0.056592\n",
      "Epoch: 2215, Training Loss: 0.074639, Validation Loss: 0.056602\n",
      "Epoch: 2216, Training Loss: 0.074617, Validation Loss: 0.056597\n",
      "Epoch: 2217, Training Loss: 0.074581, Validation Loss: 0.056618\n",
      "Epoch: 2218, Training Loss: 0.074552, Validation Loss: 0.056455\n",
      "Epoch: 2219, Training Loss: 0.074538, Validation Loss: 0.056880\n",
      "Epoch: 2220, Training Loss: 0.074542, Validation Loss: 0.056575\n",
      "Epoch: 2221, Training Loss: 0.074536, Validation Loss: 0.057178\n",
      "Epoch: 2222, Training Loss: 0.074518, Validation Loss: 0.056579\n",
      "Epoch: 2223, Training Loss: 0.074489, Validation Loss: 0.057028\n",
      "Epoch: 2224, Training Loss: 0.074469, Validation Loss: 0.056656\n",
      "Epoch: 2225, Training Loss: 0.074454, Validation Loss: 0.056809\n",
      "Epoch: 2226, Training Loss: 0.074451, Validation Loss: 0.056667\n",
      "Epoch: 2227, Training Loss: 0.074444, Validation Loss: 0.056819\n",
      "Epoch: 2228, Training Loss: 0.074433, Validation Loss: 0.056840\n",
      "Epoch: 2229, Training Loss: 0.074413, Validation Loss: 0.056861\n",
      "Epoch: 2230, Training Loss: 0.074394, Validation Loss: 0.056841\n",
      "Epoch: 2231, Training Loss: 0.074377, Validation Loss: 0.056975\n",
      "Epoch: 2232, Training Loss: 0.074365, Validation Loss: 0.056952\n",
      "Epoch: 2233, Training Loss: 0.074357, Validation Loss: 0.057000\n",
      "Epoch: 2234, Training Loss: 0.074349, Validation Loss: 0.057001\n",
      "Epoch: 2235, Training Loss: 0.074342, Validation Loss: 0.056873\n",
      "Epoch: 2236, Training Loss: 0.074338, Validation Loss: 0.057100\n",
      "Epoch: 2237, Training Loss: 0.074345, Validation Loss: 0.056737\n",
      "Epoch: 2238, Training Loss: 0.074362, Validation Loss: 0.057415\n",
      "Epoch: 2239, Training Loss: 0.074422, Validation Loss: 0.056561\n",
      "Epoch: 2240, Training Loss: 0.074547, Validation Loss: 0.058065\n",
      "Epoch: 2241, Training Loss: 0.074764, Validation Loss: 0.056184\n",
      "Epoch: 2242, Training Loss: 0.075315, Validation Loss: 0.059671\n",
      "Epoch: 2243, Training Loss: 0.076488, Validation Loss: 0.056682\n",
      "Epoch: 2244, Training Loss: 0.079383, Validation Loss: 0.066332\n",
      "Epoch: 2245, Training Loss: 0.084634, Validation Loss: 0.061866\n",
      "Epoch: 2246, Training Loss: 0.094526, Validation Loss: 0.086048\n",
      "Epoch: 2247, Training Loss: 0.107253, Validation Loss: 0.079487\n",
      "Epoch: 2248, Training Loss: 0.122792, Validation Loss: 0.118203\n",
      "Epoch: 2249, Training Loss: 0.120106, Validation Loss: 0.090749\n",
      "Epoch: 2250, Training Loss: 0.122725, Validation Loss: 0.111640\n",
      "Epoch: 2251, Training Loss: 0.110021, Validation Loss: 0.086345\n",
      "Epoch: 2252, Training Loss: 0.108838, Validation Loss: 0.085591\n",
      "Epoch: 2253, Training Loss: 0.091824, Validation Loss: 0.072823\n",
      "Epoch: 2254, Training Loss: 0.077166, Validation Loss: 0.054474\n",
      "Epoch: 2255, Training Loss: 0.077817, Validation Loss: 0.055652\n",
      "Epoch: 2256, Training Loss: 0.088065, Validation Loss: 0.071079\n",
      "Epoch: 2257, Training Loss: 0.093669, Validation Loss: 0.073669\n",
      "Epoch: 2258, Training Loss: 0.090495, Validation Loss: 0.068259\n",
      "Epoch: 2259, Training Loss: 0.090144, Validation Loss: 0.076808\n",
      "Epoch: 2260, Training Loss: 0.082682, Validation Loss: 0.058847\n",
      "Epoch: 2261, Training Loss: 0.075856, Validation Loss: 0.058811\n",
      "Epoch: 2262, Training Loss: 0.076399, Validation Loss: 0.060228\n",
      "Epoch: 2263, Training Loss: 0.081574, Validation Loss: 0.059216\n",
      "Epoch: 2264, Training Loss: 0.084656, Validation Loss: 0.071094\n",
      "Epoch: 2265, Training Loss: 0.082816, Validation Loss: 0.062480\n",
      "Epoch: 2266, Training Loss: 0.081294, Validation Loss: 0.062017\n",
      "Epoch: 2267, Training Loss: 0.077584, Validation Loss: 0.059298\n",
      "Epoch: 2268, Training Loss: 0.075289, Validation Loss: 0.055176\n",
      "Epoch: 2269, Training Loss: 0.077349, Validation Loss: 0.057216\n",
      "Epoch: 2270, Training Loss: 0.079675, Validation Loss: 0.060522\n",
      "Epoch: 2271, Training Loss: 0.079600, Validation Loss: 0.060808\n",
      "Epoch: 2272, Training Loss: 0.078082, Validation Loss: 0.056219\n",
      "Epoch: 2273, Training Loss: 0.076785, Validation Loss: 0.059353\n",
      "Epoch: 2274, Training Loss: 0.075285, Validation Loss: 0.054347\n",
      "Epoch: 2275, Training Loss: 0.075670, Validation Loss: 0.054712\n",
      "Epoch: 2276, Training Loss: 0.077330, Validation Loss: 0.060457\n",
      "Epoch: 2277, Training Loss: 0.077460, Validation Loss: 0.056368\n",
      "Epoch: 2278, Training Loss: 0.076668, Validation Loss: 0.059481\n",
      "Epoch: 2279, Training Loss: 0.075846, Validation Loss: 0.058489\n",
      "Epoch: 2280, Training Loss: 0.075235, Validation Loss: 0.056737\n",
      "Epoch: 2281, Training Loss: 0.075133, Validation Loss: 0.058145\n",
      "Epoch: 2282, Training Loss: 0.075931, Validation Loss: 0.057447\n",
      "Epoch: 2283, Training Loss: 0.076575, Validation Loss: 0.058768\n",
      "Epoch: 2284, Training Loss: 0.076117, Validation Loss: 0.057474\n",
      "Epoch: 2285, Training Loss: 0.075402, Validation Loss: 0.058174\n",
      "Epoch: 2286, Training Loss: 0.075060, Validation Loss: 0.055799\n",
      "Epoch: 2287, Training Loss: 0.074940, Validation Loss: 0.057060\n",
      "Epoch: 2288, Training Loss: 0.075082, Validation Loss: 0.056738\n",
      "Epoch: 2289, Training Loss: 0.075454, Validation Loss: 0.055272\n",
      "Epoch: 2290, Training Loss: 0.075585, Validation Loss: 0.058124\n",
      "Epoch: 2291, Training Loss: 0.075217, Validation Loss: 0.055442\n",
      "Epoch: 2292, Training Loss: 0.074835, Validation Loss: 0.056873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2293, Training Loss: 0.074702, Validation Loss: 0.056986\n",
      "Epoch: 2294, Training Loss: 0.074799, Validation Loss: 0.055931\n",
      "Epoch: 2295, Training Loss: 0.074977, Validation Loss: 0.057977\n",
      "Epoch: 2296, Training Loss: 0.075180, Validation Loss: 0.056672\n",
      "Epoch: 2297, Training Loss: 0.075218, Validation Loss: 0.058183\n",
      "Epoch: 2298, Training Loss: 0.075064, Validation Loss: 0.056762\n",
      "Epoch: 2299, Training Loss: 0.074773, Validation Loss: 0.057284\n",
      "Epoch: 2300, Training Loss: 0.074578, Validation Loss: 0.056136\n",
      "Epoch: 2301, Training Loss: 0.074554, Validation Loss: 0.056529\n",
      "Epoch: 2302, Training Loss: 0.074635, Validation Loss: 0.056862\n",
      "Epoch: 2303, Training Loss: 0.074734, Validation Loss: 0.056580\n",
      "Epoch: 2304, Training Loss: 0.074822, Validation Loss: 0.058098\n",
      "Epoch: 2305, Training Loss: 0.074821, Validation Loss: 0.056401\n",
      "Epoch: 2306, Training Loss: 0.074751, Validation Loss: 0.057890\n",
      "Epoch: 2307, Training Loss: 0.074611, Validation Loss: 0.056170\n",
      "Epoch: 2308, Training Loss: 0.074486, Validation Loss: 0.057019\n",
      "Epoch: 2309, Training Loss: 0.074439, Validation Loss: 0.056819\n",
      "Epoch: 2310, Training Loss: 0.074456, Validation Loss: 0.056450\n",
      "Epoch: 2311, Training Loss: 0.074503, Validation Loss: 0.057375\n",
      "Epoch: 2312, Training Loss: 0.074566, Validation Loss: 0.056483\n",
      "Epoch: 2313, Training Loss: 0.074586, Validation Loss: 0.057563\n",
      "Epoch: 2314, Training Loss: 0.074602, Validation Loss: 0.056319\n",
      "Epoch: 2315, Training Loss: 0.074570, Validation Loss: 0.057299\n",
      "Epoch: 2316, Training Loss: 0.074515, Validation Loss: 0.056162\n",
      "Epoch: 2317, Training Loss: 0.074451, Validation Loss: 0.057196\n",
      "Epoch: 2318, Training Loss: 0.074404, Validation Loss: 0.056451\n",
      "Epoch: 2319, Training Loss: 0.074353, Validation Loss: 0.057198\n",
      "Epoch: 2320, Training Loss: 0.074344, Validation Loss: 0.057007\n",
      "Epoch: 2321, Training Loss: 0.074337, Validation Loss: 0.056995\n",
      "Epoch: 2322, Training Loss: 0.074340, Validation Loss: 0.057339\n",
      "Epoch: 2323, Training Loss: 0.074368, Validation Loss: 0.056616\n",
      "Epoch: 2324, Training Loss: 0.074386, Validation Loss: 0.057284\n",
      "Epoch: 2325, Training Loss: 0.074428, Validation Loss: 0.056109\n",
      "Epoch: 2326, Training Loss: 0.074460, Validation Loss: 0.057519\n",
      "Epoch: 2327, Training Loss: 0.074538, Validation Loss: 0.056315\n",
      "Epoch: 2328, Training Loss: 0.074704, Validation Loss: 0.058740\n",
      "Epoch: 2329, Training Loss: 0.074943, Validation Loss: 0.056812\n",
      "Epoch: 2330, Training Loss: 0.075368, Validation Loss: 0.060126\n",
      "Epoch: 2331, Training Loss: 0.075983, Validation Loss: 0.056513\n",
      "Epoch: 2332, Training Loss: 0.076877, Validation Loss: 0.062076\n",
      "Epoch: 2333, Training Loss: 0.077851, Validation Loss: 0.057081\n",
      "Epoch: 2334, Training Loss: 0.079379, Validation Loss: 0.065608\n",
      "Epoch: 2335, Training Loss: 0.080865, Validation Loss: 0.059132\n",
      "Epoch: 2336, Training Loss: 0.083382, Validation Loss: 0.071224\n",
      "Epoch: 2337, Training Loss: 0.085075, Validation Loss: 0.061962\n",
      "Epoch: 2338, Training Loss: 0.087751, Validation Loss: 0.076396\n",
      "Epoch: 2339, Training Loss: 0.087799, Validation Loss: 0.063608\n",
      "Epoch: 2340, Training Loss: 0.088147, Validation Loss: 0.076512\n",
      "Epoch: 2341, Training Loss: 0.084586, Validation Loss: 0.061183\n",
      "Epoch: 2342, Training Loss: 0.081137, Validation Loss: 0.067069\n",
      "Epoch: 2343, Training Loss: 0.077036, Validation Loss: 0.056520\n",
      "Epoch: 2344, Training Loss: 0.074931, Validation Loss: 0.056787\n",
      "Epoch: 2345, Training Loss: 0.074948, Validation Loss: 0.058206\n",
      "Epoch: 2346, Training Loss: 0.076347, Validation Loss: 0.056036\n",
      "Epoch: 2347, Training Loss: 0.078167, Validation Loss: 0.064185\n",
      "Epoch: 2348, Training Loss: 0.078768, Validation Loss: 0.058743\n",
      "Epoch: 2349, Training Loss: 0.078664, Validation Loss: 0.064502\n",
      "Epoch: 2350, Training Loss: 0.077238, Validation Loss: 0.058512\n",
      "Epoch: 2351, Training Loss: 0.075865, Validation Loss: 0.059652\n",
      "Epoch: 2352, Training Loss: 0.074753, Validation Loss: 0.057399\n",
      "Epoch: 2353, Training Loss: 0.074410, Validation Loss: 0.056587\n",
      "Epoch: 2354, Training Loss: 0.074816, Validation Loss: 0.058420\n",
      "Epoch: 2355, Training Loss: 0.075506, Validation Loss: 0.056687\n",
      "Epoch: 2356, Training Loss: 0.076024, Validation Loss: 0.059800\n",
      "Epoch: 2357, Training Loss: 0.075982, Validation Loss: 0.057300\n",
      "Epoch: 2358, Training Loss: 0.075588, Validation Loss: 0.059230\n",
      "Epoch: 2359, Training Loss: 0.074923, Validation Loss: 0.056848\n",
      "Epoch: 2360, Training Loss: 0.074453, Validation Loss: 0.057839\n",
      "Epoch: 2361, Training Loss: 0.074279, Validation Loss: 0.056618\n",
      "Epoch: 2362, Training Loss: 0.074397, Validation Loss: 0.057346\n",
      "Epoch: 2363, Training Loss: 0.074611, Validation Loss: 0.057485\n",
      "Epoch: 2364, Training Loss: 0.074738, Validation Loss: 0.057270\n",
      "Epoch: 2365, Training Loss: 0.074758, Validation Loss: 0.058364\n",
      "Epoch: 2366, Training Loss: 0.074687, Validation Loss: 0.056753\n",
      "Epoch: 2367, Training Loss: 0.074735, Validation Loss: 0.058870\n",
      "Epoch: 2368, Training Loss: 0.074843, Validation Loss: 0.055977\n",
      "Epoch: 2369, Training Loss: 0.074989, Validation Loss: 0.059172\n",
      "Epoch: 2370, Training Loss: 0.074964, Validation Loss: 0.055829\n",
      "Epoch: 2371, Training Loss: 0.074865, Validation Loss: 0.058967\n",
      "Epoch: 2372, Training Loss: 0.074679, Validation Loss: 0.056078\n",
      "Epoch: 2373, Training Loss: 0.074547, Validation Loss: 0.058687\n",
      "Epoch: 2374, Training Loss: 0.074476, Validation Loss: 0.056576\n",
      "Epoch: 2375, Training Loss: 0.074535, Validation Loss: 0.058617\n",
      "Epoch: 2376, Training Loss: 0.074628, Validation Loss: 0.056685\n",
      "Epoch: 2377, Training Loss: 0.074798, Validation Loss: 0.058943\n",
      "Epoch: 2378, Training Loss: 0.074957, Validation Loss: 0.056263\n",
      "Epoch: 2379, Training Loss: 0.075228, Validation Loss: 0.059770\n",
      "Epoch: 2380, Training Loss: 0.075569, Validation Loss: 0.056148\n",
      "Epoch: 2381, Training Loss: 0.076245, Validation Loss: 0.061639\n",
      "Epoch: 2382, Training Loss: 0.077122, Validation Loss: 0.056758\n",
      "Epoch: 2383, Training Loss: 0.078562, Validation Loss: 0.065207\n",
      "Epoch: 2384, Training Loss: 0.080596, Validation Loss: 0.058994\n",
      "Epoch: 2385, Training Loss: 0.084995, Validation Loss: 0.074336\n",
      "Epoch: 2386, Training Loss: 0.089296, Validation Loss: 0.065383\n",
      "Epoch: 2387, Training Loss: 0.095214, Validation Loss: 0.086853\n",
      "Epoch: 2388, Training Loss: 0.095183, Validation Loss: 0.069276\n",
      "Epoch: 2389, Training Loss: 0.093839, Validation Loss: 0.082426\n",
      "Epoch: 2390, Training Loss: 0.085783, Validation Loss: 0.060817\n",
      "Epoch: 2391, Training Loss: 0.079884, Validation Loss: 0.062017\n",
      "Epoch: 2392, Training Loss: 0.077012, Validation Loss: 0.057296\n",
      "Epoch: 2393, Training Loss: 0.077997, Validation Loss: 0.055124\n",
      "Epoch: 2394, Training Loss: 0.079195, Validation Loss: 0.064376\n",
      "Epoch: 2395, Training Loss: 0.078223, Validation Loss: 0.055634\n",
      "Epoch: 2396, Training Loss: 0.076965, Validation Loss: 0.060505\n",
      "Epoch: 2397, Training Loss: 0.077244, Validation Loss: 0.058774\n",
      "Epoch: 2398, Training Loss: 0.079130, Validation Loss: 0.059302\n",
      "Epoch: 2399, Training Loss: 0.079361, Validation Loss: 0.063994\n",
      "Epoch: 2400, Training Loss: 0.077707, Validation Loss: 0.057983\n",
      "Epoch: 2401, Training Loss: 0.075173, Validation Loss: 0.058406\n",
      "Epoch: 2402, Training Loss: 0.074406, Validation Loss: 0.056996\n",
      "Epoch: 2403, Training Loss: 0.075396, Validation Loss: 0.055553\n",
      "Epoch: 2404, Training Loss: 0.076253, Validation Loss: 0.060122\n",
      "Epoch: 2405, Training Loss: 0.076080, Validation Loss: 0.056682\n",
      "Epoch: 2406, Training Loss: 0.075412, Validation Loss: 0.057307\n",
      "Epoch: 2407, Training Loss: 0.075449, Validation Loss: 0.059021\n",
      "Epoch: 2408, Training Loss: 0.075649, Validation Loss: 0.055888\n",
      "Epoch: 2409, Training Loss: 0.075306, Validation Loss: 0.059535\n",
      "Epoch: 2410, Training Loss: 0.074575, Validation Loss: 0.055535\n",
      "Epoch: 2411, Training Loss: 0.074279, Validation Loss: 0.056120\n",
      "Epoch: 2412, Training Loss: 0.074613, Validation Loss: 0.057701\n",
      "Epoch: 2413, Training Loss: 0.075093, Validation Loss: 0.055657\n",
      "Epoch: 2414, Training Loss: 0.075272, Validation Loss: 0.059460\n",
      "Epoch: 2415, Training Loss: 0.075062, Validation Loss: 0.056514\n",
      "Epoch: 2416, Training Loss: 0.074844, Validation Loss: 0.058334\n",
      "Epoch: 2417, Training Loss: 0.074712, Validation Loss: 0.057419\n",
      "Epoch: 2418, Training Loss: 0.074591, Validation Loss: 0.057118\n",
      "Epoch: 2419, Training Loss: 0.074385, Validation Loss: 0.057823\n",
      "Epoch: 2420, Training Loss: 0.074226, Validation Loss: 0.056823\n",
      "Epoch: 2421, Training Loss: 0.074251, Validation Loss: 0.057555\n",
      "Epoch: 2422, Training Loss: 0.074339, Validation Loss: 0.057321\n",
      "Epoch: 2423, Training Loss: 0.074482, Validation Loss: 0.057190\n",
      "Epoch: 2424, Training Loss: 0.074468, Validation Loss: 0.057305\n",
      "Epoch: 2425, Training Loss: 0.074345, Validation Loss: 0.057086\n",
      "Epoch: 2426, Training Loss: 0.074225, Validation Loss: 0.056746\n",
      "Epoch: 2427, Training Loss: 0.074150, Validation Loss: 0.057588\n",
      "Epoch: 2428, Training Loss: 0.074218, Validation Loss: 0.056588\n",
      "Epoch: 2429, Training Loss: 0.074312, Validation Loss: 0.058543\n",
      "Epoch: 2430, Training Loss: 0.074325, Validation Loss: 0.056586\n",
      "Epoch: 2431, Training Loss: 0.074265, Validation Loss: 0.058196\n",
      "Epoch: 2432, Training Loss: 0.074181, Validation Loss: 0.056495\n",
      "Epoch: 2433, Training Loss: 0.074119, Validation Loss: 0.057867\n",
      "Epoch: 2434, Training Loss: 0.074097, Validation Loss: 0.057200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2435, Training Loss: 0.074141, Validation Loss: 0.057709\n",
      "Epoch: 2436, Training Loss: 0.074150, Validation Loss: 0.057116\n",
      "Epoch: 2437, Training Loss: 0.074128, Validation Loss: 0.057156\n",
      "Epoch: 2438, Training Loss: 0.074120, Validation Loss: 0.056453\n",
      "Epoch: 2439, Training Loss: 0.074095, Validation Loss: 0.057502\n",
      "Epoch: 2440, Training Loss: 0.074155, Validation Loss: 0.056632\n",
      "Epoch: 2441, Training Loss: 0.074318, Validation Loss: 0.059060\n",
      "Epoch: 2442, Training Loss: 0.074600, Validation Loss: 0.056891\n",
      "Epoch: 2443, Training Loss: 0.075164, Validation Loss: 0.060669\n",
      "Epoch: 2444, Training Loss: 0.076200, Validation Loss: 0.056862\n",
      "Epoch: 2445, Training Loss: 0.078250, Validation Loss: 0.065253\n",
      "Epoch: 2446, Training Loss: 0.081047, Validation Loss: 0.059476\n",
      "Epoch: 2447, Training Loss: 0.085688, Validation Loss: 0.075046\n",
      "Epoch: 2448, Training Loss: 0.090285, Validation Loss: 0.066043\n",
      "Epoch: 2449, Training Loss: 0.097882, Validation Loss: 0.089284\n",
      "Epoch: 2450, Training Loss: 0.100000, Validation Loss: 0.073466\n",
      "Epoch: 2451, Training Loss: 0.103038, Validation Loss: 0.094523\n",
      "Epoch: 2452, Training Loss: 0.093840, Validation Loss: 0.067975\n",
      "Epoch: 2453, Training Loss: 0.086550, Validation Loss: 0.071407\n",
      "Epoch: 2454, Training Loss: 0.080973, Validation Loss: 0.060679\n",
      "Epoch: 2455, Training Loss: 0.082109, Validation Loss: 0.058990\n",
      "Epoch: 2456, Training Loss: 0.083217, Validation Loss: 0.068498\n",
      "Epoch: 2457, Training Loss: 0.080099, Validation Loss: 0.056566\n",
      "Epoch: 2458, Training Loss: 0.075568, Validation Loss: 0.059521\n",
      "Epoch: 2459, Training Loss: 0.075180, Validation Loss: 0.057959\n",
      "Epoch: 2460, Training Loss: 0.078865, Validation Loss: 0.057816\n",
      "Epoch: 2461, Training Loss: 0.080779, Validation Loss: 0.066114\n",
      "Epoch: 2462, Training Loss: 0.079062, Validation Loss: 0.058679\n",
      "Epoch: 2463, Training Loss: 0.077053, Validation Loss: 0.058346\n",
      "Epoch: 2464, Training Loss: 0.077926, Validation Loss: 0.062804\n",
      "Epoch: 2465, Training Loss: 0.078630, Validation Loss: 0.057637\n",
      "Epoch: 2466, Training Loss: 0.077566, Validation Loss: 0.064215\n",
      "Epoch: 2467, Training Loss: 0.075141, Validation Loss: 0.056894\n",
      "Epoch: 2468, Training Loss: 0.074688, Validation Loss: 0.056253\n",
      "Epoch: 2469, Training Loss: 0.075964, Validation Loss: 0.060220\n",
      "Epoch: 2470, Training Loss: 0.076110, Validation Loss: 0.054955\n",
      "Epoch: 2471, Training Loss: 0.075236, Validation Loss: 0.057633\n",
      "Epoch: 2472, Training Loss: 0.075266, Validation Loss: 0.056790\n",
      "Epoch: 2473, Training Loss: 0.076203, Validation Loss: 0.056410\n",
      "Epoch: 2474, Training Loss: 0.076007, Validation Loss: 0.059199\n",
      "Epoch: 2475, Training Loss: 0.074891, Validation Loss: 0.056289\n",
      "Epoch: 2476, Training Loss: 0.074622, Validation Loss: 0.055232\n",
      "Epoch: 2477, Training Loss: 0.075121, Validation Loss: 0.058294\n",
      "Epoch: 2478, Training Loss: 0.075033, Validation Loss: 0.055170\n",
      "Epoch: 2479, Training Loss: 0.074419, Validation Loss: 0.057192\n",
      "Epoch: 2480, Training Loss: 0.074331, Validation Loss: 0.057405\n",
      "Epoch: 2481, Training Loss: 0.074793, Validation Loss: 0.056253\n",
      "Epoch: 2482, Training Loss: 0.074984, Validation Loss: 0.059583\n",
      "Epoch: 2483, Training Loss: 0.074635, Validation Loss: 0.055992\n",
      "Epoch: 2484, Training Loss: 0.074388, Validation Loss: 0.056839\n",
      "Epoch: 2485, Training Loss: 0.074477, Validation Loss: 0.057006\n",
      "Epoch: 2486, Training Loss: 0.074506, Validation Loss: 0.055899\n",
      "Epoch: 2487, Training Loss: 0.074280, Validation Loss: 0.057703\n",
      "Epoch: 2488, Training Loss: 0.074068, Validation Loss: 0.056936\n",
      "Epoch: 2489, Training Loss: 0.074128, Validation Loss: 0.057398\n",
      "Epoch: 2490, Training Loss: 0.074318, Validation Loss: 0.058672\n",
      "Epoch: 2491, Training Loss: 0.074389, Validation Loss: 0.057111\n",
      "Epoch: 2492, Training Loss: 0.074229, Validation Loss: 0.057906\n",
      "Epoch: 2493, Training Loss: 0.074058, Validation Loss: 0.056781\n",
      "Epoch: 2494, Training Loss: 0.074017, Validation Loss: 0.056701\n",
      "Epoch: 2495, Training Loss: 0.074099, Validation Loss: 0.057621\n",
      "Epoch: 2496, Training Loss: 0.074159, Validation Loss: 0.056519\n",
      "Epoch: 2497, Training Loss: 0.074097, Validation Loss: 0.057777\n",
      "Epoch: 2498, Training Loss: 0.073979, Validation Loss: 0.056991\n",
      "Epoch: 2499, Training Loss: 0.073948, Validation Loss: 0.057286\n",
      "Epoch: 2500, Training Loss: 0.073961, Validation Loss: 0.057879\n",
      "Epoch: 2501, Training Loss: 0.074069, Validation Loss: 0.056821\n",
      "Epoch: 2502, Training Loss: 0.074130, Validation Loss: 0.058564\n",
      "Epoch: 2503, Training Loss: 0.074172, Validation Loss: 0.056585\n",
      "Epoch: 2504, Training Loss: 0.074174, Validation Loss: 0.058456\n",
      "Epoch: 2505, Training Loss: 0.074196, Validation Loss: 0.056722\n",
      "Epoch: 2506, Training Loss: 0.074288, Validation Loss: 0.058501\n",
      "Epoch: 2507, Training Loss: 0.074426, Validation Loss: 0.056925\n",
      "Epoch: 2508, Training Loss: 0.074600, Validation Loss: 0.058570\n",
      "Epoch: 2509, Training Loss: 0.074736, Validation Loss: 0.056901\n",
      "Epoch: 2510, Training Loss: 0.074940, Validation Loss: 0.059631\n",
      "Epoch: 2511, Training Loss: 0.075263, Validation Loss: 0.057073\n",
      "Epoch: 2512, Training Loss: 0.075956, Validation Loss: 0.061905\n",
      "Epoch: 2513, Training Loss: 0.076968, Validation Loss: 0.057873\n",
      "Epoch: 2514, Training Loss: 0.078687, Validation Loss: 0.066080\n",
      "Epoch: 2515, Training Loss: 0.080293, Validation Loss: 0.059550\n",
      "Epoch: 2516, Training Loss: 0.082533, Validation Loss: 0.070973\n",
      "Epoch: 2517, Training Loss: 0.083577, Validation Loss: 0.061621\n",
      "Epoch: 2518, Training Loss: 0.085640, Validation Loss: 0.073853\n",
      "Epoch: 2519, Training Loss: 0.085367, Validation Loss: 0.062801\n",
      "Epoch: 2520, Training Loss: 0.085987, Validation Loss: 0.073130\n",
      "Epoch: 2521, Training Loss: 0.083609, Validation Loss: 0.061905\n",
      "Epoch: 2522, Training Loss: 0.082071, Validation Loss: 0.067307\n",
      "Epoch: 2523, Training Loss: 0.079205, Validation Loss: 0.059975\n",
      "Epoch: 2524, Training Loss: 0.077449, Validation Loss: 0.060158\n",
      "Epoch: 2525, Training Loss: 0.076015, Validation Loss: 0.059237\n",
      "Epoch: 2526, Training Loss: 0.075298, Validation Loss: 0.056520\n",
      "Epoch: 2527, Training Loss: 0.074993, Validation Loss: 0.059979\n",
      "Epoch: 2528, Training Loss: 0.074968, Validation Loss: 0.056609\n",
      "Epoch: 2529, Training Loss: 0.075251, Validation Loss: 0.060663\n",
      "Epoch: 2530, Training Loss: 0.075752, Validation Loss: 0.058420\n",
      "Epoch: 2531, Training Loss: 0.076363, Validation Loss: 0.060741\n",
      "Epoch: 2532, Training Loss: 0.076717, Validation Loss: 0.059819\n",
      "Epoch: 2533, Training Loss: 0.076843, Validation Loss: 0.060735\n",
      "Epoch: 2534, Training Loss: 0.076286, Validation Loss: 0.059586\n",
      "Epoch: 2535, Training Loss: 0.075613, Validation Loss: 0.059577\n",
      "Epoch: 2536, Training Loss: 0.074777, Validation Loss: 0.057944\n",
      "Epoch: 2537, Training Loss: 0.074169, Validation Loss: 0.058264\n",
      "Epoch: 2538, Training Loss: 0.073976, Validation Loss: 0.056934\n",
      "Epoch: 2539, Training Loss: 0.074044, Validation Loss: 0.058603\n",
      "Epoch: 2540, Training Loss: 0.074426, Validation Loss: 0.056542\n",
      "Epoch: 2541, Training Loss: 0.074684, Validation Loss: 0.059139\n",
      "Epoch: 2542, Training Loss: 0.074697, Validation Loss: 0.056328\n",
      "Epoch: 2543, Training Loss: 0.074429, Validation Loss: 0.058263\n",
      "Epoch: 2544, Training Loss: 0.074084, Validation Loss: 0.056623\n",
      "Epoch: 2545, Training Loss: 0.073810, Validation Loss: 0.057775\n",
      "Epoch: 2546, Training Loss: 0.073754, Validation Loss: 0.057821\n",
      "Epoch: 2547, Training Loss: 0.073794, Validation Loss: 0.057494\n",
      "Epoch: 2548, Training Loss: 0.073963, Validation Loss: 0.058506\n",
      "Epoch: 2549, Training Loss: 0.074149, Validation Loss: 0.056955\n",
      "Epoch: 2550, Training Loss: 0.074257, Validation Loss: 0.058823\n",
      "Epoch: 2551, Training Loss: 0.074283, Validation Loss: 0.056466\n",
      "Epoch: 2552, Training Loss: 0.074296, Validation Loss: 0.059142\n",
      "Epoch: 2553, Training Loss: 0.074398, Validation Loss: 0.056484\n",
      "Epoch: 2554, Training Loss: 0.074773, Validation Loss: 0.060188\n",
      "Epoch: 2555, Training Loss: 0.075693, Validation Loss: 0.057034\n",
      "Epoch: 2556, Training Loss: 0.077780, Validation Loss: 0.064637\n",
      "Epoch: 2557, Training Loss: 0.081048, Validation Loss: 0.060172\n",
      "Epoch: 2558, Training Loss: 0.086668, Validation Loss: 0.076521\n",
      "Epoch: 2559, Training Loss: 0.092228, Validation Loss: 0.068353\n",
      "Epoch: 2560, Training Loss: 0.102009, Validation Loss: 0.094649\n",
      "Epoch: 2561, Training Loss: 0.104568, Validation Loss: 0.077886\n",
      "Epoch: 2562, Training Loss: 0.108330, Validation Loss: 0.101219\n",
      "Epoch: 2563, Training Loss: 0.095972, Validation Loss: 0.069711\n",
      "Epoch: 2564, Training Loss: 0.086620, Validation Loss: 0.071076\n",
      "Epoch: 2565, Training Loss: 0.082005, Validation Loss: 0.062659\n",
      "Epoch: 2566, Training Loss: 0.086681, Validation Loss: 0.062184\n",
      "Epoch: 2567, Training Loss: 0.088289, Validation Loss: 0.074738\n",
      "Epoch: 2568, Training Loss: 0.082095, Validation Loss: 0.058899\n",
      "Epoch: 2569, Training Loss: 0.075278, Validation Loss: 0.058188\n",
      "Epoch: 2570, Training Loss: 0.076133, Validation Loss: 0.061576\n",
      "Epoch: 2571, Training Loss: 0.079607, Validation Loss: 0.057826\n",
      "Epoch: 2572, Training Loss: 0.078237, Validation Loss: 0.064556\n",
      "Epoch: 2573, Training Loss: 0.074871, Validation Loss: 0.056295\n",
      "Epoch: 2574, Training Loss: 0.076362, Validation Loss: 0.056136\n",
      "Epoch: 2575, Training Loss: 0.079405, Validation Loss: 0.065551\n",
      "Epoch: 2576, Training Loss: 0.077644, Validation Loss: 0.057026\n",
      "Epoch: 2577, Training Loss: 0.075941, Validation Loss: 0.058413\n",
      "Epoch: 2578, Training Loss: 0.077135, Validation Loss: 0.061502\n",
      "Epoch: 2579, Training Loss: 0.077191, Validation Loss: 0.056919\n",
      "Epoch: 2580, Training Loss: 0.074892, Validation Loss: 0.057810\n",
      "Epoch: 2581, Training Loss: 0.074733, Validation Loss: 0.058302\n",
      "Epoch: 2582, Training Loss: 0.075747, Validation Loss: 0.055417\n",
      "Epoch: 2583, Training Loss: 0.074935, Validation Loss: 0.058922\n",
      "Epoch: 2584, Training Loss: 0.074186, Validation Loss: 0.056893\n",
      "Epoch: 2585, Training Loss: 0.075196, Validation Loss: 0.055775\n",
      "Epoch: 2586, Training Loss: 0.075843, Validation Loss: 0.061354\n",
      "Epoch: 2587, Training Loss: 0.074951, Validation Loss: 0.056434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2588, Training Loss: 0.074907, Validation Loss: 0.057081\n",
      "Epoch: 2589, Training Loss: 0.075314, Validation Loss: 0.059227\n",
      "Epoch: 2590, Training Loss: 0.074742, Validation Loss: 0.055993\n",
      "Epoch: 2591, Training Loss: 0.074021, Validation Loss: 0.056343\n",
      "Epoch: 2592, Training Loss: 0.074262, Validation Loss: 0.057893\n",
      "Epoch: 2593, Training Loss: 0.074536, Validation Loss: 0.055968\n",
      "Epoch: 2594, Training Loss: 0.074245, Validation Loss: 0.057678\n",
      "Epoch: 2595, Training Loss: 0.074175, Validation Loss: 0.057626\n",
      "Epoch: 2596, Training Loss: 0.074571, Validation Loss: 0.055906\n",
      "Epoch: 2597, Training Loss: 0.074738, Validation Loss: 0.059103\n",
      "Epoch: 2598, Training Loss: 0.074414, Validation Loss: 0.055837\n",
      "Epoch: 2599, Training Loss: 0.074163, Validation Loss: 0.057112\n",
      "Epoch: 2600, Training Loss: 0.074226, Validation Loss: 0.057471\n",
      "Epoch: 2601, Training Loss: 0.074216, Validation Loss: 0.056132\n",
      "Epoch: 2602, Training Loss: 0.073943, Validation Loss: 0.057298\n",
      "Epoch: 2603, Training Loss: 0.073773, Validation Loss: 0.056616\n",
      "Epoch: 2604, Training Loss: 0.073880, Validation Loss: 0.056285\n",
      "Epoch: 2605, Training Loss: 0.074026, Validation Loss: 0.057747\n",
      "Epoch: 2606, Training Loss: 0.073977, Validation Loss: 0.056789\n",
      "Epoch: 2607, Training Loss: 0.073856, Validation Loss: 0.057104\n",
      "Epoch: 2608, Training Loss: 0.073859, Validation Loss: 0.057775\n",
      "Epoch: 2609, Training Loss: 0.073903, Validation Loss: 0.056592\n",
      "Epoch: 2610, Training Loss: 0.073903, Validation Loss: 0.058217\n",
      "Epoch: 2611, Training Loss: 0.073801, Validation Loss: 0.056677\n",
      "Epoch: 2612, Training Loss: 0.073721, Validation Loss: 0.057451\n",
      "Epoch: 2613, Training Loss: 0.073690, Validation Loss: 0.057334\n",
      "Epoch: 2614, Training Loss: 0.073711, Validation Loss: 0.057162\n",
      "Epoch: 2615, Training Loss: 0.073703, Validation Loss: 0.058038\n",
      "Epoch: 2616, Training Loss: 0.073677, Validation Loss: 0.057101\n",
      "Epoch: 2617, Training Loss: 0.073658, Validation Loss: 0.057915\n",
      "Epoch: 2618, Training Loss: 0.073652, Validation Loss: 0.057351\n",
      "Epoch: 2619, Training Loss: 0.073688, Validation Loss: 0.057643\n",
      "Epoch: 2620, Training Loss: 0.073721, Validation Loss: 0.057705\n",
      "Epoch: 2621, Training Loss: 0.073698, Validation Loss: 0.057700\n",
      "Epoch: 2622, Training Loss: 0.073675, Validation Loss: 0.057532\n",
      "Epoch: 2623, Training Loss: 0.073654, Validation Loss: 0.058023\n",
      "Epoch: 2624, Training Loss: 0.073677, Validation Loss: 0.056961\n",
      "Epoch: 2625, Training Loss: 0.073750, Validation Loss: 0.058259\n",
      "Epoch: 2626, Training Loss: 0.073814, Validation Loss: 0.056642\n",
      "Epoch: 2627, Training Loss: 0.073885, Validation Loss: 0.058700\n",
      "Epoch: 2628, Training Loss: 0.073942, Validation Loss: 0.056882\n",
      "Epoch: 2629, Training Loss: 0.074041, Validation Loss: 0.059217\n",
      "Epoch: 2630, Training Loss: 0.074270, Validation Loss: 0.057197\n",
      "Epoch: 2631, Training Loss: 0.074719, Validation Loss: 0.060222\n",
      "Epoch: 2632, Training Loss: 0.075472, Validation Loss: 0.057467\n",
      "Epoch: 2633, Training Loss: 0.076703, Validation Loss: 0.063059\n",
      "Epoch: 2634, Training Loss: 0.078190, Validation Loss: 0.058449\n",
      "Epoch: 2635, Training Loss: 0.080376, Validation Loss: 0.067708\n",
      "Epoch: 2636, Training Loss: 0.082174, Validation Loss: 0.060973\n",
      "Epoch: 2637, Training Loss: 0.085701, Validation Loss: 0.074165\n",
      "Epoch: 2638, Training Loss: 0.087240, Validation Loss: 0.064747\n",
      "Epoch: 2639, Training Loss: 0.090226, Validation Loss: 0.078942\n",
      "Epoch: 2640, Training Loss: 0.088752, Validation Loss: 0.066343\n",
      "Epoch: 2641, Training Loss: 0.088818, Validation Loss: 0.076056\n",
      "Epoch: 2642, Training Loss: 0.084774, Validation Loss: 0.063874\n",
      "Epoch: 2643, Training Loss: 0.082202, Validation Loss: 0.066979\n",
      "Epoch: 2644, Training Loss: 0.078670, Validation Loss: 0.060575\n",
      "Epoch: 2645, Training Loss: 0.076700, Validation Loss: 0.059022\n",
      "Epoch: 2646, Training Loss: 0.075520, Validation Loss: 0.060190\n",
      "Epoch: 2647, Training Loss: 0.075113, Validation Loss: 0.056232\n",
      "Epoch: 2648, Training Loss: 0.075024, Validation Loss: 0.060661\n",
      "Epoch: 2649, Training Loss: 0.075142, Validation Loss: 0.056887\n",
      "Epoch: 2650, Training Loss: 0.075675, Validation Loss: 0.060478\n",
      "Epoch: 2651, Training Loss: 0.076398, Validation Loss: 0.059755\n",
      "Epoch: 2652, Training Loss: 0.077050, Validation Loss: 0.060750\n",
      "Epoch: 2653, Training Loss: 0.076832, Validation Loss: 0.061147\n",
      "Epoch: 2654, Training Loss: 0.076400, Validation Loss: 0.059656\n",
      "Epoch: 2655, Training Loss: 0.075280, Validation Loss: 0.059927\n",
      "Epoch: 2656, Training Loss: 0.074257, Validation Loss: 0.057738\n",
      "Epoch: 2657, Training Loss: 0.073635, Validation Loss: 0.057253\n",
      "Epoch: 2658, Training Loss: 0.073586, Validation Loss: 0.057367\n",
      "Epoch: 2659, Training Loss: 0.074009, Validation Loss: 0.056747\n",
      "Epoch: 2660, Training Loss: 0.074419, Validation Loss: 0.058776\n",
      "Epoch: 2661, Training Loss: 0.074632, Validation Loss: 0.057494\n",
      "Epoch: 2662, Training Loss: 0.074457, Validation Loss: 0.058980\n",
      "Epoch: 2663, Training Loss: 0.074110, Validation Loss: 0.057589\n",
      "Epoch: 2664, Training Loss: 0.073738, Validation Loss: 0.057370\n",
      "Epoch: 2665, Training Loss: 0.073588, Validation Loss: 0.057783\n",
      "Epoch: 2666, Training Loss: 0.073716, Validation Loss: 0.056801\n",
      "Epoch: 2667, Training Loss: 0.073961, Validation Loss: 0.059402\n",
      "Epoch: 2668, Training Loss: 0.074229, Validation Loss: 0.057117\n",
      "Epoch: 2669, Training Loss: 0.074348, Validation Loss: 0.060472\n",
      "Epoch: 2670, Training Loss: 0.074344, Validation Loss: 0.056730\n",
      "Epoch: 2671, Training Loss: 0.074454, Validation Loss: 0.060032\n",
      "Epoch: 2672, Training Loss: 0.074755, Validation Loss: 0.056521\n",
      "Epoch: 2673, Training Loss: 0.075447, Validation Loss: 0.061128\n",
      "Epoch: 2674, Training Loss: 0.076294, Validation Loss: 0.057648\n",
      "Epoch: 2675, Training Loss: 0.077691, Validation Loss: 0.064619\n",
      "Epoch: 2676, Training Loss: 0.079037, Validation Loss: 0.059711\n",
      "Epoch: 2677, Training Loss: 0.081807, Validation Loss: 0.070247\n",
      "Epoch: 2678, Training Loss: 0.084043, Validation Loss: 0.062592\n",
      "Epoch: 2679, Training Loss: 0.087904, Validation Loss: 0.077967\n",
      "Epoch: 2680, Training Loss: 0.088713, Validation Loss: 0.065332\n",
      "Epoch: 2681, Training Loss: 0.089615, Validation Loss: 0.079611\n",
      "Epoch: 2682, Training Loss: 0.085615, Validation Loss: 0.062240\n",
      "Epoch: 2683, Training Loss: 0.081628, Validation Loss: 0.067719\n",
      "Epoch: 2684, Training Loss: 0.077339, Validation Loss: 0.057379\n",
      "Epoch: 2685, Training Loss: 0.076120, Validation Loss: 0.057391\n",
      "Epoch: 2686, Training Loss: 0.076693, Validation Loss: 0.061861\n",
      "Epoch: 2687, Training Loss: 0.077771, Validation Loss: 0.057319\n",
      "Epoch: 2688, Training Loss: 0.077811, Validation Loss: 0.065445\n",
      "Epoch: 2689, Training Loss: 0.076410, Validation Loss: 0.056989\n",
      "Epoch: 2690, Training Loss: 0.074875, Validation Loss: 0.060258\n",
      "Epoch: 2691, Training Loss: 0.074808, Validation Loss: 0.058338\n",
      "Epoch: 2692, Training Loss: 0.076383, Validation Loss: 0.057516\n",
      "Epoch: 2693, Training Loss: 0.077576, Validation Loss: 0.063105\n",
      "Epoch: 2694, Training Loss: 0.077457, Validation Loss: 0.057999\n",
      "Epoch: 2695, Training Loss: 0.075786, Validation Loss: 0.061220\n",
      "Epoch: 2696, Training Loss: 0.074257, Validation Loss: 0.057321\n",
      "Epoch: 2697, Training Loss: 0.073883, Validation Loss: 0.056750\n",
      "Epoch: 2698, Training Loss: 0.074625, Validation Loss: 0.060313\n",
      "Epoch: 2699, Training Loss: 0.075206, Validation Loss: 0.056322\n",
      "Epoch: 2700, Training Loss: 0.074808, Validation Loss: 0.060178\n",
      "Epoch: 2701, Training Loss: 0.073996, Validation Loss: 0.056278\n",
      "Epoch: 2702, Training Loss: 0.073578, Validation Loss: 0.056971\n",
      "Epoch: 2703, Training Loss: 0.073923, Validation Loss: 0.058650\n",
      "Epoch: 2704, Training Loss: 0.074401, Validation Loss: 0.055928\n",
      "Epoch: 2705, Training Loss: 0.074380, Validation Loss: 0.059449\n",
      "Epoch: 2706, Training Loss: 0.074070, Validation Loss: 0.055914\n",
      "Epoch: 2707, Training Loss: 0.073703, Validation Loss: 0.057811\n",
      "Epoch: 2708, Training Loss: 0.073592, Validation Loss: 0.056957\n",
      "Epoch: 2709, Training Loss: 0.073714, Validation Loss: 0.057066\n",
      "Epoch: 2710, Training Loss: 0.073811, Validation Loss: 0.058621\n",
      "Epoch: 2711, Training Loss: 0.073732, Validation Loss: 0.056985\n",
      "Epoch: 2712, Training Loss: 0.073542, Validation Loss: 0.058109\n",
      "Epoch: 2713, Training Loss: 0.073437, Validation Loss: 0.057048\n",
      "Epoch: 2714, Training Loss: 0.073512, Validation Loss: 0.057362\n",
      "Epoch: 2715, Training Loss: 0.073600, Validation Loss: 0.057715\n",
      "Epoch: 2716, Training Loss: 0.073660, Validation Loss: 0.057289\n",
      "Epoch: 2717, Training Loss: 0.073641, Validation Loss: 0.057729\n",
      "Epoch: 2718, Training Loss: 0.073552, Validation Loss: 0.057457\n",
      "Epoch: 2719, Training Loss: 0.073485, Validation Loss: 0.056991\n",
      "Epoch: 2720, Training Loss: 0.073469, Validation Loss: 0.057771\n",
      "Epoch: 2721, Training Loss: 0.073517, Validation Loss: 0.056740\n",
      "Epoch: 2722, Training Loss: 0.073620, Validation Loss: 0.058687\n",
      "Epoch: 2723, Training Loss: 0.073679, Validation Loss: 0.056886\n",
      "Epoch: 2724, Training Loss: 0.073695, Validation Loss: 0.059036\n",
      "Epoch: 2725, Training Loss: 0.073775, Validation Loss: 0.056580\n",
      "Epoch: 2726, Training Loss: 0.073950, Validation Loss: 0.059003\n",
      "Epoch: 2727, Training Loss: 0.074195, Validation Loss: 0.056686\n",
      "Epoch: 2728, Training Loss: 0.074675, Validation Loss: 0.060526\n",
      "Epoch: 2729, Training Loss: 0.075324, Validation Loss: 0.057771\n",
      "Epoch: 2730, Training Loss: 0.076467, Validation Loss: 0.063628\n",
      "Epoch: 2731, Training Loss: 0.077875, Validation Loss: 0.059096\n",
      "Epoch: 2732, Training Loss: 0.080607, Validation Loss: 0.068874\n",
      "Epoch: 2733, Training Loss: 0.083403, Validation Loss: 0.062072\n",
      "Epoch: 2734, Training Loss: 0.088030, Validation Loss: 0.077786\n",
      "Epoch: 2735, Training Loss: 0.090454, Validation Loss: 0.067016\n",
      "Epoch: 2736, Training Loss: 0.093727, Validation Loss: 0.084415\n",
      "Epoch: 2737, Training Loss: 0.090412, Validation Loss: 0.066667\n",
      "Epoch: 2738, Training Loss: 0.087879, Validation Loss: 0.075017\n",
      "Epoch: 2739, Training Loss: 0.082816, Validation Loss: 0.062640\n",
      "Epoch: 2740, Training Loss: 0.081612, Validation Loss: 0.063246\n",
      "Epoch: 2741, Training Loss: 0.080963, Validation Loss: 0.065827\n",
      "Epoch: 2742, Training Loss: 0.080760, Validation Loss: 0.060607\n",
      "Epoch: 2743, Training Loss: 0.078312, Validation Loss: 0.065503\n",
      "Epoch: 2744, Training Loss: 0.075407, Validation Loss: 0.056789\n",
      "Epoch: 2745, Training Loss: 0.073856, Validation Loss: 0.058362\n",
      "Epoch: 2746, Training Loss: 0.074931, Validation Loss: 0.059273\n",
      "Epoch: 2747, Training Loss: 0.077255, Validation Loss: 0.057954\n",
      "Epoch: 2748, Training Loss: 0.077891, Validation Loss: 0.062693\n",
      "Epoch: 2749, Training Loss: 0.076970, Validation Loss: 0.058186\n",
      "Epoch: 2750, Training Loss: 0.075220, Validation Loss: 0.058268\n",
      "Epoch: 2751, Training Loss: 0.074812, Validation Loss: 0.059458\n",
      "Epoch: 2752, Training Loss: 0.075174, Validation Loss: 0.056021\n",
      "Epoch: 2753, Training Loss: 0.075115, Validation Loss: 0.060779\n",
      "Epoch: 2754, Training Loss: 0.074277, Validation Loss: 0.055459\n",
      "Epoch: 2755, Training Loss: 0.073591, Validation Loss: 0.056473\n",
      "Epoch: 2756, Training Loss: 0.073866, Validation Loss: 0.057470\n",
      "Epoch: 2757, Training Loss: 0.074618, Validation Loss: 0.055449\n",
      "Epoch: 2758, Training Loss: 0.074913, Validation Loss: 0.060078\n",
      "Epoch: 2759, Training Loss: 0.074684, Validation Loss: 0.055985\n",
      "Epoch: 2760, Training Loss: 0.074358, Validation Loss: 0.058430\n",
      "Epoch: 2761, Training Loss: 0.074261, Validation Loss: 0.057158\n",
      "Epoch: 2762, Training Loss: 0.074261, Validation Loss: 0.056731\n",
      "Epoch: 2763, Training Loss: 0.073977, Validation Loss: 0.057540\n",
      "Epoch: 2764, Training Loss: 0.073576, Validation Loss: 0.056324\n",
      "Epoch: 2765, Training Loss: 0.073368, Validation Loss: 0.056898\n",
      "Epoch: 2766, Training Loss: 0.073475, Validation Loss: 0.057720\n",
      "Epoch: 2767, Training Loss: 0.073754, Validation Loss: 0.057711\n",
      "Epoch: 2768, Training Loss: 0.073823, Validation Loss: 0.058808\n",
      "Epoch: 2769, Training Loss: 0.073700, Validation Loss: 0.058181\n",
      "Epoch: 2770, Training Loss: 0.073584, Validation Loss: 0.057404\n",
      "Epoch: 2771, Training Loss: 0.073624, Validation Loss: 0.058401\n",
      "Epoch: 2772, Training Loss: 0.073737, Validation Loss: 0.056369\n",
      "Epoch: 2773, Training Loss: 0.073781, Validation Loss: 0.058709\n",
      "Epoch: 2774, Training Loss: 0.073751, Validation Loss: 0.056342\n",
      "Epoch: 2775, Training Loss: 0.073707, Validation Loss: 0.059006\n",
      "Epoch: 2776, Training Loss: 0.073672, Validation Loss: 0.057055\n",
      "Epoch: 2777, Training Loss: 0.073663, Validation Loss: 0.058673\n",
      "Epoch: 2778, Training Loss: 0.073673, Validation Loss: 0.057314\n",
      "Epoch: 2779, Training Loss: 0.073721, Validation Loss: 0.058136\n",
      "Epoch: 2780, Training Loss: 0.073722, Validation Loss: 0.057206\n",
      "Epoch: 2781, Training Loss: 0.073677, Validation Loss: 0.058401\n",
      "Epoch: 2782, Training Loss: 0.073627, Validation Loss: 0.057102\n",
      "Epoch: 2783, Training Loss: 0.073653, Validation Loss: 0.059190\n",
      "Epoch: 2784, Training Loss: 0.073798, Validation Loss: 0.057101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2785, Training Loss: 0.074008, Validation Loss: 0.060048\n",
      "Epoch: 2786, Training Loss: 0.074416, Validation Loss: 0.056629\n",
      "Epoch: 2787, Training Loss: 0.074987, Validation Loss: 0.060949\n",
      "Epoch: 2788, Training Loss: 0.075570, Validation Loss: 0.056754\n",
      "Epoch: 2789, Training Loss: 0.076503, Validation Loss: 0.063148\n",
      "Epoch: 2790, Training Loss: 0.077382, Validation Loss: 0.058304\n",
      "Epoch: 2791, Training Loss: 0.078950, Validation Loss: 0.066652\n",
      "Epoch: 2792, Training Loss: 0.080532, Validation Loss: 0.060884\n",
      "Epoch: 2793, Training Loss: 0.084217, Validation Loss: 0.073291\n",
      "Epoch: 2794, Training Loss: 0.086617, Validation Loss: 0.064845\n",
      "Epoch: 2795, Training Loss: 0.090445, Validation Loss: 0.080958\n",
      "Epoch: 2796, Training Loss: 0.089963, Validation Loss: 0.067072\n",
      "Epoch: 2797, Training Loss: 0.089938, Validation Loss: 0.079061\n",
      "Epoch: 2798, Training Loss: 0.084506, Validation Loss: 0.062781\n",
      "Epoch: 2799, Training Loss: 0.080674, Validation Loss: 0.065790\n",
      "Epoch: 2800, Training Loss: 0.077361, Validation Loss: 0.059721\n",
      "Epoch: 2801, Training Loss: 0.076597, Validation Loss: 0.058114\n",
      "Epoch: 2802, Training Loss: 0.076719, Validation Loss: 0.063178\n",
      "Epoch: 2803, Training Loss: 0.076796, Validation Loss: 0.057413\n",
      "Epoch: 2804, Training Loss: 0.076071, Validation Loss: 0.063628\n",
      "Epoch: 2805, Training Loss: 0.075074, Validation Loss: 0.057466\n",
      "Epoch: 2806, Training Loss: 0.075005, Validation Loss: 0.060240\n",
      "Epoch: 2807, Training Loss: 0.075803, Validation Loss: 0.060492\n",
      "Epoch: 2808, Training Loss: 0.076956, Validation Loss: 0.059308\n",
      "Epoch: 2809, Training Loss: 0.076967, Validation Loss: 0.062349\n",
      "Epoch: 2810, Training Loss: 0.076141, Validation Loss: 0.058293\n",
      "Epoch: 2811, Training Loss: 0.074648, Validation Loss: 0.059068\n",
      "Epoch: 2812, Training Loss: 0.073713, Validation Loss: 0.057895\n",
      "Epoch: 2813, Training Loss: 0.073755, Validation Loss: 0.056483\n",
      "Epoch: 2814, Training Loss: 0.074301, Validation Loss: 0.060342\n",
      "Epoch: 2815, Training Loss: 0.074535, Validation Loss: 0.056876\n",
      "Epoch: 2816, Training Loss: 0.074173, Validation Loss: 0.059730\n",
      "Epoch: 2817, Training Loss: 0.073687, Validation Loss: 0.057086\n",
      "Epoch: 2818, Training Loss: 0.073449, Validation Loss: 0.057213\n",
      "Epoch: 2819, Training Loss: 0.073619, Validation Loss: 0.058642\n",
      "Epoch: 2820, Training Loss: 0.073972, Validation Loss: 0.056433\n",
      "Epoch: 2821, Training Loss: 0.074197, Validation Loss: 0.060055\n",
      "Epoch: 2822, Training Loss: 0.074074, Validation Loss: 0.056442\n",
      "Epoch: 2823, Training Loss: 0.073809, Validation Loss: 0.059124\n",
      "Epoch: 2824, Training Loss: 0.073598, Validation Loss: 0.056743\n",
      "Epoch: 2825, Training Loss: 0.073538, Validation Loss: 0.057830\n",
      "Epoch: 2826, Training Loss: 0.073545, Validation Loss: 0.057827\n",
      "Epoch: 2827, Training Loss: 0.073491, Validation Loss: 0.057437\n",
      "Epoch: 2828, Training Loss: 0.073374, Validation Loss: 0.058249\n",
      "Epoch: 2829, Training Loss: 0.073268, Validation Loss: 0.057423\n",
      "Epoch: 2830, Training Loss: 0.073196, Validation Loss: 0.058083\n",
      "Epoch: 2831, Training Loss: 0.073249, Validation Loss: 0.057511\n",
      "Epoch: 2832, Training Loss: 0.073380, Validation Loss: 0.057984\n",
      "Epoch: 2833, Training Loss: 0.073482, Validation Loss: 0.057572\n",
      "Epoch: 2834, Training Loss: 0.073529, Validation Loss: 0.058164\n",
      "Epoch: 2835, Training Loss: 0.073559, Validation Loss: 0.057352\n",
      "Epoch: 2836, Training Loss: 0.073658, Validation Loss: 0.059137\n",
      "Epoch: 2837, Training Loss: 0.073868, Validation Loss: 0.057164\n",
      "Epoch: 2838, Training Loss: 0.074274, Validation Loss: 0.060782\n",
      "Epoch: 2839, Training Loss: 0.074913, Validation Loss: 0.057147\n",
      "Epoch: 2840, Training Loss: 0.075994, Validation Loss: 0.063271\n",
      "Epoch: 2841, Training Loss: 0.077533, Validation Loss: 0.058158\n",
      "Epoch: 2842, Training Loss: 0.079870, Validation Loss: 0.068341\n",
      "Epoch: 2843, Training Loss: 0.082086, Validation Loss: 0.060860\n",
      "Epoch: 2844, Training Loss: 0.085090, Validation Loss: 0.074388\n",
      "Epoch: 2845, Training Loss: 0.086006, Validation Loss: 0.063822\n",
      "Epoch: 2846, Training Loss: 0.087728, Validation Loss: 0.076198\n",
      "Epoch: 2847, Training Loss: 0.085521, Validation Loss: 0.064574\n",
      "Epoch: 2848, Training Loss: 0.084645, Validation Loss: 0.070936\n",
      "Epoch: 2849, Training Loss: 0.081082, Validation Loss: 0.063140\n",
      "Epoch: 2850, Training Loss: 0.078618, Validation Loss: 0.063367\n",
      "Epoch: 2851, Training Loss: 0.075883, Validation Loss: 0.060870\n",
      "Epoch: 2852, Training Loss: 0.074352, Validation Loss: 0.058512\n",
      "Epoch: 2853, Training Loss: 0.073977, Validation Loss: 0.060841\n",
      "Epoch: 2854, Training Loss: 0.074744, Validation Loss: 0.057698\n",
      "Epoch: 2855, Training Loss: 0.076313, Validation Loss: 0.063083\n",
      "Epoch: 2856, Training Loss: 0.077398, Validation Loss: 0.058929\n",
      "Epoch: 2857, Training Loss: 0.078225, Validation Loss: 0.063454\n",
      "Epoch: 2858, Training Loss: 0.077664, Validation Loss: 0.059616\n",
      "Epoch: 2859, Training Loss: 0.076923, Validation Loss: 0.060825\n",
      "Epoch: 2860, Training Loss: 0.075470, Validation Loss: 0.059136\n",
      "Epoch: 2861, Training Loss: 0.074207, Validation Loss: 0.058364\n",
      "Epoch: 2862, Training Loss: 0.073358, Validation Loss: 0.058616\n",
      "Epoch: 2863, Training Loss: 0.073222, Validation Loss: 0.057926\n",
      "Epoch: 2864, Training Loss: 0.073667, Validation Loss: 0.059271\n",
      "Epoch: 2865, Training Loss: 0.074345, Validation Loss: 0.058557\n",
      "Epoch: 2866, Training Loss: 0.074974, Validation Loss: 0.060044\n",
      "Epoch: 2867, Training Loss: 0.075083, Validation Loss: 0.058475\n",
      "Epoch: 2868, Training Loss: 0.074934, Validation Loss: 0.060024\n",
      "Epoch: 2869, Training Loss: 0.074474, Validation Loss: 0.057424\n",
      "Epoch: 2870, Training Loss: 0.074154, Validation Loss: 0.059903\n",
      "Epoch: 2871, Training Loss: 0.073923, Validation Loss: 0.056707\n",
      "Epoch: 2872, Training Loss: 0.073900, Validation Loss: 0.059987\n",
      "Epoch: 2873, Training Loss: 0.073964, Validation Loss: 0.056912\n",
      "Epoch: 2874, Training Loss: 0.073989, Validation Loss: 0.060421\n",
      "Epoch: 2875, Training Loss: 0.073924, Validation Loss: 0.057279\n",
      "Epoch: 2876, Training Loss: 0.073757, Validation Loss: 0.060208\n",
      "Epoch: 2877, Training Loss: 0.073661, Validation Loss: 0.056894\n",
      "Epoch: 2878, Training Loss: 0.073675, Validation Loss: 0.059293\n",
      "Epoch: 2879, Training Loss: 0.073827, Validation Loss: 0.056730\n",
      "Epoch: 2880, Training Loss: 0.074192, Validation Loss: 0.060004\n",
      "Epoch: 2881, Training Loss: 0.074553, Validation Loss: 0.057455\n",
      "Epoch: 2882, Training Loss: 0.075036, Validation Loss: 0.061786\n",
      "Epoch: 2883, Training Loss: 0.075818, Validation Loss: 0.058013\n",
      "Epoch: 2884, Training Loss: 0.077740, Validation Loss: 0.065981\n",
      "Epoch: 2885, Training Loss: 0.080425, Validation Loss: 0.060307\n",
      "Epoch: 2886, Training Loss: 0.084146, Validation Loss: 0.074779\n",
      "Epoch: 2887, Training Loss: 0.086782, Validation Loss: 0.063451\n",
      "Epoch: 2888, Training Loss: 0.087781, Validation Loss: 0.078116\n",
      "Epoch: 2889, Training Loss: 0.084277, Validation Loss: 0.061023\n",
      "Epoch: 2890, Training Loss: 0.081003, Validation Loss: 0.066069\n",
      "Epoch: 2891, Training Loss: 0.078614, Validation Loss: 0.059591\n",
      "Epoch: 2892, Training Loss: 0.078766, Validation Loss: 0.059778\n",
      "Epoch: 2893, Training Loss: 0.077534, Validation Loss: 0.063454\n",
      "Epoch: 2894, Training Loss: 0.075188, Validation Loss: 0.057874\n",
      "Epoch: 2895, Training Loss: 0.073789, Validation Loss: 0.060226\n",
      "Epoch: 2896, Training Loss: 0.074823, Validation Loss: 0.060318\n",
      "Epoch: 2897, Training Loss: 0.077156, Validation Loss: 0.061641\n",
      "Epoch: 2898, Training Loss: 0.078044, Validation Loss: 0.063615\n",
      "Epoch: 2899, Training Loss: 0.077464, Validation Loss: 0.061787\n",
      "Epoch: 2900, Training Loss: 0.075766, Validation Loss: 0.059223\n",
      "Epoch: 2901, Training Loss: 0.075072, Validation Loss: 0.061019\n",
      "Epoch: 2902, Training Loss: 0.075043, Validation Loss: 0.056500\n",
      "Epoch: 2903, Training Loss: 0.074969, Validation Loss: 0.061257\n",
      "Epoch: 2904, Training Loss: 0.074495, Validation Loss: 0.056339\n",
      "Epoch: 2905, Training Loss: 0.073919, Validation Loss: 0.058412\n",
      "Epoch: 2906, Training Loss: 0.073797, Validation Loss: 0.057793\n",
      "Epoch: 2907, Training Loss: 0.073957, Validation Loss: 0.057147\n",
      "Epoch: 2908, Training Loss: 0.074594, Validation Loss: 0.061003\n",
      "Epoch: 2909, Training Loss: 0.075268, Validation Loss: 0.056865\n",
      "Epoch: 2910, Training Loss: 0.075428, Validation Loss: 0.062187\n",
      "Epoch: 2911, Training Loss: 0.074767, Validation Loss: 0.056022\n",
      "Epoch: 2912, Training Loss: 0.073973, Validation Loss: 0.058713\n",
      "Epoch: 2913, Training Loss: 0.073473, Validation Loss: 0.056719\n",
      "Epoch: 2914, Training Loss: 0.073518, Validation Loss: 0.057055\n",
      "Epoch: 2915, Training Loss: 0.073663, Validation Loss: 0.059671\n",
      "Epoch: 2916, Training Loss: 0.073659, Validation Loss: 0.057572\n",
      "Epoch: 2917, Training Loss: 0.073620, Validation Loss: 0.060314\n",
      "Epoch: 2918, Training Loss: 0.073798, Validation Loss: 0.058058\n",
      "Epoch: 2919, Training Loss: 0.074218, Validation Loss: 0.060019\n",
      "Epoch: 2920, Training Loss: 0.074466, Validation Loss: 0.058532\n",
      "Epoch: 2921, Training Loss: 0.074575, Validation Loss: 0.060287\n",
      "Epoch: 2922, Training Loss: 0.074464, Validation Loss: 0.057929\n",
      "Epoch: 2923, Training Loss: 0.074516, Validation Loss: 0.060813\n",
      "Epoch: 2924, Training Loss: 0.074618, Validation Loss: 0.056628\n",
      "Epoch: 2925, Training Loss: 0.074930, Validation Loss: 0.061155\n",
      "Epoch: 2926, Training Loss: 0.074944, Validation Loss: 0.056228\n",
      "Epoch: 2927, Training Loss: 0.074777, Validation Loss: 0.060869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2928, Training Loss: 0.074360, Validation Loss: 0.056645\n",
      "Epoch: 2929, Training Loss: 0.074120, Validation Loss: 0.060309\n",
      "Epoch: 2930, Training Loss: 0.074013, Validation Loss: 0.058042\n",
      "Epoch: 2931, Training Loss: 0.074008, Validation Loss: 0.060294\n",
      "Epoch: 2932, Training Loss: 0.073984, Validation Loss: 0.057851\n",
      "Epoch: 2933, Training Loss: 0.074054, Validation Loss: 0.059937\n",
      "Epoch: 2934, Training Loss: 0.074140, Validation Loss: 0.056871\n",
      "Epoch: 2935, Training Loss: 0.074347, Validation Loss: 0.060548\n",
      "Epoch: 2936, Training Loss: 0.074629, Validation Loss: 0.056820\n",
      "Epoch: 2937, Training Loss: 0.075103, Validation Loss: 0.062333\n",
      "Epoch: 2938, Training Loss: 0.075628, Validation Loss: 0.057228\n",
      "Epoch: 2939, Training Loss: 0.076241, Validation Loss: 0.063932\n",
      "Epoch: 2940, Training Loss: 0.076696, Validation Loss: 0.058035\n",
      "Epoch: 2941, Training Loss: 0.077491, Validation Loss: 0.065178\n",
      "Epoch: 2942, Training Loss: 0.077967, Validation Loss: 0.058970\n",
      "Epoch: 2943, Training Loss: 0.078735, Validation Loss: 0.066047\n",
      "Epoch: 2944, Training Loss: 0.078456, Validation Loss: 0.059747\n",
      "Epoch: 2945, Training Loss: 0.078465, Validation Loss: 0.065781\n",
      "Epoch: 2946, Training Loss: 0.077545, Validation Loss: 0.059517\n",
      "Epoch: 2947, Training Loss: 0.077237, Validation Loss: 0.064916\n",
      "Epoch: 2948, Training Loss: 0.076289, Validation Loss: 0.058363\n",
      "Epoch: 2949, Training Loss: 0.075551, Validation Loss: 0.062732\n",
      "Epoch: 2950, Training Loss: 0.074711, Validation Loss: 0.056936\n",
      "Epoch: 2951, Training Loss: 0.074022, Validation Loss: 0.060121\n",
      "Epoch: 2952, Training Loss: 0.073543, Validation Loss: 0.056605\n",
      "Epoch: 2953, Training Loss: 0.073229, Validation Loss: 0.058914\n",
      "Epoch: 2954, Training Loss: 0.073064, Validation Loss: 0.057529\n",
      "Epoch: 2955, Training Loss: 0.072983, Validation Loss: 0.057774\n",
      "Epoch: 2956, Training Loss: 0.073074, Validation Loss: 0.058492\n",
      "Epoch: 2957, Training Loss: 0.073345, Validation Loss: 0.056848\n",
      "Epoch: 2958, Training Loss: 0.073715, Validation Loss: 0.059869\n",
      "Epoch: 2959, Training Loss: 0.074182, Validation Loss: 0.056824\n",
      "Epoch: 2960, Training Loss: 0.074597, Validation Loss: 0.061766\n",
      "Epoch: 2961, Training Loss: 0.074851, Validation Loss: 0.057020\n",
      "Epoch: 2962, Training Loss: 0.075174, Validation Loss: 0.062362\n",
      "Epoch: 2963, Training Loss: 0.075289, Validation Loss: 0.057058\n",
      "Epoch: 2964, Training Loss: 0.075484, Validation Loss: 0.062526\n",
      "Epoch: 2965, Training Loss: 0.075392, Validation Loss: 0.057285\n",
      "Epoch: 2966, Training Loss: 0.075266, Validation Loss: 0.061991\n",
      "Epoch: 2967, Training Loss: 0.074931, Validation Loss: 0.057695\n",
      "Epoch: 2968, Training Loss: 0.074717, Validation Loss: 0.061286\n",
      "Epoch: 2969, Training Loss: 0.074480, Validation Loss: 0.058358\n",
      "Epoch: 2970, Training Loss: 0.074340, Validation Loss: 0.061028\n",
      "Epoch: 2971, Training Loss: 0.074139, Validation Loss: 0.057895\n",
      "Epoch: 2972, Training Loss: 0.074067, Validation Loss: 0.060516\n",
      "Epoch: 2973, Training Loss: 0.074002, Validation Loss: 0.056853\n",
      "Epoch: 2974, Training Loss: 0.074075, Validation Loss: 0.060444\n",
      "Epoch: 2975, Training Loss: 0.074192, Validation Loss: 0.056631\n",
      "Epoch: 2976, Training Loss: 0.074311, Validation Loss: 0.061181\n",
      "Epoch: 2977, Training Loss: 0.074365, Validation Loss: 0.056943\n",
      "Epoch: 2978, Training Loss: 0.074382, Validation Loss: 0.061400\n",
      "Epoch: 2979, Training Loss: 0.074325, Validation Loss: 0.056991\n",
      "Epoch: 2980, Training Loss: 0.074423, Validation Loss: 0.061109\n",
      "Epoch: 2981, Training Loss: 0.074529, Validation Loss: 0.057259\n",
      "Epoch: 2982, Training Loss: 0.074823, Validation Loss: 0.061777\n",
      "Epoch: 2983, Training Loss: 0.075066, Validation Loss: 0.058104\n",
      "Epoch: 2984, Training Loss: 0.075630, Validation Loss: 0.063095\n",
      "Epoch: 2985, Training Loss: 0.076026, Validation Loss: 0.058343\n",
      "Epoch: 2986, Training Loss: 0.076809, Validation Loss: 0.064401\n",
      "Epoch: 2987, Training Loss: 0.077433, Validation Loss: 0.058523\n",
      "Epoch: 2988, Training Loss: 0.078471, Validation Loss: 0.066522\n",
      "Epoch: 2989, Training Loss: 0.078966, Validation Loss: 0.058932\n",
      "Epoch: 2990, Training Loss: 0.079322, Validation Loss: 0.067749\n",
      "Epoch: 2991, Training Loss: 0.078437, Validation Loss: 0.058608\n",
      "Epoch: 2992, Training Loss: 0.077323, Validation Loss: 0.064741\n",
      "Epoch: 2993, Training Loss: 0.075739, Validation Loss: 0.057976\n",
      "Epoch: 2994, Training Loss: 0.074908, Validation Loss: 0.061092\n",
      "Epoch: 2995, Training Loss: 0.074304, Validation Loss: 0.059065\n",
      "Epoch: 2996, Training Loss: 0.074087, Validation Loss: 0.059394\n",
      "Epoch: 2997, Training Loss: 0.074000, Validation Loss: 0.060360\n",
      "Epoch: 2998, Training Loss: 0.074122, Validation Loss: 0.058087\n",
      "Epoch: 2999, Training Loss: 0.074335, Validation Loss: 0.061254\n",
      "Epoch: 3000, Training Loss: 0.074585, Validation Loss: 0.056992\n",
      "Epoch: 3001, Training Loss: 0.074689, Validation Loss: 0.061685\n",
      "Epoch: 3002, Training Loss: 0.074594, Validation Loss: 0.056459\n",
      "Epoch: 3003, Training Loss: 0.074404, Validation Loss: 0.060694\n",
      "Epoch: 3004, Training Loss: 0.074185, Validation Loss: 0.057119\n",
      "Epoch: 3005, Training Loss: 0.074325, Validation Loss: 0.059739\n",
      "Epoch: 3006, Training Loss: 0.074518, Validation Loss: 0.059114\n",
      "Epoch: 3007, Training Loss: 0.074741, Validation Loss: 0.060009\n",
      "Epoch: 3008, Training Loss: 0.074586, Validation Loss: 0.060115\n",
      "Epoch: 3009, Training Loss: 0.074291, Validation Loss: 0.059597\n",
      "Epoch: 3010, Training Loss: 0.073777, Validation Loss: 0.059165\n",
      "Epoch: 3011, Training Loss: 0.073315, Validation Loss: 0.058313\n",
      "Epoch: 3012, Training Loss: 0.072982, Validation Loss: 0.057266\n",
      "Epoch: 3013, Training Loss: 0.072866, Validation Loss: 0.057742\n",
      "Epoch: 3014, Training Loss: 0.072920, Validation Loss: 0.056666\n",
      "Epoch: 3015, Training Loss: 0.073038, Validation Loss: 0.058552\n",
      "Epoch: 3016, Training Loss: 0.073134, Validation Loss: 0.057328\n",
      "Epoch: 3017, Training Loss: 0.073144, Validation Loss: 0.059321\n",
      "Epoch: 3018, Training Loss: 0.073057, Validation Loss: 0.057745\n",
      "Epoch: 3019, Training Loss: 0.072925, Validation Loss: 0.059047\n",
      "Epoch: 3020, Training Loss: 0.072873, Validation Loss: 0.057454\n",
      "Epoch: 3021, Training Loss: 0.072906, Validation Loss: 0.058620\n",
      "Epoch: 3022, Training Loss: 0.073088, Validation Loss: 0.056969\n",
      "Epoch: 3023, Training Loss: 0.073434, Validation Loss: 0.059443\n",
      "Epoch: 3024, Training Loss: 0.073984, Validation Loss: 0.057306\n",
      "Epoch: 3025, Training Loss: 0.075145, Validation Loss: 0.062471\n",
      "Epoch: 3026, Training Loss: 0.077014, Validation Loss: 0.058885\n",
      "Epoch: 3027, Training Loss: 0.080856, Validation Loss: 0.070649\n",
      "Epoch: 3028, Training Loss: 0.086540, Validation Loss: 0.065483\n",
      "Epoch: 3029, Training Loss: 0.096056, Validation Loss: 0.089344\n",
      "Epoch: 3030, Training Loss: 0.102398, Validation Loss: 0.076899\n",
      "Epoch: 3031, Training Loss: 0.105848, Validation Loss: 0.097938\n",
      "Epoch: 3032, Training Loss: 0.096253, Validation Loss: 0.072581\n",
      "Epoch: 3033, Training Loss: 0.099045, Validation Loss: 0.079889\n",
      "Epoch: 3034, Training Loss: 0.120678, Validation Loss: 0.103213\n",
      "Epoch: 3035, Training Loss: 0.204186, Validation Loss: 0.185390\n",
      "Epoch: 3036, Training Loss: 0.223762, Validation Loss: 0.203951\n",
      "Epoch: 3037, Training Loss: 0.260669, Validation Loss: 0.257356\n",
      "Epoch: 3038, Training Loss: 0.125356, Validation Loss: 0.100532\n",
      "Epoch: 3039, Training Loss: 0.102572, Validation Loss: 0.096360\n",
      "Epoch: 3040, Training Loss: 0.135368, Validation Loss: 0.123062\n",
      "Epoch: 3041, Training Loss: 0.152511, Validation Loss: 0.123306\n",
      "Epoch: 3042, Training Loss: 0.151765, Validation Loss: 0.151769\n",
      "Epoch: 3043, Training Loss: 0.075940, Validation Loss: 0.053126\n",
      "Epoch: 3044, Training Loss: 0.116622, Validation Loss: 0.084297\n",
      "Epoch: 3045, Training Loss: 0.143969, Validation Loss: 0.137873\n",
      "Epoch: 3046, Training Loss: 0.099678, Validation Loss: 0.079783\n",
      "Epoch: 3047, Training Loss: 0.098498, Validation Loss: 0.065685\n",
      "Epoch: 3048, Training Loss: 0.098940, Validation Loss: 0.080481\n",
      "Epoch: 3049, Training Loss: 0.118121, Validation Loss: 0.102619\n",
      "Epoch: 3050, Training Loss: 0.080278, Validation Loss: 0.058727\n",
      "Epoch: 3051, Training Loss: 0.092393, Validation Loss: 0.065835\n",
      "Epoch: 3052, Training Loss: 0.091840, Validation Loss: 0.071567\n",
      "Epoch: 3053, Training Loss: 0.093492, Validation Loss: 0.082301\n",
      "Epoch: 3054, Training Loss: 0.077028, Validation Loss: 0.055129\n",
      "Epoch: 3055, Training Loss: 0.092364, Validation Loss: 0.065796\n",
      "Epoch: 3056, Training Loss: 0.089546, Validation Loss: 0.076561\n",
      "Epoch: 3057, Training Loss: 0.081638, Validation Loss: 0.067438\n",
      "Epoch: 3058, Training Loss: 0.082897, Validation Loss: 0.059239\n",
      "Epoch: 3059, Training Loss: 0.091076, Validation Loss: 0.069577\n",
      "Epoch: 3060, Training Loss: 0.076899, Validation Loss: 0.057956\n",
      "Epoch: 3061, Training Loss: 0.083084, Validation Loss: 0.065433\n",
      "Epoch: 3062, Training Loss: 0.084674, Validation Loss: 0.065561\n",
      "Epoch: 3063, Training Loss: 0.080306, Validation Loss: 0.056509\n",
      "Epoch: 3064, Training Loss: 0.077358, Validation Loss: 0.057820\n",
      "Epoch: 3065, Training Loss: 0.084169, Validation Loss: 0.070898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3066, Training Loss: 0.076284, Validation Loss: 0.056181\n",
      "Epoch: 3067, Training Loss: 0.078817, Validation Loss: 0.056319\n",
      "Epoch: 3068, Training Loss: 0.079722, Validation Loss: 0.063979\n",
      "Epoch: 3069, Training Loss: 0.078039, Validation Loss: 0.061801\n",
      "Epoch: 3070, Training Loss: 0.075476, Validation Loss: 0.055795\n",
      "Epoch: 3071, Training Loss: 0.079742, Validation Loss: 0.059515\n",
      "Epoch: 3072, Training Loss: 0.075565, Validation Loss: 0.055249\n",
      "Epoch: 3073, Training Loss: 0.076599, Validation Loss: 0.058940\n",
      "Epoch: 3074, Training Loss: 0.076764, Validation Loss: 0.058953\n",
      "Epoch: 3075, Training Loss: 0.076461, Validation Loss: 0.054619\n",
      "Epoch: 3076, Training Loss: 0.074646, Validation Loss: 0.055362\n",
      "Epoch: 3077, Training Loss: 0.076918, Validation Loss: 0.061502\n",
      "Epoch: 3078, Training Loss: 0.074984, Validation Loss: 0.056066\n",
      "Epoch: 3079, Training Loss: 0.075216, Validation Loss: 0.054688\n",
      "Epoch: 3080, Training Loss: 0.075390, Validation Loss: 0.057334\n",
      "Epoch: 3081, Training Loss: 0.075446, Validation Loss: 0.057519\n",
      "Epoch: 3082, Training Loss: 0.074275, Validation Loss: 0.055562\n",
      "Epoch: 3083, Training Loss: 0.075439, Validation Loss: 0.056448\n",
      "Epoch: 3084, Training Loss: 0.074532, Validation Loss: 0.055717\n",
      "Epoch: 3085, Training Loss: 0.074608, Validation Loss: 0.057612\n",
      "Epoch: 3086, Training Loss: 0.074563, Validation Loss: 0.056962\n",
      "Epoch: 3087, Training Loss: 0.074790, Validation Loss: 0.055011\n",
      "Epoch: 3088, Training Loss: 0.074054, Validation Loss: 0.055875\n",
      "Epoch: 3089, Training Loss: 0.074605, Validation Loss: 0.057553\n",
      "Epoch: 3090, Training Loss: 0.074346, Validation Loss: 0.055256\n",
      "Epoch: 3091, Training Loss: 0.074179, Validation Loss: 0.055360\n",
      "Epoch: 3092, Training Loss: 0.074137, Validation Loss: 0.056884\n",
      "Epoch: 3093, Training Loss: 0.074308, Validation Loss: 0.056732\n",
      "Epoch: 3094, Training Loss: 0.073993, Validation Loss: 0.056097\n",
      "Epoch: 3095, Training Loss: 0.074041, Validation Loss: 0.056182\n",
      "Epoch: 3096, Training Loss: 0.074118, Validation Loss: 0.056496\n",
      "Epoch: 3097, Training Loss: 0.073929, Validation Loss: 0.056695\n",
      "Epoch: 3098, Training Loss: 0.073951, Validation Loss: 0.055855\n",
      "Epoch: 3099, Training Loss: 0.073930, Validation Loss: 0.055745\n",
      "Epoch: 3100, Training Loss: 0.073943, Validation Loss: 0.057059\n",
      "Epoch: 3101, Training Loss: 0.073765, Validation Loss: 0.056219\n",
      "Epoch: 3102, Training Loss: 0.073915, Validation Loss: 0.055536\n",
      "Epoch: 3103, Training Loss: 0.073805, Validation Loss: 0.056532\n",
      "Epoch: 3104, Training Loss: 0.073772, Validation Loss: 0.056457\n",
      "Epoch: 3105, Training Loss: 0.073757, Validation Loss: 0.055726\n",
      "Epoch: 3106, Training Loss: 0.073771, Validation Loss: 0.056241\n",
      "Epoch: 3107, Training Loss: 0.073699, Validation Loss: 0.056402\n",
      "Epoch: 3108, Training Loss: 0.073667, Validation Loss: 0.056158\n",
      "Epoch: 3109, Training Loss: 0.073715, Validation Loss: 0.056059\n",
      "Epoch: 3110, Training Loss: 0.073624, Validation Loss: 0.055850\n",
      "Epoch: 3111, Training Loss: 0.073637, Validation Loss: 0.056212\n",
      "Epoch: 3112, Training Loss: 0.073618, Validation Loss: 0.056142\n",
      "Epoch: 3113, Training Loss: 0.073610, Validation Loss: 0.055677\n",
      "Epoch: 3114, Training Loss: 0.073565, Validation Loss: 0.056197\n",
      "Epoch: 3115, Training Loss: 0.073559, Validation Loss: 0.056284\n",
      "Epoch: 3116, Training Loss: 0.073562, Validation Loss: 0.055683\n",
      "Epoch: 3117, Training Loss: 0.073510, Validation Loss: 0.056032\n",
      "Epoch: 3118, Training Loss: 0.073516, Validation Loss: 0.056363\n",
      "Epoch: 3119, Training Loss: 0.073492, Validation Loss: 0.055976\n",
      "Epoch: 3120, Training Loss: 0.073479, Validation Loss: 0.056123\n",
      "Epoch: 3121, Training Loss: 0.073453, Validation Loss: 0.056338\n",
      "Epoch: 3122, Training Loss: 0.073450, Validation Loss: 0.056203\n",
      "Epoch: 3123, Training Loss: 0.073437, Validation Loss: 0.056229\n",
      "Epoch: 3124, Training Loss: 0.073407, Validation Loss: 0.056234\n",
      "Epoch: 3125, Training Loss: 0.073403, Validation Loss: 0.056439\n",
      "Epoch: 3126, Training Loss: 0.073385, Validation Loss: 0.056508\n",
      "Epoch: 3127, Training Loss: 0.073369, Validation Loss: 0.056377\n",
      "Epoch: 3128, Training Loss: 0.073353, Validation Loss: 0.056548\n",
      "Epoch: 3129, Training Loss: 0.073340, Validation Loss: 0.056592\n",
      "Epoch: 3130, Training Loss: 0.073328, Validation Loss: 0.056343\n",
      "Epoch: 3131, Training Loss: 0.073308, Validation Loss: 0.056540\n",
      "Epoch: 3132, Training Loss: 0.073298, Validation Loss: 0.056642\n",
      "Epoch: 3133, Training Loss: 0.073283, Validation Loss: 0.056442\n",
      "Epoch: 3134, Training Loss: 0.073267, Validation Loss: 0.056476\n",
      "Epoch: 3135, Training Loss: 0.073256, Validation Loss: 0.056562\n",
      "Epoch: 3136, Training Loss: 0.073239, Validation Loss: 0.056503\n",
      "Epoch: 3137, Training Loss: 0.073227, Validation Loss: 0.056597\n",
      "Epoch: 3138, Training Loss: 0.073213, Validation Loss: 0.056675\n",
      "Epoch: 3139, Training Loss: 0.073201, Validation Loss: 0.056644\n",
      "Epoch: 3140, Training Loss: 0.073188, Validation Loss: 0.056648\n",
      "Epoch: 3141, Training Loss: 0.073174, Validation Loss: 0.056589\n",
      "Epoch: 3142, Training Loss: 0.073161, Validation Loss: 0.056585\n",
      "Epoch: 3143, Training Loss: 0.073148, Validation Loss: 0.056674\n",
      "Epoch: 3144, Training Loss: 0.073136, Validation Loss: 0.056681\n",
      "Epoch: 3145, Training Loss: 0.073123, Validation Loss: 0.056801\n",
      "Epoch: 3146, Training Loss: 0.073111, Validation Loss: 0.056838\n",
      "Epoch: 3147, Training Loss: 0.073100, Validation Loss: 0.056782\n",
      "Epoch: 3148, Training Loss: 0.073085, Validation Loss: 0.056884\n",
      "Epoch: 3149, Training Loss: 0.073073, Validation Loss: 0.056903\n",
      "Epoch: 3150, Training Loss: 0.073063, Validation Loss: 0.056832\n",
      "Epoch: 3151, Training Loss: 0.073051, Validation Loss: 0.056942\n",
      "Epoch: 3152, Training Loss: 0.073039, Validation Loss: 0.056968\n",
      "Epoch: 3153, Training Loss: 0.073028, Validation Loss: 0.056994\n",
      "Epoch: 3154, Training Loss: 0.073017, Validation Loss: 0.057083\n",
      "Epoch: 3155, Training Loss: 0.073005, Validation Loss: 0.057074\n",
      "Epoch: 3156, Training Loss: 0.072992, Validation Loss: 0.057095\n",
      "Epoch: 3157, Training Loss: 0.072981, Validation Loss: 0.057099\n",
      "Epoch: 3158, Training Loss: 0.072970, Validation Loss: 0.057115\n",
      "Epoch: 3159, Training Loss: 0.072960, Validation Loss: 0.057146\n",
      "Epoch: 3160, Training Loss: 0.072948, Validation Loss: 0.057168\n",
      "Epoch: 3161, Training Loss: 0.072938, Validation Loss: 0.057217\n",
      "Epoch: 3162, Training Loss: 0.072928, Validation Loss: 0.057173\n",
      "Epoch: 3163, Training Loss: 0.072917, Validation Loss: 0.057262\n",
      "Epoch: 3164, Training Loss: 0.072908, Validation Loss: 0.057237\n",
      "Epoch: 3165, Training Loss: 0.072897, Validation Loss: 0.057352\n",
      "Epoch: 3166, Training Loss: 0.072887, Validation Loss: 0.057328\n",
      "Epoch: 3167, Training Loss: 0.072877, Validation Loss: 0.057426\n",
      "Epoch: 3168, Training Loss: 0.072867, Validation Loss: 0.057483\n",
      "Epoch: 3169, Training Loss: 0.072859, Validation Loss: 0.057438\n",
      "Epoch: 3170, Training Loss: 0.072849, Validation Loss: 0.057505\n",
      "Epoch: 3171, Training Loss: 0.072840, Validation Loss: 0.057501\n",
      "Epoch: 3172, Training Loss: 0.072831, Validation Loss: 0.057550\n",
      "Epoch: 3173, Training Loss: 0.072822, Validation Loss: 0.057595\n",
      "Epoch: 3174, Training Loss: 0.072813, Validation Loss: 0.057659\n",
      "Epoch: 3175, Training Loss: 0.072804, Validation Loss: 0.057652\n",
      "Epoch: 3176, Training Loss: 0.072795, Validation Loss: 0.057637\n",
      "Epoch: 3177, Training Loss: 0.072786, Validation Loss: 0.057646\n",
      "Epoch: 3178, Training Loss: 0.072777, Validation Loss: 0.057558\n",
      "Epoch: 3179, Training Loss: 0.072765, Validation Loss: 0.057570\n",
      "Epoch: 3180, Training Loss: 0.072752, Validation Loss: 0.057597\n",
      "Epoch: 3181, Training Loss: 0.072735, Validation Loss: 0.057526\n",
      "Epoch: 3182, Training Loss: 0.072719, Validation Loss: 0.057592\n",
      "Epoch: 3183, Training Loss: 0.072708, Validation Loss: 0.057564\n",
      "Epoch: 3184, Training Loss: 0.072697, Validation Loss: 0.057711\n",
      "Epoch: 3185, Training Loss: 0.072688, Validation Loss: 0.057769\n",
      "Epoch: 3186, Training Loss: 0.072680, Validation Loss: 0.057931\n",
      "Epoch: 3187, Training Loss: 0.072669, Validation Loss: 0.057961\n",
      "Epoch: 3188, Training Loss: 0.072661, Validation Loss: 0.057991\n",
      "Epoch: 3189, Training Loss: 0.072652, Validation Loss: 0.057952\n",
      "Epoch: 3190, Training Loss: 0.072640, Validation Loss: 0.057931\n",
      "Epoch: 3191, Training Loss: 0.072629, Validation Loss: 0.057931\n",
      "Epoch: 3192, Training Loss: 0.072617, Validation Loss: 0.057924\n",
      "Epoch: 3193, Training Loss: 0.072606, Validation Loss: 0.058029\n",
      "Epoch: 3194, Training Loss: 0.072594, Validation Loss: 0.058002\n",
      "Epoch: 3195, Training Loss: 0.072583, Validation Loss: 0.058033\n",
      "Epoch: 3196, Training Loss: 0.072573, Validation Loss: 0.058021\n",
      "Epoch: 3197, Training Loss: 0.072568, Validation Loss: 0.057927\n",
      "Epoch: 3198, Training Loss: 0.072559, Validation Loss: 0.058050\n",
      "Epoch: 3199, Training Loss: 0.072549, Validation Loss: 0.057994\n",
      "Epoch: 3200, Training Loss: 0.072541, Validation Loss: 0.058160\n",
      "Epoch: 3201, Training Loss: 0.072529, Validation Loss: 0.058053\n",
      "Epoch: 3202, Training Loss: 0.072520, Validation Loss: 0.058140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3203, Training Loss: 0.072509, Validation Loss: 0.058122\n",
      "Epoch: 3204, Training Loss: 0.072499, Validation Loss: 0.058143\n",
      "Epoch: 3205, Training Loss: 0.072489, Validation Loss: 0.058301\n",
      "Epoch: 3206, Training Loss: 0.072480, Validation Loss: 0.058240\n",
      "Epoch: 3207, Training Loss: 0.072471, Validation Loss: 0.058337\n",
      "Epoch: 3208, Training Loss: 0.072461, Validation Loss: 0.058247\n",
      "Epoch: 3209, Training Loss: 0.072452, Validation Loss: 0.058336\n",
      "Epoch: 3210, Training Loss: 0.072445, Validation Loss: 0.058228\n",
      "Epoch: 3211, Training Loss: 0.072437, Validation Loss: 0.058365\n",
      "Epoch: 3212, Training Loss: 0.072429, Validation Loss: 0.058209\n",
      "Epoch: 3213, Training Loss: 0.072419, Validation Loss: 0.058287\n",
      "Epoch: 3214, Training Loss: 0.072408, Validation Loss: 0.058176\n",
      "Epoch: 3215, Training Loss: 0.072397, Validation Loss: 0.058257\n",
      "Epoch: 3216, Training Loss: 0.072384, Validation Loss: 0.058091\n",
      "Epoch: 3217, Training Loss: 0.072369, Validation Loss: 0.058083\n",
      "Epoch: 3218, Training Loss: 0.072364, Validation Loss: 0.057845\n",
      "Epoch: 3219, Training Loss: 0.072359, Validation Loss: 0.057877\n",
      "Epoch: 3220, Training Loss: 0.072347, Validation Loss: 0.057887\n",
      "Epoch: 3221, Training Loss: 0.072337, Validation Loss: 0.058120\n",
      "Epoch: 3222, Training Loss: 0.072329, Validation Loss: 0.058196\n",
      "Epoch: 3223, Training Loss: 0.072321, Validation Loss: 0.058246\n",
      "Epoch: 3224, Training Loss: 0.072312, Validation Loss: 0.058251\n",
      "Epoch: 3225, Training Loss: 0.072302, Validation Loss: 0.058106\n",
      "Epoch: 3226, Training Loss: 0.072295, Validation Loss: 0.058098\n",
      "Epoch: 3227, Training Loss: 0.072286, Validation Loss: 0.058086\n",
      "Epoch: 3228, Training Loss: 0.072277, Validation Loss: 0.058182\n",
      "Epoch: 3229, Training Loss: 0.072269, Validation Loss: 0.058297\n",
      "Epoch: 3230, Training Loss: 0.072260, Validation Loss: 0.058402\n",
      "Epoch: 3231, Training Loss: 0.072253, Validation Loss: 0.058517\n",
      "Epoch: 3232, Training Loss: 0.072245, Validation Loss: 0.058378\n",
      "Epoch: 3233, Training Loss: 0.072240, Validation Loss: 0.058515\n",
      "Epoch: 3234, Training Loss: 0.072233, Validation Loss: 0.058299\n",
      "Epoch: 3235, Training Loss: 0.072226, Validation Loss: 0.058538\n",
      "Epoch: 3236, Training Loss: 0.072218, Validation Loss: 0.058428\n",
      "Epoch: 3237, Training Loss: 0.072208, Validation Loss: 0.058682\n",
      "Epoch: 3238, Training Loss: 0.072199, Validation Loss: 0.058564\n",
      "Epoch: 3239, Training Loss: 0.072192, Validation Loss: 0.058739\n",
      "Epoch: 3240, Training Loss: 0.072185, Validation Loss: 0.058528\n",
      "Epoch: 3241, Training Loss: 0.072180, Validation Loss: 0.058730\n",
      "Epoch: 3242, Training Loss: 0.072177, Validation Loss: 0.058462\n",
      "Epoch: 3243, Training Loss: 0.072185, Validation Loss: 0.058951\n",
      "Epoch: 3244, Training Loss: 0.072200, Validation Loss: 0.058442\n",
      "Epoch: 3245, Training Loss: 0.072227, Validation Loss: 0.059331\n",
      "Epoch: 3246, Training Loss: 0.072284, Validation Loss: 0.058354\n",
      "Epoch: 3247, Training Loss: 0.072418, Validation Loss: 0.059949\n",
      "Epoch: 3248, Training Loss: 0.072643, Validation Loss: 0.058128\n",
      "Epoch: 3249, Training Loss: 0.073046, Validation Loss: 0.061199\n",
      "Epoch: 3250, Training Loss: 0.073655, Validation Loss: 0.058190\n",
      "Epoch: 3251, Training Loss: 0.074653, Validation Loss: 0.063701\n",
      "Epoch: 3252, Training Loss: 0.075864, Validation Loss: 0.059082\n",
      "Epoch: 3253, Training Loss: 0.077708, Validation Loss: 0.067681\n",
      "Epoch: 3254, Training Loss: 0.079895, Validation Loss: 0.061141\n",
      "Epoch: 3255, Training Loss: 0.083164, Validation Loss: 0.074529\n",
      "Epoch: 3256, Training Loss: 0.083584, Validation Loss: 0.063407\n",
      "Epoch: 3257, Training Loss: 0.083578, Validation Loss: 0.074837\n",
      "Epoch: 3258, Training Loss: 0.079675, Validation Loss: 0.060368\n",
      "Epoch: 3259, Training Loss: 0.075678, Validation Loss: 0.063882\n",
      "Epoch: 3260, Training Loss: 0.072726, Validation Loss: 0.056840\n",
      "Epoch: 3261, Training Loss: 0.072429, Validation Loss: 0.057309\n",
      "Epoch: 3262, Training Loss: 0.074072, Validation Loss: 0.062217\n",
      "Epoch: 3263, Training Loss: 0.075512, Validation Loss: 0.058782\n",
      "Epoch: 3264, Training Loss: 0.075804, Validation Loss: 0.065534\n",
      "Epoch: 3265, Training Loss: 0.074311, Validation Loss: 0.058635\n",
      "Epoch: 3266, Training Loss: 0.072822, Validation Loss: 0.060770\n",
      "Epoch: 3267, Training Loss: 0.072255, Validation Loss: 0.059056\n",
      "Epoch: 3268, Training Loss: 0.072962, Validation Loss: 0.058039\n",
      "Epoch: 3269, Training Loss: 0.073944, Validation Loss: 0.062495\n",
      "Epoch: 3270, Training Loss: 0.074134, Validation Loss: 0.057999\n",
      "Epoch: 3271, Training Loss: 0.073453, Validation Loss: 0.061450\n",
      "Epoch: 3272, Training Loss: 0.072515, Validation Loss: 0.057897\n",
      "Epoch: 3273, Training Loss: 0.072260, Validation Loss: 0.058289\n",
      "Epoch: 3274, Training Loss: 0.072744, Validation Loss: 0.060330\n",
      "Epoch: 3275, Training Loss: 0.073312, Validation Loss: 0.058037\n",
      "Epoch: 3276, Training Loss: 0.073342, Validation Loss: 0.061538\n",
      "Epoch: 3277, Training Loss: 0.072791, Validation Loss: 0.058170\n",
      "Epoch: 3278, Training Loss: 0.072275, Validation Loss: 0.059674\n",
      "Epoch: 3279, Training Loss: 0.072185, Validation Loss: 0.059589\n",
      "Epoch: 3280, Training Loss: 0.072488, Validation Loss: 0.058930\n",
      "Epoch: 3281, Training Loss: 0.072878, Validation Loss: 0.061722\n",
      "Epoch: 3282, Training Loss: 0.073089, Validation Loss: 0.058913\n",
      "Epoch: 3283, Training Loss: 0.072965, Validation Loss: 0.061488\n",
      "Epoch: 3284, Training Loss: 0.072583, Validation Loss: 0.058485\n",
      "Epoch: 3285, Training Loss: 0.072242, Validation Loss: 0.059735\n",
      "Epoch: 3286, Training Loss: 0.072093, Validation Loss: 0.058945\n",
      "Epoch: 3287, Training Loss: 0.072183, Validation Loss: 0.058547\n",
      "Epoch: 3288, Training Loss: 0.072401, Validation Loss: 0.060253\n",
      "Epoch: 3289, Training Loss: 0.072666, Validation Loss: 0.058272\n",
      "Epoch: 3290, Training Loss: 0.072876, Validation Loss: 0.061132\n",
      "Epoch: 3291, Training Loss: 0.073008, Validation Loss: 0.058154\n",
      "Epoch: 3292, Training Loss: 0.073038, Validation Loss: 0.061453\n",
      "Epoch: 3293, Training Loss: 0.073039, Validation Loss: 0.058042\n",
      "Epoch: 3294, Training Loss: 0.072959, Validation Loss: 0.061275\n",
      "Epoch: 3295, Training Loss: 0.072845, Validation Loss: 0.058097\n",
      "Epoch: 3296, Training Loss: 0.072743, Validation Loss: 0.060927\n",
      "Epoch: 3297, Training Loss: 0.072608, Validation Loss: 0.058310\n",
      "Epoch: 3298, Training Loss: 0.072584, Validation Loss: 0.061046\n",
      "Epoch: 3299, Training Loss: 0.072544, Validation Loss: 0.058649\n",
      "Epoch: 3300, Training Loss: 0.072524, Validation Loss: 0.061048\n",
      "Epoch: 3301, Training Loss: 0.072522, Validation Loss: 0.058528\n",
      "Epoch: 3302, Training Loss: 0.072547, Validation Loss: 0.060719\n",
      "Epoch: 3303, Training Loss: 0.072593, Validation Loss: 0.058206\n",
      "Epoch: 3304, Training Loss: 0.072674, Validation Loss: 0.060983\n",
      "Epoch: 3305, Training Loss: 0.072808, Validation Loss: 0.058285\n",
      "Epoch: 3306, Training Loss: 0.073039, Validation Loss: 0.061874\n",
      "Epoch: 3307, Training Loss: 0.073397, Validation Loss: 0.058479\n",
      "Epoch: 3308, Training Loss: 0.074019, Validation Loss: 0.063343\n",
      "Epoch: 3309, Training Loss: 0.075022, Validation Loss: 0.058975\n",
      "Epoch: 3310, Training Loss: 0.076789, Validation Loss: 0.067207\n",
      "Epoch: 3311, Training Loss: 0.078960, Validation Loss: 0.061150\n",
      "Epoch: 3312, Training Loss: 0.082370, Validation Loss: 0.074592\n",
      "Epoch: 3313, Training Loss: 0.083906, Validation Loss: 0.064412\n",
      "Epoch: 3314, Training Loss: 0.085651, Validation Loss: 0.078116\n",
      "Epoch: 3315, Training Loss: 0.082997, Validation Loss: 0.063071\n",
      "Epoch: 3316, Training Loss: 0.079361, Validation Loss: 0.069465\n",
      "Epoch: 3317, Training Loss: 0.074312, Validation Loss: 0.057093\n",
      "Epoch: 3318, Training Loss: 0.072173, Validation Loss: 0.057755\n",
      "Epoch: 3319, Training Loss: 0.073544, Validation Loss: 0.061468\n",
      "Epoch: 3320, Training Loss: 0.076289, Validation Loss: 0.059190\n",
      "Epoch: 3321, Training Loss: 0.077704, Validation Loss: 0.068080\n",
      "Epoch: 3322, Training Loss: 0.075815, Validation Loss: 0.059560\n",
      "Epoch: 3323, Training Loss: 0.073314, Validation Loss: 0.062184\n",
      "Epoch: 3324, Training Loss: 0.072179, Validation Loss: 0.059310\n",
      "Epoch: 3325, Training Loss: 0.073239, Validation Loss: 0.058262\n",
      "Epoch: 3326, Training Loss: 0.074797, Validation Loss: 0.064098\n",
      "Epoch: 3327, Training Loss: 0.074600, Validation Loss: 0.058369\n",
      "Epoch: 3328, Training Loss: 0.073274, Validation Loss: 0.061696\n",
      "Epoch: 3329, Training Loss: 0.072241, Validation Loss: 0.058708\n",
      "Epoch: 3330, Training Loss: 0.072640, Validation Loss: 0.058242\n",
      "Epoch: 3331, Training Loss: 0.073522, Validation Loss: 0.062196\n",
      "Epoch: 3332, Training Loss: 0.073499, Validation Loss: 0.058145\n",
      "Epoch: 3333, Training Loss: 0.072818, Validation Loss: 0.060761\n",
      "Epoch: 3334, Training Loss: 0.072172, Validation Loss: 0.058607\n",
      "Epoch: 3335, Training Loss: 0.072221, Validation Loss: 0.058807\n",
      "Epoch: 3336, Training Loss: 0.072720, Validation Loss: 0.061337\n",
      "Epoch: 3337, Training Loss: 0.072933, Validation Loss: 0.058983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3338, Training Loss: 0.072693, Validation Loss: 0.061420\n",
      "Epoch: 3339, Training Loss: 0.072275, Validation Loss: 0.058898\n",
      "Epoch: 3340, Training Loss: 0.072041, Validation Loss: 0.059455\n",
      "Epoch: 3341, Training Loss: 0.072120, Validation Loss: 0.059651\n",
      "Epoch: 3342, Training Loss: 0.072348, Validation Loss: 0.058184\n",
      "Epoch: 3343, Training Loss: 0.072543, Validation Loss: 0.060673\n",
      "Epoch: 3344, Training Loss: 0.072502, Validation Loss: 0.058295\n",
      "Epoch: 3345, Training Loss: 0.072292, Validation Loss: 0.060336\n",
      "Epoch: 3346, Training Loss: 0.072063, Validation Loss: 0.058840\n",
      "Epoch: 3347, Training Loss: 0.071965, Validation Loss: 0.059178\n",
      "Epoch: 3348, Training Loss: 0.072064, Validation Loss: 0.059795\n",
      "Epoch: 3349, Training Loss: 0.072334, Validation Loss: 0.058461\n",
      "Epoch: 3350, Training Loss: 0.072720, Validation Loss: 0.061271\n",
      "Epoch: 3351, Training Loss: 0.072909, Validation Loss: 0.058408\n",
      "Epoch: 3352, Training Loss: 0.072920, Validation Loss: 0.061551\n",
      "Epoch: 3353, Training Loss: 0.072950, Validation Loss: 0.058201\n",
      "Epoch: 3354, Training Loss: 0.072918, Validation Loss: 0.061259\n",
      "Epoch: 3355, Training Loss: 0.072797, Validation Loss: 0.058187\n",
      "Epoch: 3356, Training Loss: 0.072707, Validation Loss: 0.061200\n",
      "Epoch: 3357, Training Loss: 0.072639, Validation Loss: 0.058545\n",
      "Epoch: 3358, Training Loss: 0.072567, Validation Loss: 0.061259\n",
      "Epoch: 3359, Training Loss: 0.072585, Validation Loss: 0.058625\n",
      "Epoch: 3360, Training Loss: 0.072732, Validation Loss: 0.061489\n",
      "Epoch: 3361, Training Loss: 0.072941, Validation Loss: 0.058587\n",
      "Epoch: 3362, Training Loss: 0.073325, Validation Loss: 0.062710\n",
      "Epoch: 3363, Training Loss: 0.073892, Validation Loss: 0.058957\n",
      "Epoch: 3364, Training Loss: 0.074463, Validation Loss: 0.064362\n",
      "Epoch: 3365, Training Loss: 0.075203, Validation Loss: 0.059177\n",
      "Epoch: 3366, Training Loss: 0.076160, Validation Loss: 0.066255\n",
      "Epoch: 3367, Training Loss: 0.076746, Validation Loss: 0.059835\n",
      "Epoch: 3368, Training Loss: 0.077449, Validation Loss: 0.068292\n",
      "Epoch: 3369, Training Loss: 0.076929, Validation Loss: 0.060317\n",
      "Epoch: 3370, Training Loss: 0.076438, Validation Loss: 0.067268\n",
      "Epoch: 3371, Training Loss: 0.074798, Validation Loss: 0.059321\n",
      "Epoch: 3372, Training Loss: 0.073289, Validation Loss: 0.062303\n",
      "Epoch: 3373, Training Loss: 0.072275, Validation Loss: 0.058216\n",
      "Epoch: 3374, Training Loss: 0.071915, Validation Loss: 0.058880\n",
      "Epoch: 3375, Training Loss: 0.072158, Validation Loss: 0.060081\n",
      "Epoch: 3376, Training Loss: 0.072670, Validation Loss: 0.058613\n",
      "Epoch: 3377, Training Loss: 0.073218, Validation Loss: 0.062731\n",
      "Epoch: 3378, Training Loss: 0.073436, Validation Loss: 0.058885\n",
      "Epoch: 3379, Training Loss: 0.073475, Validation Loss: 0.063081\n",
      "Epoch: 3380, Training Loss: 0.073048, Validation Loss: 0.058507\n",
      "Epoch: 3381, Training Loss: 0.072595, Validation Loss: 0.061119\n",
      "Epoch: 3382, Training Loss: 0.072123, Validation Loss: 0.058372\n",
      "Epoch: 3383, Training Loss: 0.071905, Validation Loss: 0.058922\n",
      "Epoch: 3384, Training Loss: 0.071955, Validation Loss: 0.059318\n",
      "Epoch: 3385, Training Loss: 0.072159, Validation Loss: 0.058465\n",
      "Epoch: 3386, Training Loss: 0.072520, Validation Loss: 0.061304\n",
      "Epoch: 3387, Training Loss: 0.072920, Validation Loss: 0.058997\n",
      "Epoch: 3388, Training Loss: 0.073111, Validation Loss: 0.062661\n",
      "Epoch: 3389, Training Loss: 0.073275, Validation Loss: 0.058670\n",
      "Epoch: 3390, Training Loss: 0.073433, Validation Loss: 0.062697\n",
      "Epoch: 3391, Training Loss: 0.073454, Validation Loss: 0.058459\n",
      "Epoch: 3392, Training Loss: 0.073436, Validation Loss: 0.062702\n",
      "Epoch: 3393, Training Loss: 0.073142, Validation Loss: 0.058475\n",
      "Epoch: 3394, Training Loss: 0.072754, Validation Loss: 0.061410\n",
      "Epoch: 3395, Training Loss: 0.072329, Validation Loss: 0.058316\n",
      "Epoch: 3396, Training Loss: 0.072056, Validation Loss: 0.060006\n",
      "Epoch: 3397, Training Loss: 0.071908, Validation Loss: 0.058830\n",
      "Epoch: 3398, Training Loss: 0.071840, Validation Loss: 0.059346\n",
      "Epoch: 3399, Training Loss: 0.071878, Validation Loss: 0.059919\n",
      "Epoch: 3400, Training Loss: 0.072072, Validation Loss: 0.059071\n",
      "Epoch: 3401, Training Loss: 0.072458, Validation Loss: 0.061827\n",
      "Epoch: 3402, Training Loss: 0.073046, Validation Loss: 0.059465\n",
      "Epoch: 3403, Training Loss: 0.073996, Validation Loss: 0.064373\n",
      "Epoch: 3404, Training Loss: 0.074968, Validation Loss: 0.060004\n",
      "Epoch: 3405, Training Loss: 0.076589, Validation Loss: 0.067633\n",
      "Epoch: 3406, Training Loss: 0.078061, Validation Loss: 0.060918\n",
      "Epoch: 3407, Training Loss: 0.080150, Validation Loss: 0.071476\n",
      "Epoch: 3408, Training Loss: 0.080658, Validation Loss: 0.061829\n",
      "Epoch: 3409, Training Loss: 0.080831, Validation Loss: 0.071937\n",
      "Epoch: 3410, Training Loss: 0.077919, Validation Loss: 0.059879\n",
      "Epoch: 3411, Training Loss: 0.074941, Validation Loss: 0.063755\n",
      "Epoch: 3412, Training Loss: 0.072579, Validation Loss: 0.057611\n",
      "Epoch: 3413, Training Loss: 0.071921, Validation Loss: 0.058442\n",
      "Epoch: 3414, Training Loss: 0.072799, Validation Loss: 0.061436\n",
      "Epoch: 3415, Training Loss: 0.073952, Validation Loss: 0.059592\n",
      "Epoch: 3416, Training Loss: 0.074911, Validation Loss: 0.065885\n",
      "Epoch: 3417, Training Loss: 0.074915, Validation Loss: 0.060618\n",
      "Epoch: 3418, Training Loss: 0.074304, Validation Loss: 0.065139\n",
      "Epoch: 3419, Training Loss: 0.073037, Validation Loss: 0.059130\n",
      "Epoch: 3420, Training Loss: 0.072114, Validation Loss: 0.060439\n",
      "Epoch: 3421, Training Loss: 0.071930, Validation Loss: 0.059415\n",
      "Epoch: 3422, Training Loss: 0.072328, Validation Loss: 0.058354\n",
      "Epoch: 3423, Training Loss: 0.072839, Validation Loss: 0.061793\n",
      "Epoch: 3424, Training Loss: 0.072894, Validation Loss: 0.058447\n",
      "Epoch: 3425, Training Loss: 0.072630, Validation Loss: 0.061091\n",
      "Epoch: 3426, Training Loss: 0.072104, Validation Loss: 0.058519\n",
      "Epoch: 3427, Training Loss: 0.071868, Validation Loss: 0.058877\n",
      "Epoch: 3428, Training Loss: 0.071984, Validation Loss: 0.059819\n",
      "Epoch: 3429, Training Loss: 0.072333, Validation Loss: 0.058809\n",
      "Epoch: 3430, Training Loss: 0.072748, Validation Loss: 0.062073\n",
      "Epoch: 3431, Training Loss: 0.072857, Validation Loss: 0.059333\n",
      "Epoch: 3432, Training Loss: 0.072688, Validation Loss: 0.062059\n",
      "Epoch: 3433, Training Loss: 0.072405, Validation Loss: 0.058696\n",
      "Epoch: 3434, Training Loss: 0.072171, Validation Loss: 0.060446\n",
      "Epoch: 3435, Training Loss: 0.071955, Validation Loss: 0.058437\n",
      "Epoch: 3436, Training Loss: 0.071848, Validation Loss: 0.059317\n",
      "Epoch: 3437, Training Loss: 0.071810, Validation Loss: 0.059182\n",
      "Epoch: 3438, Training Loss: 0.071827, Validation Loss: 0.058886\n",
      "Epoch: 3439, Training Loss: 0.071879, Validation Loss: 0.059840\n",
      "Epoch: 3440, Training Loss: 0.071955, Validation Loss: 0.058968\n",
      "Epoch: 3441, Training Loss: 0.072052, Validation Loss: 0.060775\n",
      "Epoch: 3442, Training Loss: 0.072176, Validation Loss: 0.059289\n",
      "Epoch: 3443, Training Loss: 0.072466, Validation Loss: 0.061966\n",
      "Epoch: 3444, Training Loss: 0.072967, Validation Loss: 0.059117\n",
      "Epoch: 3445, Training Loss: 0.073748, Validation Loss: 0.063648\n",
      "Epoch: 3446, Training Loss: 0.074584, Validation Loss: 0.059332\n",
      "Epoch: 3447, Training Loss: 0.075929, Validation Loss: 0.066464\n",
      "Epoch: 3448, Training Loss: 0.077340, Validation Loss: 0.060701\n",
      "Epoch: 3449, Training Loss: 0.079228, Validation Loss: 0.070609\n",
      "Epoch: 3450, Training Loss: 0.079891, Validation Loss: 0.061912\n",
      "Epoch: 3451, Training Loss: 0.080310, Validation Loss: 0.071495\n",
      "Epoch: 3452, Training Loss: 0.078150, Validation Loss: 0.060376\n",
      "Epoch: 3453, Training Loss: 0.075818, Validation Loss: 0.065359\n",
      "Epoch: 3454, Training Loss: 0.073198, Validation Loss: 0.058045\n",
      "Epoch: 3455, Training Loss: 0.071888, Validation Loss: 0.059218\n",
      "Epoch: 3456, Training Loss: 0.072278, Validation Loss: 0.060776\n",
      "Epoch: 3457, Training Loss: 0.073806, Validation Loss: 0.059451\n",
      "Epoch: 3458, Training Loss: 0.075420, Validation Loss: 0.066373\n",
      "Epoch: 3459, Training Loss: 0.075696, Validation Loss: 0.060750\n",
      "Epoch: 3460, Training Loss: 0.075192, Validation Loss: 0.066234\n",
      "Epoch: 3461, Training Loss: 0.073670, Validation Loss: 0.059313\n",
      "Epoch: 3462, Training Loss: 0.072359, Validation Loss: 0.061422\n",
      "Epoch: 3463, Training Loss: 0.071837, Validation Loss: 0.059224\n",
      "Epoch: 3464, Training Loss: 0.072144, Validation Loss: 0.058630\n",
      "Epoch: 3465, Training Loss: 0.072810, Validation Loss: 0.062102\n",
      "Epoch: 3466, Training Loss: 0.073221, Validation Loss: 0.058780\n",
      "Epoch: 3467, Training Loss: 0.073056, Validation Loss: 0.062565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3468, Training Loss: 0.072408, Validation Loss: 0.058648\n",
      "Epoch: 3469, Training Loss: 0.071914, Validation Loss: 0.059860\n",
      "Epoch: 3470, Training Loss: 0.071807, Validation Loss: 0.059553\n",
      "Epoch: 3471, Training Loss: 0.072033, Validation Loss: 0.058818\n",
      "Epoch: 3472, Training Loss: 0.072416, Validation Loss: 0.061581\n",
      "Epoch: 3473, Training Loss: 0.072617, Validation Loss: 0.059131\n",
      "Epoch: 3474, Training Loss: 0.072553, Validation Loss: 0.061809\n",
      "Epoch: 3475, Training Loss: 0.072238, Validation Loss: 0.058782\n",
      "Epoch: 3476, Training Loss: 0.071917, Validation Loss: 0.060020\n",
      "Epoch: 3477, Training Loss: 0.071764, Validation Loss: 0.058844\n",
      "Epoch: 3478, Training Loss: 0.071777, Validation Loss: 0.058741\n",
      "Epoch: 3479, Training Loss: 0.071925, Validation Loss: 0.060052\n",
      "Epoch: 3480, Training Loss: 0.072156, Validation Loss: 0.058573\n",
      "Epoch: 3481, Training Loss: 0.072355, Validation Loss: 0.061425\n",
      "Epoch: 3482, Training Loss: 0.072485, Validation Loss: 0.058839\n",
      "Epoch: 3483, Training Loss: 0.072543, Validation Loss: 0.061877\n",
      "Epoch: 3484, Training Loss: 0.072550, Validation Loss: 0.058852\n",
      "Epoch: 3485, Training Loss: 0.072497, Validation Loss: 0.061714\n",
      "Epoch: 3486, Training Loss: 0.072441, Validation Loss: 0.058754\n",
      "Epoch: 3487, Training Loss: 0.072384, Validation Loss: 0.061497\n",
      "Epoch: 3488, Training Loss: 0.072326, Validation Loss: 0.058714\n",
      "Epoch: 3489, Training Loss: 0.072286, Validation Loss: 0.061295\n",
      "Epoch: 3490, Training Loss: 0.072305, Validation Loss: 0.058783\n",
      "Epoch: 3491, Training Loss: 0.072396, Validation Loss: 0.061554\n",
      "Epoch: 3492, Training Loss: 0.072585, Validation Loss: 0.058864\n",
      "Epoch: 3493, Training Loss: 0.072864, Validation Loss: 0.062387\n",
      "Epoch: 3494, Training Loss: 0.073210, Validation Loss: 0.058898\n",
      "Epoch: 3495, Training Loss: 0.073660, Validation Loss: 0.063449\n",
      "Epoch: 3496, Training Loss: 0.074220, Validation Loss: 0.059009\n",
      "Epoch: 3497, Training Loss: 0.075017, Validation Loss: 0.065260\n",
      "Epoch: 3498, Training Loss: 0.075892, Validation Loss: 0.059773\n",
      "Epoch: 3499, Training Loss: 0.077132, Validation Loss: 0.068261\n",
      "Epoch: 3500, Training Loss: 0.077902, Validation Loss: 0.061153\n",
      "Epoch: 3501, Training Loss: 0.078652, Validation Loss: 0.070289\n",
      "Epoch: 3502, Training Loss: 0.077905, Validation Loss: 0.060897\n",
      "Epoch: 3503, Training Loss: 0.076741, Validation Loss: 0.067384\n",
      "Epoch: 3504, Training Loss: 0.074520, Validation Loss: 0.058768\n",
      "Epoch: 3505, Training Loss: 0.072803, Validation Loss: 0.061702\n",
      "Epoch: 3506, Training Loss: 0.071789, Validation Loss: 0.058906\n",
      "Epoch: 3507, Training Loss: 0.071869, Validation Loss: 0.059146\n",
      "Epoch: 3508, Training Loss: 0.072777, Validation Loss: 0.062669\n",
      "Epoch: 3509, Training Loss: 0.073831, Validation Loss: 0.059698\n",
      "Epoch: 3510, Training Loss: 0.074614, Validation Loss: 0.065530\n",
      "Epoch: 3511, Training Loss: 0.074410, Validation Loss: 0.059487\n",
      "Epoch: 3512, Training Loss: 0.073809, Validation Loss: 0.064127\n",
      "Epoch: 3513, Training Loss: 0.072636, Validation Loss: 0.058731\n",
      "Epoch: 3514, Training Loss: 0.071882, Validation Loss: 0.059968\n",
      "Epoch: 3515, Training Loss: 0.071782, Validation Loss: 0.059421\n",
      "Epoch: 3516, Training Loss: 0.072263, Validation Loss: 0.058053\n",
      "Epoch: 3517, Training Loss: 0.072861, Validation Loss: 0.061806\n",
      "Epoch: 3518, Training Loss: 0.073223, Validation Loss: 0.058568\n",
      "Epoch: 3519, Training Loss: 0.073188, Validation Loss: 0.062816\n",
      "Epoch: 3520, Training Loss: 0.072640, Validation Loss: 0.058891\n",
      "Epoch: 3521, Training Loss: 0.072100, Validation Loss: 0.060969\n",
      "Epoch: 3522, Training Loss: 0.071741, Validation Loss: 0.059224\n",
      "Epoch: 3523, Training Loss: 0.071732, Validation Loss: 0.059412\n",
      "Epoch: 3524, Training Loss: 0.071933, Validation Loss: 0.060847\n",
      "Epoch: 3525, Training Loss: 0.072322, Validation Loss: 0.059151\n",
      "Epoch: 3526, Training Loss: 0.072753, Validation Loss: 0.062545\n",
      "Epoch: 3527, Training Loss: 0.072865, Validation Loss: 0.058960\n",
      "Epoch: 3528, Training Loss: 0.072853, Validation Loss: 0.062386\n",
      "Epoch: 3529, Training Loss: 0.072647, Validation Loss: 0.058483\n",
      "Epoch: 3530, Training Loss: 0.072359, Validation Loss: 0.061125\n",
      "Epoch: 3531, Training Loss: 0.072076, Validation Loss: 0.058522\n",
      "Epoch: 3532, Training Loss: 0.071840, Validation Loss: 0.060257\n",
      "Epoch: 3533, Training Loss: 0.071691, Validation Loss: 0.059150\n",
      "Epoch: 3534, Training Loss: 0.071637, Validation Loss: 0.059899\n",
      "Epoch: 3535, Training Loss: 0.071627, Validation Loss: 0.059760\n",
      "Epoch: 3536, Training Loss: 0.071632, Validation Loss: 0.059770\n",
      "Epoch: 3537, Training Loss: 0.071651, Validation Loss: 0.060301\n",
      "Epoch: 3538, Training Loss: 0.071681, Validation Loss: 0.059534\n",
      "Epoch: 3539, Training Loss: 0.071718, Validation Loss: 0.060390\n",
      "Epoch: 3540, Training Loss: 0.071785, Validation Loss: 0.059076\n",
      "Epoch: 3541, Training Loss: 0.071896, Validation Loss: 0.060661\n",
      "Epoch: 3542, Training Loss: 0.072088, Validation Loss: 0.058911\n",
      "Epoch: 3543, Training Loss: 0.072470, Validation Loss: 0.061844\n",
      "Epoch: 3544, Training Loss: 0.073161, Validation Loss: 0.058985\n",
      "Epoch: 3545, Training Loss: 0.074283, Validation Loss: 0.064487\n",
      "Epoch: 3546, Training Loss: 0.075553, Validation Loss: 0.059977\n",
      "Epoch: 3547, Training Loss: 0.077646, Validation Loss: 0.069016\n",
      "Epoch: 3548, Training Loss: 0.079299, Validation Loss: 0.062446\n",
      "Epoch: 3549, Training Loss: 0.082431, Validation Loss: 0.075254\n",
      "Epoch: 3550, Training Loss: 0.083547, Validation Loss: 0.065385\n",
      "Epoch: 3551, Training Loss: 0.084402, Validation Loss: 0.077121\n",
      "Epoch: 3552, Training Loss: 0.080646, Validation Loss: 0.062107\n",
      "Epoch: 3553, Training Loss: 0.076569, Validation Loss: 0.065877\n",
      "Epoch: 3554, Training Loss: 0.072740, Validation Loss: 0.057389\n",
      "Epoch: 3555, Training Loss: 0.072010, Validation Loss: 0.057638\n",
      "Epoch: 3556, Training Loss: 0.073915, Validation Loss: 0.063543\n",
      "Epoch: 3557, Training Loss: 0.076124, Validation Loss: 0.060747\n",
      "Epoch: 3558, Training Loss: 0.077040, Validation Loss: 0.068904\n",
      "Epoch: 3559, Training Loss: 0.075174, Validation Loss: 0.060926\n",
      "Epoch: 3560, Training Loss: 0.072907, Validation Loss: 0.062814\n",
      "Epoch: 3561, Training Loss: 0.071790, Validation Loss: 0.059618\n",
      "Epoch: 3562, Training Loss: 0.072430, Validation Loss: 0.058863\n",
      "Epoch: 3563, Training Loss: 0.073793, Validation Loss: 0.063899\n",
      "Epoch: 3564, Training Loss: 0.074140, Validation Loss: 0.059477\n",
      "Epoch: 3565, Training Loss: 0.073443, Validation Loss: 0.063355\n",
      "Epoch: 3566, Training Loss: 0.072169, Validation Loss: 0.058818\n",
      "Epoch: 3567, Training Loss: 0.071785, Validation Loss: 0.058934\n",
      "Epoch: 3568, Training Loss: 0.072385, Validation Loss: 0.060839\n",
      "Epoch: 3569, Training Loss: 0.073003, Validation Loss: 0.058526\n",
      "Epoch: 3570, Training Loss: 0.073046, Validation Loss: 0.062360\n",
      "Epoch: 3571, Training Loss: 0.072395, Validation Loss: 0.058982\n",
      "Epoch: 3572, Training Loss: 0.071800, Validation Loss: 0.060111\n",
      "Epoch: 3573, Training Loss: 0.071758, Validation Loss: 0.060128\n",
      "Epoch: 3574, Training Loss: 0.072302, Validation Loss: 0.058939\n",
      "Epoch: 3575, Training Loss: 0.072886, Validation Loss: 0.062476\n",
      "Epoch: 3576, Training Loss: 0.073089, Validation Loss: 0.059305\n",
      "Epoch: 3577, Training Loss: 0.072826, Validation Loss: 0.062609\n",
      "Epoch: 3578, Training Loss: 0.072227, Validation Loss: 0.058892\n",
      "Epoch: 3579, Training Loss: 0.071765, Validation Loss: 0.060037\n",
      "Epoch: 3580, Training Loss: 0.071636, Validation Loss: 0.059243\n",
      "Epoch: 3581, Training Loss: 0.071846, Validation Loss: 0.058684\n",
      "Epoch: 3582, Training Loss: 0.072260, Validation Loss: 0.061376\n",
      "Epoch: 3583, Training Loss: 0.072739, Validation Loss: 0.059027\n",
      "Epoch: 3584, Training Loss: 0.073064, Validation Loss: 0.062997\n",
      "Epoch: 3585, Training Loss: 0.073083, Validation Loss: 0.059106\n",
      "Epoch: 3586, Training Loss: 0.072950, Validation Loss: 0.062656\n",
      "Epoch: 3587, Training Loss: 0.072630, Validation Loss: 0.058876\n",
      "Epoch: 3588, Training Loss: 0.072310, Validation Loss: 0.061507\n",
      "Epoch: 3589, Training Loss: 0.071920, Validation Loss: 0.058954\n",
      "Epoch: 3590, Training Loss: 0.071655, Validation Loss: 0.059896\n",
      "Epoch: 3591, Training Loss: 0.071578, Validation Loss: 0.059532\n",
      "Epoch: 3592, Training Loss: 0.071636, Validation Loss: 0.059168\n",
      "Epoch: 3593, Training Loss: 0.071782, Validation Loss: 0.060564\n",
      "Epoch: 3594, Training Loss: 0.072015, Validation Loss: 0.059166\n",
      "Epoch: 3595, Training Loss: 0.072263, Validation Loss: 0.061849\n",
      "Epoch: 3596, Training Loss: 0.072467, Validation Loss: 0.059098\n",
      "Epoch: 3597, Training Loss: 0.072652, Validation Loss: 0.062429\n",
      "Epoch: 3598, Training Loss: 0.072836, Validation Loss: 0.058938\n",
      "Epoch: 3599, Training Loss: 0.073099, Validation Loss: 0.063201\n",
      "Epoch: 3600, Training Loss: 0.073319, Validation Loss: 0.059126\n",
      "Epoch: 3601, Training Loss: 0.073474, Validation Loss: 0.063513\n",
      "Epoch: 3602, Training Loss: 0.073521, Validation Loss: 0.058876\n",
      "Epoch: 3603, Training Loss: 0.073649, Validation Loss: 0.063656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3604, Training Loss: 0.073540, Validation Loss: 0.059085\n",
      "Epoch: 3605, Training Loss: 0.073411, Validation Loss: 0.063443\n",
      "Epoch: 3606, Training Loss: 0.072975, Validation Loss: 0.058818\n",
      "Epoch: 3607, Training Loss: 0.072538, Validation Loss: 0.061749\n",
      "Epoch: 3608, Training Loss: 0.072063, Validation Loss: 0.058717\n",
      "Epoch: 3609, Training Loss: 0.071714, Validation Loss: 0.060228\n",
      "Epoch: 3610, Training Loss: 0.071558, Validation Loss: 0.059319\n",
      "Epoch: 3611, Training Loss: 0.071531, Validation Loss: 0.059463\n",
      "Epoch: 3612, Training Loss: 0.071604, Validation Loss: 0.060145\n",
      "Epoch: 3613, Training Loss: 0.071704, Validation Loss: 0.059291\n",
      "Epoch: 3614, Training Loss: 0.071808, Validation Loss: 0.061030\n",
      "Epoch: 3615, Training Loss: 0.071938, Validation Loss: 0.059305\n",
      "Epoch: 3616, Training Loss: 0.072054, Validation Loss: 0.061679\n",
      "Epoch: 3617, Training Loss: 0.072147, Validation Loss: 0.059029\n",
      "Epoch: 3618, Training Loss: 0.072234, Validation Loss: 0.061794\n",
      "Epoch: 3619, Training Loss: 0.072314, Validation Loss: 0.058840\n",
      "Epoch: 3620, Training Loss: 0.072396, Validation Loss: 0.061959\n",
      "Epoch: 3621, Training Loss: 0.072511, Validation Loss: 0.058887\n",
      "Epoch: 3622, Training Loss: 0.072690, Validation Loss: 0.062394\n",
      "Epoch: 3623, Training Loss: 0.072924, Validation Loss: 0.058961\n",
      "Epoch: 3624, Training Loss: 0.073284, Validation Loss: 0.063368\n",
      "Epoch: 3625, Training Loss: 0.073676, Validation Loss: 0.059142\n",
      "Epoch: 3626, Training Loss: 0.074266, Validation Loss: 0.064741\n",
      "Epoch: 3627, Training Loss: 0.074853, Validation Loss: 0.059518\n",
      "Epoch: 3628, Training Loss: 0.075722, Validation Loss: 0.066547\n",
      "Epoch: 3629, Training Loss: 0.076164, Validation Loss: 0.060376\n",
      "Epoch: 3630, Training Loss: 0.076748, Validation Loss: 0.068006\n",
      "Epoch: 3631, Training Loss: 0.076615, Validation Loss: 0.060747\n",
      "Epoch: 3632, Training Loss: 0.076744, Validation Loss: 0.068169\n",
      "Epoch: 3633, Training Loss: 0.076011, Validation Loss: 0.060313\n",
      "Epoch: 3634, Training Loss: 0.075119, Validation Loss: 0.065851\n",
      "Epoch: 3635, Training Loss: 0.073475, Validation Loss: 0.058855\n",
      "Epoch: 3636, Training Loss: 0.072262, Validation Loss: 0.061356\n",
      "Epoch: 3637, Training Loss: 0.071611, Validation Loss: 0.059140\n",
      "Epoch: 3638, Training Loss: 0.071575, Validation Loss: 0.059391\n",
      "Epoch: 3639, Training Loss: 0.071980, Validation Loss: 0.061560\n",
      "Epoch: 3640, Training Loss: 0.072663, Validation Loss: 0.059407\n",
      "Epoch: 3641, Training Loss: 0.073328, Validation Loss: 0.063979\n",
      "Epoch: 3642, Training Loss: 0.073631, Validation Loss: 0.059445\n",
      "Epoch: 3643, Training Loss: 0.073655, Validation Loss: 0.064263\n",
      "Epoch: 3644, Training Loss: 0.073137, Validation Loss: 0.058979\n",
      "Epoch: 3645, Training Loss: 0.072502, Validation Loss: 0.062125\n",
      "Epoch: 3646, Training Loss: 0.071870, Validation Loss: 0.058820\n",
      "Epoch: 3647, Training Loss: 0.071534, Validation Loss: 0.059772\n",
      "Epoch: 3648, Training Loss: 0.071576, Validation Loss: 0.060199\n",
      "Epoch: 3649, Training Loss: 0.071936, Validation Loss: 0.059114\n",
      "Epoch: 3650, Training Loss: 0.072433, Validation Loss: 0.062387\n",
      "Epoch: 3651, Training Loss: 0.072750, Validation Loss: 0.059339\n",
      "Epoch: 3652, Training Loss: 0.072943, Validation Loss: 0.063041\n",
      "Epoch: 3653, Training Loss: 0.072889, Validation Loss: 0.059080\n",
      "Epoch: 3654, Training Loss: 0.072717, Validation Loss: 0.062374\n",
      "Epoch: 3655, Training Loss: 0.072456, Validation Loss: 0.058671\n",
      "Epoch: 3656, Training Loss: 0.072161, Validation Loss: 0.061384\n",
      "Epoch: 3657, Training Loss: 0.071872, Validation Loss: 0.058692\n",
      "Epoch: 3658, Training Loss: 0.071645, Validation Loss: 0.060415\n",
      "Epoch: 3659, Training Loss: 0.071497, Validation Loss: 0.059399\n",
      "Epoch: 3660, Training Loss: 0.071448, Validation Loss: 0.059868\n",
      "Epoch: 3661, Training Loss: 0.071494, Validation Loss: 0.060471\n",
      "Epoch: 3662, Training Loss: 0.071589, Validation Loss: 0.059677\n",
      "Epoch: 3663, Training Loss: 0.071724, Validation Loss: 0.061141\n",
      "Epoch: 3664, Training Loss: 0.071967, Validation Loss: 0.059238\n",
      "Epoch: 3665, Training Loss: 0.072412, Validation Loss: 0.062153\n",
      "Epoch: 3666, Training Loss: 0.073063, Validation Loss: 0.059032\n",
      "Epoch: 3667, Training Loss: 0.073960, Validation Loss: 0.064258\n",
      "Epoch: 3668, Training Loss: 0.074869, Validation Loss: 0.059625\n",
      "Epoch: 3669, Training Loss: 0.076091, Validation Loss: 0.067061\n",
      "Epoch: 3670, Training Loss: 0.076880, Validation Loss: 0.060893\n",
      "Epoch: 3671, Training Loss: 0.078309, Validation Loss: 0.070131\n",
      "Epoch: 3672, Training Loss: 0.078584, Validation Loss: 0.062359\n",
      "Epoch: 3673, Training Loss: 0.079104, Validation Loss: 0.071267\n",
      "Epoch: 3674, Training Loss: 0.077725, Validation Loss: 0.061572\n",
      "Epoch: 3675, Training Loss: 0.076120, Validation Loss: 0.066937\n",
      "Epoch: 3676, Training Loss: 0.073602, Validation Loss: 0.058864\n",
      "Epoch: 3677, Training Loss: 0.072007, Validation Loss: 0.060429\n",
      "Epoch: 3678, Training Loss: 0.071579, Validation Loss: 0.059908\n",
      "Epoch: 3679, Training Loss: 0.072260, Validation Loss: 0.059074\n",
      "Epoch: 3680, Training Loss: 0.073459, Validation Loss: 0.064178\n",
      "Epoch: 3681, Training Loss: 0.074347, Validation Loss: 0.060309\n",
      "Epoch: 3682, Training Loss: 0.074830, Validation Loss: 0.066126\n",
      "Epoch: 3683, Training Loss: 0.074210, Validation Loss: 0.059857\n",
      "Epoch: 3684, Training Loss: 0.073230, Validation Loss: 0.063311\n",
      "Epoch: 3685, Training Loss: 0.072087, Validation Loss: 0.058559\n",
      "Epoch: 3686, Training Loss: 0.071541, Validation Loss: 0.059493\n",
      "Epoch: 3687, Training Loss: 0.071707, Validation Loss: 0.060195\n",
      "Epoch: 3688, Training Loss: 0.072394, Validation Loss: 0.058788\n",
      "Epoch: 3689, Training Loss: 0.073050, Validation Loss: 0.062915\n",
      "Epoch: 3690, Training Loss: 0.073152, Validation Loss: 0.059108\n",
      "Epoch: 3691, Training Loss: 0.072859, Validation Loss: 0.062565\n",
      "Epoch: 3692, Training Loss: 0.072228, Validation Loss: 0.059038\n",
      "Epoch: 3693, Training Loss: 0.071712, Validation Loss: 0.060607\n",
      "Epoch: 3694, Training Loss: 0.071457, Validation Loss: 0.059678\n",
      "Epoch: 3695, Training Loss: 0.071512, Validation Loss: 0.059460\n",
      "Epoch: 3696, Training Loss: 0.071816, Validation Loss: 0.061202\n",
      "Epoch: 3697, Training Loss: 0.072151, Validation Loss: 0.059386\n",
      "Epoch: 3698, Training Loss: 0.072477, Validation Loss: 0.062451\n",
      "Epoch: 3699, Training Loss: 0.072642, Validation Loss: 0.059358\n",
      "Epoch: 3700, Training Loss: 0.072842, Validation Loss: 0.063123\n",
      "Epoch: 3701, Training Loss: 0.072961, Validation Loss: 0.059052\n",
      "Epoch: 3702, Training Loss: 0.073122, Validation Loss: 0.063366\n",
      "Epoch: 3703, Training Loss: 0.073154, Validation Loss: 0.058911\n",
      "Epoch: 3704, Training Loss: 0.073161, Validation Loss: 0.063261\n",
      "Epoch: 3705, Training Loss: 0.072970, Validation Loss: 0.059068\n",
      "Epoch: 3706, Training Loss: 0.072767, Validation Loss: 0.062515\n",
      "Epoch: 3707, Training Loss: 0.072499, Validation Loss: 0.059073\n",
      "Epoch: 3708, Training Loss: 0.072245, Validation Loss: 0.061587\n",
      "Epoch: 3709, Training Loss: 0.071944, Validation Loss: 0.058999\n",
      "Epoch: 3710, Training Loss: 0.071741, Validation Loss: 0.060916\n",
      "Epoch: 3711, Training Loss: 0.071610, Validation Loss: 0.059132\n",
      "Epoch: 3712, Training Loss: 0.071582, Validation Loss: 0.060911\n",
      "Epoch: 3713, Training Loss: 0.071576, Validation Loss: 0.059449\n",
      "Epoch: 3714, Training Loss: 0.071556, Validation Loss: 0.060890\n",
      "Epoch: 3715, Training Loss: 0.071589, Validation Loss: 0.059356\n",
      "Epoch: 3716, Training Loss: 0.071662, Validation Loss: 0.061019\n",
      "Epoch: 3717, Training Loss: 0.071844, Validation Loss: 0.059099\n",
      "Epoch: 3718, Training Loss: 0.072033, Validation Loss: 0.061620\n",
      "Epoch: 3719, Training Loss: 0.072213, Validation Loss: 0.059048\n",
      "Epoch: 3720, Training Loss: 0.072443, Validation Loss: 0.062316\n",
      "Epoch: 3721, Training Loss: 0.072799, Validation Loss: 0.059185\n",
      "Epoch: 3722, Training Loss: 0.073463, Validation Loss: 0.064176\n",
      "Epoch: 3723, Training Loss: 0.074146, Validation Loss: 0.059897\n",
      "Epoch: 3724, Training Loss: 0.075066, Validation Loss: 0.066327\n",
      "Epoch: 3725, Training Loss: 0.075917, Validation Loss: 0.060539\n",
      "Epoch: 3726, Training Loss: 0.077274, Validation Loss: 0.069058\n",
      "Epoch: 3727, Training Loss: 0.077752, Validation Loss: 0.061780\n",
      "Epoch: 3728, Training Loss: 0.078377, Validation Loss: 0.070362\n",
      "Epoch: 3729, Training Loss: 0.077536, Validation Loss: 0.061495\n",
      "Epoch: 3730, Training Loss: 0.076545, Validation Loss: 0.067425\n",
      "Epoch: 3731, Training Loss: 0.074380, Validation Loss: 0.059407\n",
      "Epoch: 3732, Training Loss: 0.072777, Validation Loss: 0.062211\n",
      "Epoch: 3733, Training Loss: 0.071666, Validation Loss: 0.059156\n",
      "Epoch: 3734, Training Loss: 0.071434, Validation Loss: 0.059448\n",
      "Epoch: 3735, Training Loss: 0.071962, Validation Loss: 0.061702\n",
      "Epoch: 3736, Training Loss: 0.072868, Validation Loss: 0.059764\n",
      "Epoch: 3737, Training Loss: 0.073817, Validation Loss: 0.065056\n",
      "Epoch: 3738, Training Loss: 0.074267, Validation Loss: 0.060288\n",
      "Epoch: 3739, Training Loss: 0.074322, Validation Loss: 0.065532\n",
      "Epoch: 3740, Training Loss: 0.073714, Validation Loss: 0.059209\n",
      "Epoch: 3741, Training Loss: 0.072801, Validation Loss: 0.062567\n",
      "Epoch: 3742, Training Loss: 0.071859, Validation Loss: 0.058401\n",
      "Epoch: 3743, Training Loss: 0.071432, Validation Loss: 0.059233\n",
      "Epoch: 3744, Training Loss: 0.071582, Validation Loss: 0.060105\n",
      "Epoch: 3745, Training Loss: 0.072024, Validation Loss: 0.058628\n",
      "Epoch: 3746, Training Loss: 0.072436, Validation Loss: 0.062210\n",
      "Epoch: 3747, Training Loss: 0.072641, Validation Loss: 0.059316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3748, Training Loss: 0.072624, Validation Loss: 0.062763\n",
      "Epoch: 3749, Training Loss: 0.072346, Validation Loss: 0.059631\n",
      "Epoch: 3750, Training Loss: 0.071986, Validation Loss: 0.061651\n",
      "Epoch: 3751, Training Loss: 0.071642, Validation Loss: 0.059357\n",
      "Epoch: 3752, Training Loss: 0.071428, Validation Loss: 0.060253\n",
      "Epoch: 3753, Training Loss: 0.071355, Validation Loss: 0.059524\n",
      "Epoch: 3754, Training Loss: 0.071378, Validation Loss: 0.059706\n",
      "Epoch: 3755, Training Loss: 0.071459, Validation Loss: 0.060401\n",
      "Epoch: 3756, Training Loss: 0.071552, Validation Loss: 0.059648\n",
      "Epoch: 3757, Training Loss: 0.071689, Validation Loss: 0.061385\n",
      "Epoch: 3758, Training Loss: 0.071952, Validation Loss: 0.059332\n",
      "Epoch: 3759, Training Loss: 0.072347, Validation Loss: 0.062475\n",
      "Epoch: 3760, Training Loss: 0.072911, Validation Loss: 0.059046\n",
      "Epoch: 3761, Training Loss: 0.073667, Validation Loss: 0.064299\n",
      "Epoch: 3762, Training Loss: 0.074446, Validation Loss: 0.059684\n",
      "Epoch: 3763, Training Loss: 0.075573, Validation Loss: 0.066742\n",
      "Epoch: 3764, Training Loss: 0.076194, Validation Loss: 0.060876\n",
      "Epoch: 3765, Training Loss: 0.076938, Validation Loss: 0.068058\n",
      "Epoch: 3766, Training Loss: 0.076720, Validation Loss: 0.061163\n",
      "Epoch: 3767, Training Loss: 0.076832, Validation Loss: 0.068057\n",
      "Epoch: 3768, Training Loss: 0.075641, Validation Loss: 0.060794\n",
      "Epoch: 3769, Training Loss: 0.074515, Validation Loss: 0.065275\n",
      "Epoch: 3770, Training Loss: 0.072998, Validation Loss: 0.059244\n",
      "Epoch: 3771, Training Loss: 0.071970, Validation Loss: 0.061359\n",
      "Epoch: 3772, Training Loss: 0.071421, Validation Loss: 0.059300\n",
      "Epoch: 3773, Training Loss: 0.071376, Validation Loss: 0.059699\n",
      "Epoch: 3774, Training Loss: 0.071741, Validation Loss: 0.061390\n",
      "Epoch: 3775, Training Loss: 0.072340, Validation Loss: 0.059335\n",
      "Epoch: 3776, Training Loss: 0.073040, Validation Loss: 0.063607\n",
      "Epoch: 3777, Training Loss: 0.073404, Validation Loss: 0.059322\n",
      "Epoch: 3778, Training Loss: 0.073432, Validation Loss: 0.064093\n",
      "Epoch: 3779, Training Loss: 0.073056, Validation Loss: 0.059123\n",
      "Epoch: 3780, Training Loss: 0.072579, Validation Loss: 0.062734\n",
      "Epoch: 3781, Training Loss: 0.071979, Validation Loss: 0.058941\n",
      "Epoch: 3782, Training Loss: 0.071538, Validation Loss: 0.060380\n",
      "Epoch: 3783, Training Loss: 0.071350, Validation Loss: 0.059454\n",
      "Epoch: 3784, Training Loss: 0.071344, Validation Loss: 0.059426\n",
      "Epoch: 3785, Training Loss: 0.071544, Validation Loss: 0.061050\n",
      "Epoch: 3786, Training Loss: 0.071839, Validation Loss: 0.059335\n",
      "Epoch: 3787, Training Loss: 0.072226, Validation Loss: 0.062282\n",
      "Epoch: 3788, Training Loss: 0.072637, Validation Loss: 0.059311\n",
      "Epoch: 3789, Training Loss: 0.073130, Validation Loss: 0.063738\n",
      "Epoch: 3790, Training Loss: 0.073704, Validation Loss: 0.059543\n",
      "Epoch: 3791, Training Loss: 0.074332, Validation Loss: 0.065282\n",
      "Epoch: 3792, Training Loss: 0.074616, Validation Loss: 0.059640\n",
      "Epoch: 3793, Training Loss: 0.074806, Validation Loss: 0.065616\n",
      "Epoch: 3794, Training Loss: 0.074588, Validation Loss: 0.059592\n",
      "Epoch: 3795, Training Loss: 0.074304, Validation Loss: 0.064881\n",
      "Epoch: 3796, Training Loss: 0.073498, Validation Loss: 0.059483\n",
      "Epoch: 3797, Training Loss: 0.072934, Validation Loss: 0.063068\n",
      "Epoch: 3798, Training Loss: 0.072250, Validation Loss: 0.059495\n",
      "Epoch: 3799, Training Loss: 0.071741, Validation Loss: 0.061203\n",
      "Epoch: 3800, Training Loss: 0.071402, Validation Loss: 0.059650\n",
      "Epoch: 3801, Training Loss: 0.071276, Validation Loss: 0.060327\n",
      "Epoch: 3802, Training Loss: 0.071300, Validation Loss: 0.060257\n",
      "Epoch: 3803, Training Loss: 0.071402, Validation Loss: 0.059923\n",
      "Epoch: 3804, Training Loss: 0.071515, Validation Loss: 0.060846\n",
      "Epoch: 3805, Training Loss: 0.071602, Validation Loss: 0.059669\n",
      "Epoch: 3806, Training Loss: 0.071765, Validation Loss: 0.061749\n",
      "Epoch: 3807, Training Loss: 0.071978, Validation Loss: 0.059443\n",
      "Epoch: 3808, Training Loss: 0.072412, Validation Loss: 0.063009\n",
      "Epoch: 3809, Training Loss: 0.073137, Validation Loss: 0.059375\n",
      "Epoch: 3810, Training Loss: 0.075126, Validation Loss: 0.066608\n",
      "Epoch: 3811, Training Loss: 0.080906, Validation Loss: 0.063745\n",
      "Epoch: 3812, Training Loss: 0.088401, Validation Loss: 0.083244\n",
      "Epoch: 3813, Training Loss: 0.086126, Validation Loss: 0.066636\n",
      "Epoch: 3814, Training Loss: 0.080096, Validation Loss: 0.071968\n",
      "Epoch: 3815, Training Loss: 0.075982, Validation Loss: 0.060399\n",
      "Epoch: 3816, Training Loss: 0.079017, Validation Loss: 0.063915\n",
      "Epoch: 3817, Training Loss: 0.079970, Validation Loss: 0.069374\n",
      "Epoch: 3818, Training Loss: 0.075363, Validation Loss: 0.060531\n",
      "Epoch: 3819, Training Loss: 0.071563, Validation Loss: 0.060048\n",
      "Epoch: 3820, Training Loss: 0.074162, Validation Loss: 0.064299\n",
      "Epoch: 3821, Training Loss: 0.077295, Validation Loss: 0.064148\n",
      "Epoch: 3822, Training Loss: 0.075087, Validation Loss: 0.064905\n",
      "Epoch: 3823, Training Loss: 0.073049, Validation Loss: 0.062823\n",
      "Epoch: 3824, Training Loss: 0.074609, Validation Loss: 0.059824\n",
      "Epoch: 3825, Training Loss: 0.075491, Validation Loss: 0.067086\n",
      "Epoch: 3826, Training Loss: 0.073093, Validation Loss: 0.058565\n",
      "Epoch: 3827, Training Loss: 0.071967, Validation Loss: 0.059440\n",
      "Epoch: 3828, Training Loss: 0.073278, Validation Loss: 0.062481\n",
      "Epoch: 3829, Training Loss: 0.073048, Validation Loss: 0.058320\n",
      "Epoch: 3830, Training Loss: 0.071681, Validation Loss: 0.060052\n",
      "Epoch: 3831, Training Loss: 0.072391, Validation Loss: 0.061230\n",
      "Epoch: 3832, Training Loss: 0.073347, Validation Loss: 0.060533\n",
      "Epoch: 3833, Training Loss: 0.072297, Validation Loss: 0.061273\n",
      "Epoch: 3834, Training Loss: 0.071871, Validation Loss: 0.061386\n",
      "Epoch: 3835, Training Loss: 0.072668, Validation Loss: 0.059138\n",
      "Epoch: 3836, Training Loss: 0.072485, Validation Loss: 0.062326\n",
      "Epoch: 3837, Training Loss: 0.071660, Validation Loss: 0.058808\n",
      "Epoch: 3838, Training Loss: 0.071724, Validation Loss: 0.059348\n",
      "Epoch: 3839, Training Loss: 0.072119, Validation Loss: 0.060957\n",
      "Epoch: 3840, Training Loss: 0.071784, Validation Loss: 0.058901\n",
      "Epoch: 3841, Training Loss: 0.071412, Validation Loss: 0.059435\n",
      "Epoch: 3842, Training Loss: 0.071674, Validation Loss: 0.060413\n",
      "Epoch: 3843, Training Loss: 0.071907, Validation Loss: 0.059530\n",
      "Epoch: 3844, Training Loss: 0.071635, Validation Loss: 0.060726\n",
      "Epoch: 3845, Training Loss: 0.071380, Validation Loss: 0.060019\n",
      "Epoch: 3846, Training Loss: 0.071549, Validation Loss: 0.059155\n",
      "Epoch: 3847, Training Loss: 0.071663, Validation Loss: 0.060448\n",
      "Epoch: 3848, Training Loss: 0.071466, Validation Loss: 0.059308\n",
      "Epoch: 3849, Training Loss: 0.071373, Validation Loss: 0.059812\n",
      "Epoch: 3850, Training Loss: 0.071467, Validation Loss: 0.060246\n",
      "Epoch: 3851, Training Loss: 0.071543, Validation Loss: 0.059148\n",
      "Epoch: 3852, Training Loss: 0.071409, Validation Loss: 0.059973\n",
      "Epoch: 3853, Training Loss: 0.071327, Validation Loss: 0.059778\n",
      "Epoch: 3854, Training Loss: 0.071416, Validation Loss: 0.059565\n",
      "Epoch: 3855, Training Loss: 0.071423, Validation Loss: 0.060140\n",
      "Epoch: 3856, Training Loss: 0.071347, Validation Loss: 0.059644\n",
      "Epoch: 3857, Training Loss: 0.071306, Validation Loss: 0.059783\n",
      "Epoch: 3858, Training Loss: 0.071423, Validation Loss: 0.060881\n",
      "Epoch: 3859, Training Loss: 0.071457, Validation Loss: 0.059669\n",
      "Epoch: 3860, Training Loss: 0.071449, Validation Loss: 0.060548\n",
      "Epoch: 3861, Training Loss: 0.071464, Validation Loss: 0.059324\n",
      "Epoch: 3862, Training Loss: 0.071606, Validation Loss: 0.060731\n",
      "Epoch: 3863, Training Loss: 0.071783, Validation Loss: 0.059652\n",
      "Epoch: 3864, Training Loss: 0.071962, Validation Loss: 0.061198\n",
      "Epoch: 3865, Training Loss: 0.072264, Validation Loss: 0.059198\n",
      "Epoch: 3866, Training Loss: 0.072854, Validation Loss: 0.062981\n",
      "Epoch: 3867, Training Loss: 0.074076, Validation Loss: 0.060004\n",
      "Epoch: 3868, Training Loss: 0.076037, Validation Loss: 0.067632\n",
      "Epoch: 3869, Training Loss: 0.078046, Validation Loss: 0.062432\n",
      "Epoch: 3870, Training Loss: 0.081570, Validation Loss: 0.073609\n",
      "Epoch: 3871, Training Loss: 0.084025, Validation Loss: 0.066733\n",
      "Epoch: 3872, Training Loss: 0.088566, Validation Loss: 0.080989\n",
      "Epoch: 3873, Training Loss: 0.087657, Validation Loss: 0.069539\n",
      "Epoch: 3874, Training Loss: 0.087613, Validation Loss: 0.080081\n",
      "Epoch: 3875, Training Loss: 0.080824, Validation Loss: 0.064291\n",
      "Epoch: 3876, Training Loss: 0.075284, Validation Loss: 0.064262\n",
      "Epoch: 3877, Training Loss: 0.072097, Validation Loss: 0.059233\n",
      "Epoch: 3878, Training Loss: 0.072520, Validation Loss: 0.058251\n",
      "Epoch: 3879, Training Loss: 0.075040, Validation Loss: 0.065729\n",
      "Epoch: 3880, Training Loss: 0.076931, Validation Loss: 0.062948\n",
      "Epoch: 3881, Training Loss: 0.077783, Validation Loss: 0.069060\n",
      "Epoch: 3882, Training Loss: 0.075766, Validation Loss: 0.063370\n",
      "Epoch: 3883, Training Loss: 0.073542, Validation Loss: 0.061942\n",
      "Epoch: 3884, Training Loss: 0.071935, Validation Loss: 0.060300\n",
      "Epoch: 3885, Training Loss: 0.071831, Validation Loss: 0.058828\n",
      "Epoch: 3886, Training Loss: 0.073125, Validation Loss: 0.062574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3887, Training Loss: 0.074264, Validation Loss: 0.061767\n",
      "Epoch: 3888, Training Loss: 0.074352, Validation Loss: 0.063350\n",
      "Epoch: 3889, Training Loss: 0.073069, Validation Loss: 0.060190\n",
      "Epoch: 3890, Training Loss: 0.071836, Validation Loss: 0.060453\n",
      "Epoch: 3891, Training Loss: 0.071458, Validation Loss: 0.059185\n",
      "Epoch: 3892, Training Loss: 0.071918, Validation Loss: 0.061140\n",
      "Epoch: 3893, Training Loss: 0.072312, Validation Loss: 0.061078\n",
      "Epoch: 3894, Training Loss: 0.072260, Validation Loss: 0.060207\n",
      "Epoch: 3895, Training Loss: 0.072368, Validation Loss: 0.062149\n",
      "Epoch: 3896, Training Loss: 0.072736, Validation Loss: 0.058997\n",
      "Epoch: 3897, Training Loss: 0.073134, Validation Loss: 0.063796\n",
      "Epoch: 3898, Training Loss: 0.072650, Validation Loss: 0.058865\n",
      "Epoch: 3899, Training Loss: 0.071761, Validation Loss: 0.060774\n",
      "Epoch: 3900, Training Loss: 0.071476, Validation Loss: 0.059087\n",
      "Epoch: 3901, Training Loss: 0.071672, Validation Loss: 0.058980\n",
      "Epoch: 3902, Training Loss: 0.071924, Validation Loss: 0.062237\n",
      "Epoch: 3903, Training Loss: 0.072231, Validation Loss: 0.060047\n",
      "Epoch: 3904, Training Loss: 0.072594, Validation Loss: 0.063434\n",
      "Epoch: 3905, Training Loss: 0.073199, Validation Loss: 0.060400\n",
      "Epoch: 3906, Training Loss: 0.073886, Validation Loss: 0.064296\n",
      "Epoch: 3907, Training Loss: 0.074163, Validation Loss: 0.060399\n",
      "Epoch: 3908, Training Loss: 0.074338, Validation Loss: 0.065211\n",
      "Epoch: 3909, Training Loss: 0.073616, Validation Loss: 0.059380\n",
      "Epoch: 3910, Training Loss: 0.072679, Validation Loss: 0.062365\n",
      "Epoch: 3911, Training Loss: 0.072025, Validation Loss: 0.058235\n",
      "Epoch: 3912, Training Loss: 0.071451, Validation Loss: 0.060077\n",
      "Epoch: 3913, Training Loss: 0.071203, Validation Loss: 0.059971\n",
      "Epoch: 3914, Training Loss: 0.071416, Validation Loss: 0.059875\n",
      "Epoch: 3915, Training Loss: 0.071695, Validation Loss: 0.061706\n",
      "Epoch: 3916, Training Loss: 0.072201, Validation Loss: 0.059047\n",
      "Epoch: 3917, Training Loss: 0.072699, Validation Loss: 0.062828\n",
      "Epoch: 3918, Training Loss: 0.073041, Validation Loss: 0.059308\n",
      "Epoch: 3919, Training Loss: 0.073433, Validation Loss: 0.064390\n",
      "Epoch: 3920, Training Loss: 0.073327, Validation Loss: 0.059570\n",
      "Epoch: 3921, Training Loss: 0.072766, Validation Loss: 0.062522\n",
      "Epoch: 3922, Training Loss: 0.072346, Validation Loss: 0.058266\n",
      "Epoch: 3923, Training Loss: 0.071819, Validation Loss: 0.060144\n",
      "Epoch: 3924, Training Loss: 0.071332, Validation Loss: 0.059223\n",
      "Epoch: 3925, Training Loss: 0.071315, Validation Loss: 0.060605\n",
      "Epoch: 3926, Training Loss: 0.071307, Validation Loss: 0.061033\n",
      "Epoch: 3927, Training Loss: 0.071474, Validation Loss: 0.059829\n",
      "Epoch: 3928, Training Loss: 0.071848, Validation Loss: 0.061157\n",
      "Epoch: 3929, Training Loss: 0.072093, Validation Loss: 0.059021\n",
      "Epoch: 3930, Training Loss: 0.072421, Validation Loss: 0.062889\n",
      "Epoch: 3931, Training Loss: 0.072888, Validation Loss: 0.059318\n",
      "Epoch: 3932, Training Loss: 0.072935, Validation Loss: 0.063455\n",
      "Epoch: 3933, Training Loss: 0.072999, Validation Loss: 0.058686\n",
      "Epoch: 3934, Training Loss: 0.072925, Validation Loss: 0.062682\n",
      "Epoch: 3935, Training Loss: 0.072637, Validation Loss: 0.059364\n",
      "Epoch: 3936, Training Loss: 0.072585, Validation Loss: 0.063430\n",
      "Epoch: 3937, Training Loss: 0.072316, Validation Loss: 0.060154\n",
      "Epoch: 3938, Training Loss: 0.071974, Validation Loss: 0.062306\n",
      "Epoch: 3939, Training Loss: 0.071881, Validation Loss: 0.059017\n",
      "Epoch: 3940, Training Loss: 0.071730, Validation Loss: 0.061295\n",
      "Epoch: 3941, Training Loss: 0.071546, Validation Loss: 0.059184\n",
      "Epoch: 3942, Training Loss: 0.071431, Validation Loss: 0.061261\n",
      "Epoch: 3943, Training Loss: 0.071296, Validation Loss: 0.059588\n",
      "Epoch: 3944, Training Loss: 0.071243, Validation Loss: 0.060553\n",
      "Epoch: 3945, Training Loss: 0.071240, Validation Loss: 0.059491\n",
      "Epoch: 3946, Training Loss: 0.071211, Validation Loss: 0.060647\n",
      "Epoch: 3947, Training Loss: 0.071243, Validation Loss: 0.059818\n",
      "Epoch: 3948, Training Loss: 0.071271, Validation Loss: 0.060862\n",
      "Epoch: 3949, Training Loss: 0.071307, Validation Loss: 0.059113\n",
      "Epoch: 3950, Training Loss: 0.071481, Validation Loss: 0.060955\n",
      "Epoch: 3951, Training Loss: 0.071707, Validation Loss: 0.059087\n",
      "Epoch: 3952, Training Loss: 0.072217, Validation Loss: 0.062910\n",
      "Epoch: 3953, Training Loss: 0.073043, Validation Loss: 0.059764\n",
      "Epoch: 3954, Training Loss: 0.074474, Validation Loss: 0.066041\n",
      "Epoch: 3955, Training Loss: 0.076332, Validation Loss: 0.061006\n",
      "Epoch: 3956, Training Loss: 0.079379, Validation Loss: 0.071565\n",
      "Epoch: 3957, Training Loss: 0.082026, Validation Loss: 0.064584\n",
      "Epoch: 3958, Training Loss: 0.085826, Validation Loss: 0.079228\n",
      "Epoch: 3959, Training Loss: 0.085345, Validation Loss: 0.066885\n",
      "Epoch: 3960, Training Loss: 0.084673, Validation Loss: 0.077297\n",
      "Epoch: 3961, Training Loss: 0.079438, Validation Loss: 0.062551\n",
      "Epoch: 3962, Training Loss: 0.075111, Validation Loss: 0.064712\n",
      "Epoch: 3963, Training Loss: 0.072034, Validation Loss: 0.059522\n",
      "Epoch: 3964, Training Loss: 0.072016, Validation Loss: 0.058843\n",
      "Epoch: 3965, Training Loss: 0.074045, Validation Loss: 0.065442\n",
      "Epoch: 3966, Training Loss: 0.075696, Validation Loss: 0.061497\n",
      "Epoch: 3967, Training Loss: 0.076286, Validation Loss: 0.067969\n",
      "Epoch: 3968, Training Loss: 0.074758, Validation Loss: 0.061407\n",
      "Epoch: 3969, Training Loss: 0.073207, Validation Loss: 0.061858\n",
      "Epoch: 3970, Training Loss: 0.072149, Validation Loss: 0.061439\n",
      "Epoch: 3971, Training Loss: 0.072089, Validation Loss: 0.059024\n",
      "Epoch: 3972, Training Loss: 0.072638, Validation Loss: 0.063364\n",
      "Epoch: 3973, Training Loss: 0.073075, Validation Loss: 0.060236\n",
      "Epoch: 3974, Training Loss: 0.073406, Validation Loss: 0.062754\n",
      "Epoch: 3975, Training Loss: 0.072934, Validation Loss: 0.061423\n",
      "Epoch: 3976, Training Loss: 0.072132, Validation Loss: 0.060436\n",
      "Epoch: 3977, Training Loss: 0.071431, Validation Loss: 0.060980\n",
      "Epoch: 3978, Training Loss: 0.071426, Validation Loss: 0.059493\n",
      "Epoch: 3979, Training Loss: 0.072058, Validation Loss: 0.061479\n",
      "Epoch: 3980, Training Loss: 0.072547, Validation Loss: 0.060633\n",
      "Epoch: 3981, Training Loss: 0.072538, Validation Loss: 0.061978\n",
      "Epoch: 3982, Training Loss: 0.071974, Validation Loss: 0.059846\n",
      "Epoch: 3983, Training Loss: 0.071506, Validation Loss: 0.061004\n",
      "Epoch: 3984, Training Loss: 0.071447, Validation Loss: 0.058857\n",
      "Epoch: 3985, Training Loss: 0.071528, Validation Loss: 0.060927\n",
      "Epoch: 3986, Training Loss: 0.071437, Validation Loss: 0.059542\n",
      "Epoch: 3987, Training Loss: 0.071211, Validation Loss: 0.060317\n",
      "Epoch: 3988, Training Loss: 0.071114, Validation Loss: 0.060455\n",
      "Epoch: 3989, Training Loss: 0.071263, Validation Loss: 0.059554\n",
      "Epoch: 3990, Training Loss: 0.071494, Validation Loss: 0.061441\n",
      "Epoch: 3991, Training Loss: 0.071649, Validation Loss: 0.059068\n",
      "Epoch: 3992, Training Loss: 0.071786, Validation Loss: 0.061867\n",
      "Epoch: 3993, Training Loss: 0.072134, Validation Loss: 0.059037\n",
      "Epoch: 3994, Training Loss: 0.072779, Validation Loss: 0.063520\n",
      "Epoch: 3995, Training Loss: 0.073756, Validation Loss: 0.060148\n",
      "Epoch: 3996, Training Loss: 0.075143, Validation Loss: 0.066751\n",
      "Epoch: 3997, Training Loss: 0.076347, Validation Loss: 0.061552\n",
      "Epoch: 3998, Training Loss: 0.077919, Validation Loss: 0.070493\n",
      "Epoch: 3999, Training Loss: 0.078236, Validation Loss: 0.062201\n",
      "Epoch: 4000, Training Loss: 0.078279, Validation Loss: 0.070687\n",
      "Epoch: 4001, Training Loss: 0.076288, Validation Loss: 0.060280\n",
      "Epoch: 4002, Training Loss: 0.074183, Validation Loss: 0.064258\n",
      "Epoch: 4003, Training Loss: 0.072171, Validation Loss: 0.058496\n",
      "Epoch: 4004, Training Loss: 0.071322, Validation Loss: 0.059430\n",
      "Epoch: 4005, Training Loss: 0.071614, Validation Loss: 0.061948\n",
      "Epoch: 4006, Training Loss: 0.072619, Validation Loss: 0.060133\n",
      "Epoch: 4007, Training Loss: 0.073591, Validation Loss: 0.065456\n",
      "Epoch: 4008, Training Loss: 0.073976, Validation Loss: 0.060322\n",
      "Epoch: 4009, Training Loss: 0.073719, Validation Loss: 0.064250\n",
      "Epoch: 4010, Training Loss: 0.072713, Validation Loss: 0.058927\n",
      "Epoch: 4011, Training Loss: 0.071739, Validation Loss: 0.060469\n",
      "Epoch: 4012, Training Loss: 0.071208, Validation Loss: 0.059816\n",
      "Epoch: 4013, Training Loss: 0.071338, Validation Loss: 0.059469\n",
      "Epoch: 4014, Training Loss: 0.071885, Validation Loss: 0.062332\n",
      "Epoch: 4015, Training Loss: 0.072424, Validation Loss: 0.060234\n",
      "Epoch: 4016, Training Loss: 0.072711, Validation Loss: 0.063384\n",
      "Epoch: 4017, Training Loss: 0.072470, Validation Loss: 0.059672\n",
      "Epoch: 4018, Training Loss: 0.071976, Validation Loss: 0.061570\n",
      "Epoch: 4019, Training Loss: 0.071413, Validation Loss: 0.058539\n",
      "Epoch: 4020, Training Loss: 0.071092, Validation Loss: 0.059636\n",
      "Epoch: 4021, Training Loss: 0.071096, Validation Loss: 0.060034\n",
      "Epoch: 4022, Training Loss: 0.071325, Validation Loss: 0.059900\n",
      "Epoch: 4023, Training Loss: 0.071629, Validation Loss: 0.061961\n",
      "Epoch: 4024, Training Loss: 0.071936, Validation Loss: 0.059443\n",
      "Epoch: 4025, Training Loss: 0.072281, Validation Loss: 0.062803\n",
      "Epoch: 4026, Training Loss: 0.072586, Validation Loss: 0.058858\n",
      "Epoch: 4027, Training Loss: 0.072670, Validation Loss: 0.063341\n",
      "Epoch: 4028, Training Loss: 0.072788, Validation Loss: 0.058784\n",
      "Epoch: 4029, Training Loss: 0.072668, Validation Loss: 0.063282\n",
      "Epoch: 4030, Training Loss: 0.072625, Validation Loss: 0.059167\n",
      "Epoch: 4031, Training Loss: 0.072554, Validation Loss: 0.063405\n",
      "Epoch: 4032, Training Loss: 0.072407, Validation Loss: 0.059504\n",
      "Epoch: 4033, Training Loss: 0.072317, Validation Loss: 0.063065\n",
      "Epoch: 4034, Training Loss: 0.072251, Validation Loss: 0.059257\n",
      "Epoch: 4035, Training Loss: 0.072206, Validation Loss: 0.062776\n",
      "Epoch: 4036, Training Loss: 0.072144, Validation Loss: 0.059181\n",
      "Epoch: 4037, Training Loss: 0.072061, Validation Loss: 0.062731\n",
      "Epoch: 4038, Training Loss: 0.071958, Validation Loss: 0.059008\n",
      "Epoch: 4039, Training Loss: 0.071894, Validation Loss: 0.062319\n",
      "Epoch: 4040, Training Loss: 0.071799, Validation Loss: 0.059236\n",
      "Epoch: 4041, Training Loss: 0.071773, Validation Loss: 0.062327\n",
      "Epoch: 4042, Training Loss: 0.071690, Validation Loss: 0.059353\n",
      "Epoch: 4043, Training Loss: 0.071596, Validation Loss: 0.061684\n",
      "Epoch: 4044, Training Loss: 0.071491, Validation Loss: 0.059155\n",
      "Epoch: 4045, Training Loss: 0.071437, Validation Loss: 0.061618\n",
      "Epoch: 4046, Training Loss: 0.071432, Validation Loss: 0.059346\n",
      "Epoch: 4047, Training Loss: 0.071533, Validation Loss: 0.061819\n",
      "Epoch: 4048, Training Loss: 0.071789, Validation Loss: 0.058946\n",
      "Epoch: 4049, Training Loss: 0.072140, Validation Loss: 0.062692\n",
      "Epoch: 4050, Training Loss: 0.072743, Validation Loss: 0.058830\n",
      "Epoch: 4051, Training Loss: 0.073427, Validation Loss: 0.064504\n",
      "Epoch: 4052, Training Loss: 0.074214, Validation Loss: 0.059455\n",
      "Epoch: 4053, Training Loss: 0.075161, Validation Loss: 0.066873\n",
      "Epoch: 4054, Training Loss: 0.075740, Validation Loss: 0.060591\n",
      "Epoch: 4055, Training Loss: 0.076387, Validation Loss: 0.068406\n",
      "Epoch: 4056, Training Loss: 0.076161, Validation Loss: 0.061006\n",
      "Epoch: 4057, Training Loss: 0.075894, Validation Loss: 0.067789\n",
      "Epoch: 4058, Training Loss: 0.074439, Validation Loss: 0.060113\n",
      "Epoch: 4059, Training Loss: 0.073112, Validation Loss: 0.063973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4060, Training Loss: 0.071869, Validation Loss: 0.059253\n",
      "Epoch: 4061, Training Loss: 0.071126, Validation Loss: 0.060547\n",
      "Epoch: 4062, Training Loss: 0.071000, Validation Loss: 0.060485\n",
      "Epoch: 4063, Training Loss: 0.071379, Validation Loss: 0.059574\n",
      "Epoch: 4064, Training Loss: 0.071964, Validation Loss: 0.062597\n",
      "Epoch: 4065, Training Loss: 0.072428, Validation Loss: 0.059344\n",
      "Epoch: 4066, Training Loss: 0.072707, Validation Loss: 0.063143\n",
      "Epoch: 4067, Training Loss: 0.072575, Validation Loss: 0.059349\n",
      "Epoch: 4068, Training Loss: 0.072291, Validation Loss: 0.062986\n",
      "Epoch: 4069, Training Loss: 0.071841, Validation Loss: 0.059232\n",
      "Epoch: 4070, Training Loss: 0.071430, Validation Loss: 0.061522\n",
      "Epoch: 4071, Training Loss: 0.071161, Validation Loss: 0.059110\n",
      "Epoch: 4072, Training Loss: 0.071015, Validation Loss: 0.060585\n",
      "Epoch: 4073, Training Loss: 0.070944, Validation Loss: 0.059875\n",
      "Epoch: 4074, Training Loss: 0.070935, Validation Loss: 0.060025\n",
      "Epoch: 4075, Training Loss: 0.070987, Validation Loss: 0.060510\n",
      "Epoch: 4076, Training Loss: 0.071145, Validation Loss: 0.059300\n",
      "Epoch: 4077, Training Loss: 0.071393, Validation Loss: 0.061551\n",
      "Epoch: 4078, Training Loss: 0.071822, Validation Loss: 0.059101\n",
      "Epoch: 4079, Training Loss: 0.072307, Validation Loss: 0.063179\n",
      "Epoch: 4080, Training Loss: 0.073169, Validation Loss: 0.059197\n",
      "Epoch: 4081, Training Loss: 0.074171, Validation Loss: 0.065728\n",
      "Epoch: 4082, Training Loss: 0.075241, Validation Loss: 0.060328\n",
      "Epoch: 4083, Training Loss: 0.076541, Validation Loss: 0.069011\n",
      "Epoch: 4084, Training Loss: 0.077586, Validation Loss: 0.061998\n",
      "Epoch: 4085, Training Loss: 0.078881, Validation Loss: 0.071850\n",
      "Epoch: 4086, Training Loss: 0.078972, Validation Loss: 0.062715\n",
      "Epoch: 4087, Training Loss: 0.078815, Validation Loss: 0.071608\n",
      "Epoch: 4088, Training Loss: 0.076691, Validation Loss: 0.061064\n",
      "Epoch: 4089, Training Loss: 0.074401, Validation Loss: 0.065811\n",
      "Epoch: 4090, Training Loss: 0.072116, Validation Loss: 0.059389\n",
      "Epoch: 4091, Training Loss: 0.071107, Validation Loss: 0.060361\n",
      "Epoch: 4092, Training Loss: 0.071309, Validation Loss: 0.061816\n",
      "Epoch: 4093, Training Loss: 0.072112, Validation Loss: 0.059841\n",
      "Epoch: 4094, Training Loss: 0.072922, Validation Loss: 0.064281\n",
      "Epoch: 4095, Training Loss: 0.073109, Validation Loss: 0.059822\n",
      "Epoch: 4096, Training Loss: 0.072878, Validation Loss: 0.063177\n",
      "Epoch: 4097, Training Loss: 0.072106, Validation Loss: 0.059625\n",
      "Epoch: 4098, Training Loss: 0.071483, Validation Loss: 0.060241\n",
      "Epoch: 4099, Training Loss: 0.071183, Validation Loss: 0.060591\n",
      "Epoch: 4100, Training Loss: 0.071234, Validation Loss: 0.059340\n",
      "Epoch: 4101, Training Loss: 0.071465, Validation Loss: 0.061658\n",
      "Epoch: 4102, Training Loss: 0.071750, Validation Loss: 0.059742\n",
      "Epoch: 4103, Training Loss: 0.071958, Validation Loss: 0.061936\n",
      "Epoch: 4104, Training Loss: 0.071852, Validation Loss: 0.059862\n",
      "Epoch: 4105, Training Loss: 0.071573, Validation Loss: 0.061197\n",
      "Epoch: 4106, Training Loss: 0.071195, Validation Loss: 0.059479\n",
      "Epoch: 4107, Training Loss: 0.070960, Validation Loss: 0.060416\n",
      "Epoch: 4108, Training Loss: 0.070902, Validation Loss: 0.059726\n",
      "Epoch: 4109, Training Loss: 0.070963, Validation Loss: 0.060297\n",
      "Epoch: 4110, Training Loss: 0.071031, Validation Loss: 0.060063\n",
      "Epoch: 4111, Training Loss: 0.071031, Validation Loss: 0.059836\n",
      "Epoch: 4112, Training Loss: 0.071012, Validation Loss: 0.060406\n",
      "Epoch: 4113, Training Loss: 0.071039, Validation Loss: 0.059395\n",
      "Epoch: 4114, Training Loss: 0.071167, Validation Loss: 0.061343\n",
      "Epoch: 4115, Training Loss: 0.071440, Validation Loss: 0.059439\n",
      "Epoch: 4116, Training Loss: 0.071869, Validation Loss: 0.062757\n",
      "Epoch: 4117, Training Loss: 0.072530, Validation Loss: 0.059566\n",
      "Epoch: 4118, Training Loss: 0.073603, Validation Loss: 0.065352\n",
      "Epoch: 4119, Training Loss: 0.075101, Validation Loss: 0.060317\n",
      "Epoch: 4120, Training Loss: 0.077427, Validation Loss: 0.069985\n",
      "Epoch: 4121, Training Loss: 0.079688, Validation Loss: 0.063097\n",
      "Epoch: 4122, Training Loss: 0.082538, Validation Loss: 0.075942\n",
      "Epoch: 4123, Training Loss: 0.083196, Validation Loss: 0.065672\n",
      "Epoch: 4124, Training Loss: 0.083401, Validation Loss: 0.077012\n",
      "Epoch: 4125, Training Loss: 0.079225, Validation Loss: 0.062982\n",
      "Epoch: 4126, Training Loss: 0.075277, Validation Loss: 0.066316\n",
      "Epoch: 4127, Training Loss: 0.072128, Validation Loss: 0.059899\n",
      "Epoch: 4128, Training Loss: 0.071668, Validation Loss: 0.059402\n",
      "Epoch: 4129, Training Loss: 0.072955, Validation Loss: 0.064604\n",
      "Epoch: 4130, Training Loss: 0.074325, Validation Loss: 0.060906\n",
      "Epoch: 4131, Training Loss: 0.074985, Validation Loss: 0.067406\n",
      "Epoch: 4132, Training Loss: 0.073900, Validation Loss: 0.060222\n",
      "Epoch: 4133, Training Loss: 0.072994, Validation Loss: 0.061325\n",
      "Epoch: 4134, Training Loss: 0.072546, Validation Loss: 0.060946\n",
      "Epoch: 4135, Training Loss: 0.072429, Validation Loss: 0.058861\n",
      "Epoch: 4136, Training Loss: 0.072198, Validation Loss: 0.063230\n",
      "Epoch: 4137, Training Loss: 0.071738, Validation Loss: 0.059568\n",
      "Epoch: 4138, Training Loss: 0.071663, Validation Loss: 0.061369\n",
      "Epoch: 4139, Training Loss: 0.072091, Validation Loss: 0.061182\n",
      "Epoch: 4140, Training Loss: 0.072489, Validation Loss: 0.059748\n",
      "Epoch: 4141, Training Loss: 0.072092, Validation Loss: 0.061907\n",
      "Epoch: 4142, Training Loss: 0.071362, Validation Loss: 0.058794\n",
      "Epoch: 4143, Training Loss: 0.070923, Validation Loss: 0.059909\n",
      "Epoch: 4144, Training Loss: 0.071294, Validation Loss: 0.060535\n",
      "Epoch: 4145, Training Loss: 0.071912, Validation Loss: 0.059853\n",
      "Epoch: 4146, Training Loss: 0.071989, Validation Loss: 0.062381\n",
      "Epoch: 4147, Training Loss: 0.071538, Validation Loss: 0.059666\n",
      "Epoch: 4148, Training Loss: 0.071088, Validation Loss: 0.061050\n",
      "Epoch: 4149, Training Loss: 0.071148, Validation Loss: 0.059616\n",
      "Epoch: 4150, Training Loss: 0.071592, Validation Loss: 0.060396\n",
      "Epoch: 4151, Training Loss: 0.071696, Validation Loss: 0.060709\n",
      "Epoch: 4152, Training Loss: 0.071413, Validation Loss: 0.060778\n",
      "Epoch: 4153, Training Loss: 0.071071, Validation Loss: 0.059794\n",
      "Epoch: 4154, Training Loss: 0.071083, Validation Loss: 0.060889\n",
      "Epoch: 4155, Training Loss: 0.071414, Validation Loss: 0.059058\n",
      "Epoch: 4156, Training Loss: 0.071614, Validation Loss: 0.062067\n",
      "Epoch: 4157, Training Loss: 0.071580, Validation Loss: 0.059158\n",
      "Epoch: 4158, Training Loss: 0.071428, Validation Loss: 0.061811\n",
      "Epoch: 4159, Training Loss: 0.071674, Validation Loss: 0.059307\n",
      "Epoch: 4160, Training Loss: 0.072200, Validation Loss: 0.062804\n",
      "Epoch: 4161, Training Loss: 0.072749, Validation Loss: 0.060628\n",
      "Epoch: 4162, Training Loss: 0.073412, Validation Loss: 0.065003\n",
      "Epoch: 4163, Training Loss: 0.074214, Validation Loss: 0.060757\n",
      "Epoch: 4164, Training Loss: 0.075510, Validation Loss: 0.067662\n",
      "Epoch: 4165, Training Loss: 0.076784, Validation Loss: 0.061012\n",
      "Epoch: 4166, Training Loss: 0.077867, Validation Loss: 0.070569\n",
      "Epoch: 4167, Training Loss: 0.077591, Validation Loss: 0.061266\n",
      "Epoch: 4168, Training Loss: 0.076426, Validation Loss: 0.068650\n",
      "Epoch: 4169, Training Loss: 0.074262, Validation Loss: 0.060035\n",
      "Epoch: 4170, Training Loss: 0.072710, Validation Loss: 0.062812\n",
      "Epoch: 4171, Training Loss: 0.071724, Validation Loss: 0.061058\n",
      "Epoch: 4172, Training Loss: 0.071461, Validation Loss: 0.059921\n",
      "Epoch: 4173, Training Loss: 0.071736, Validation Loss: 0.063078\n",
      "Epoch: 4174, Training Loss: 0.072431, Validation Loss: 0.059568\n",
      "Epoch: 4175, Training Loss: 0.073194, Validation Loss: 0.064065\n",
      "Epoch: 4176, Training Loss: 0.073294, Validation Loss: 0.060005\n",
      "Epoch: 4177, Training Loss: 0.072964, Validation Loss: 0.062612\n",
      "Epoch: 4178, Training Loss: 0.072029, Validation Loss: 0.060553\n",
      "Epoch: 4179, Training Loss: 0.071221, Validation Loss: 0.060618\n",
      "Epoch: 4180, Training Loss: 0.070833, Validation Loss: 0.060769\n",
      "Epoch: 4181, Training Loss: 0.071040, Validation Loss: 0.059982\n",
      "Epoch: 4182, Training Loss: 0.071599, Validation Loss: 0.061176\n",
      "Epoch: 4183, Training Loss: 0.072001, Validation Loss: 0.059837\n",
      "Epoch: 4184, Training Loss: 0.072127, Validation Loss: 0.061924\n",
      "Epoch: 4185, Training Loss: 0.071904, Validation Loss: 0.059365\n",
      "Epoch: 4186, Training Loss: 0.071832, Validation Loss: 0.062563\n",
      "Epoch: 4187, Training Loss: 0.071982, Validation Loss: 0.059209\n",
      "Epoch: 4188, Training Loss: 0.072154, Validation Loss: 0.063110\n",
      "Epoch: 4189, Training Loss: 0.072108, Validation Loss: 0.058880\n",
      "Epoch: 4190, Training Loss: 0.071896, Validation Loss: 0.062656\n",
      "Epoch: 4191, Training Loss: 0.071717, Validation Loss: 0.058836\n",
      "Epoch: 4192, Training Loss: 0.071688, Validation Loss: 0.062152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4193, Training Loss: 0.071620, Validation Loss: 0.060067\n",
      "Epoch: 4194, Training Loss: 0.071591, Validation Loss: 0.062202\n",
      "Epoch: 4195, Training Loss: 0.071370, Validation Loss: 0.060528\n",
      "Epoch: 4196, Training Loss: 0.071141, Validation Loss: 0.061468\n",
      "Epoch: 4197, Training Loss: 0.070940, Validation Loss: 0.059525\n",
      "Epoch: 4198, Training Loss: 0.070880, Validation Loss: 0.060665\n",
      "Epoch: 4199, Training Loss: 0.070882, Validation Loss: 0.059139\n",
      "Epoch: 4200, Training Loss: 0.070872, Validation Loss: 0.060594\n",
      "Epoch: 4201, Training Loss: 0.070866, Validation Loss: 0.059465\n",
      "Epoch: 4202, Training Loss: 0.070907, Validation Loss: 0.061037\n",
      "Epoch: 4203, Training Loss: 0.071077, Validation Loss: 0.059518\n",
      "Epoch: 4204, Training Loss: 0.071484, Validation Loss: 0.062273\n",
      "Epoch: 4205, Training Loss: 0.072224, Validation Loss: 0.059324\n",
      "Epoch: 4206, Training Loss: 0.073487, Validation Loss: 0.065073\n",
      "Epoch: 4207, Training Loss: 0.075175, Validation Loss: 0.060703\n",
      "Epoch: 4208, Training Loss: 0.077526, Validation Loss: 0.070499\n",
      "Epoch: 4209, Training Loss: 0.079187, Validation Loss: 0.063140\n",
      "Epoch: 4210, Training Loss: 0.081090, Validation Loss: 0.074803\n",
      "Epoch: 4211, Training Loss: 0.080278, Validation Loss: 0.063304\n",
      "Epoch: 4212, Training Loss: 0.078697, Validation Loss: 0.071349\n",
      "Epoch: 4213, Training Loss: 0.075335, Validation Loss: 0.060259\n",
      "Epoch: 4214, Training Loss: 0.072730, Validation Loss: 0.062933\n",
      "Epoch: 4215, Training Loss: 0.071295, Validation Loss: 0.060572\n",
      "Epoch: 4216, Training Loss: 0.071371, Validation Loss: 0.059848\n",
      "Epoch: 4217, Training Loss: 0.072224, Validation Loss: 0.064131\n",
      "Epoch: 4218, Training Loss: 0.073118, Validation Loss: 0.060061\n",
      "Epoch: 4219, Training Loss: 0.073589, Validation Loss: 0.064554\n",
      "Epoch: 4220, Training Loss: 0.073024, Validation Loss: 0.059753\n",
      "Epoch: 4221, Training Loss: 0.072252, Validation Loss: 0.061393\n",
      "Epoch: 4222, Training Loss: 0.071561, Validation Loss: 0.061124\n",
      "Epoch: 4223, Training Loss: 0.071255, Validation Loss: 0.059838\n",
      "Epoch: 4224, Training Loss: 0.071245, Validation Loss: 0.061917\n",
      "Epoch: 4225, Training Loss: 0.071553, Validation Loss: 0.059453\n",
      "Epoch: 4226, Training Loss: 0.071914, Validation Loss: 0.061965\n",
      "Epoch: 4227, Training Loss: 0.071998, Validation Loss: 0.060146\n",
      "Epoch: 4228, Training Loss: 0.071776, Validation Loss: 0.060974\n",
      "Epoch: 4229, Training Loss: 0.071231, Validation Loss: 0.060267\n",
      "Epoch: 4230, Training Loss: 0.070811, Validation Loss: 0.059778\n",
      "Epoch: 4231, Training Loss: 0.070732, Validation Loss: 0.060445\n",
      "Epoch: 4232, Training Loss: 0.070987, Validation Loss: 0.060144\n",
      "Epoch: 4233, Training Loss: 0.071325, Validation Loss: 0.061202\n",
      "Epoch: 4234, Training Loss: 0.071452, Validation Loss: 0.060117\n",
      "Epoch: 4235, Training Loss: 0.071459, Validation Loss: 0.061496\n",
      "Epoch: 4236, Training Loss: 0.071403, Validation Loss: 0.059246\n",
      "Epoch: 4237, Training Loss: 0.071634, Validation Loss: 0.062348\n",
      "Epoch: 4238, Training Loss: 0.072164, Validation Loss: 0.059301\n",
      "Epoch: 4239, Training Loss: 0.072882, Validation Loss: 0.064491\n",
      "Epoch: 4240, Training Loss: 0.073478, Validation Loss: 0.059609\n",
      "Epoch: 4241, Training Loss: 0.073894, Validation Loss: 0.065585\n",
      "Epoch: 4242, Training Loss: 0.073965, Validation Loss: 0.059752\n",
      "Epoch: 4243, Training Loss: 0.074243, Validation Loss: 0.065782\n",
      "Epoch: 4244, Training Loss: 0.074257, Validation Loss: 0.060919\n",
      "Epoch: 4245, Training Loss: 0.074214, Validation Loss: 0.065895\n",
      "Epoch: 4246, Training Loss: 0.073695, Validation Loss: 0.060603\n",
      "Epoch: 4247, Training Loss: 0.073182, Validation Loss: 0.064757\n",
      "Epoch: 4248, Training Loss: 0.072479, Validation Loss: 0.059296\n",
      "Epoch: 4249, Training Loss: 0.071808, Validation Loss: 0.062822\n",
      "Epoch: 4250, Training Loss: 0.071228, Validation Loss: 0.058864\n",
      "Epoch: 4251, Training Loss: 0.070812, Validation Loss: 0.060584\n",
      "Epoch: 4252, Training Loss: 0.070711, Validation Loss: 0.060367\n",
      "Epoch: 4253, Training Loss: 0.070969, Validation Loss: 0.059833\n",
      "Epoch: 4254, Training Loss: 0.071436, Validation Loss: 0.062815\n",
      "Epoch: 4255, Training Loss: 0.071957, Validation Loss: 0.059558\n",
      "Epoch: 4256, Training Loss: 0.072284, Validation Loss: 0.063660\n",
      "Epoch: 4257, Training Loss: 0.072443, Validation Loss: 0.058710\n",
      "Epoch: 4258, Training Loss: 0.072303, Validation Loss: 0.063124\n",
      "Epoch: 4259, Training Loss: 0.071960, Validation Loss: 0.058739\n",
      "Epoch: 4260, Training Loss: 0.071491, Validation Loss: 0.061885\n",
      "Epoch: 4261, Training Loss: 0.071087, Validation Loss: 0.059733\n",
      "Epoch: 4262, Training Loss: 0.070824, Validation Loss: 0.060614\n",
      "Epoch: 4263, Training Loss: 0.070718, Validation Loss: 0.060686\n",
      "Epoch: 4264, Training Loss: 0.070764, Validation Loss: 0.059618\n",
      "Epoch: 4265, Training Loss: 0.070968, Validation Loss: 0.061405\n",
      "Epoch: 4266, Training Loss: 0.071292, Validation Loss: 0.059247\n",
      "Epoch: 4267, Training Loss: 0.071665, Validation Loss: 0.062392\n",
      "Epoch: 4268, Training Loss: 0.072004, Validation Loss: 0.059621\n",
      "Epoch: 4269, Training Loss: 0.072440, Validation Loss: 0.063722\n",
      "Epoch: 4270, Training Loss: 0.072934, Validation Loss: 0.059841\n",
      "Epoch: 4271, Training Loss: 0.073592, Validation Loss: 0.065409\n",
      "Epoch: 4272, Training Loss: 0.074259, Validation Loss: 0.059845\n",
      "Epoch: 4273, Training Loss: 0.074940, Validation Loss: 0.067012\n",
      "Epoch: 4274, Training Loss: 0.074969, Validation Loss: 0.060355\n",
      "Epoch: 4275, Training Loss: 0.074969, Validation Loss: 0.067451\n",
      "Epoch: 4276, Training Loss: 0.074538, Validation Loss: 0.060447\n",
      "Epoch: 4277, Training Loss: 0.073948, Validation Loss: 0.065902\n",
      "Epoch: 4278, Training Loss: 0.072872, Validation Loss: 0.059532\n",
      "Epoch: 4279, Training Loss: 0.071885, Validation Loss: 0.062130\n",
      "Epoch: 4280, Training Loss: 0.071123, Validation Loss: 0.059959\n",
      "Epoch: 4281, Training Loss: 0.070841, Validation Loss: 0.060090\n",
      "Epoch: 4282, Training Loss: 0.070973, Validation Loss: 0.061937\n",
      "Epoch: 4283, Training Loss: 0.071414, Validation Loss: 0.059588\n",
      "Epoch: 4284, Training Loss: 0.071899, Validation Loss: 0.062841\n",
      "Epoch: 4285, Training Loss: 0.072279, Validation Loss: 0.059579\n",
      "Epoch: 4286, Training Loss: 0.072389, Validation Loss: 0.063161\n",
      "Epoch: 4287, Training Loss: 0.072098, Validation Loss: 0.059837\n",
      "Epoch: 4288, Training Loss: 0.071648, Validation Loss: 0.062299\n",
      "Epoch: 4289, Training Loss: 0.071089, Validation Loss: 0.059781\n",
      "Epoch: 4290, Training Loss: 0.070738, Validation Loss: 0.060685\n",
      "Epoch: 4291, Training Loss: 0.070605, Validation Loss: 0.059777\n",
      "Epoch: 4292, Training Loss: 0.070668, Validation Loss: 0.060156\n",
      "Epoch: 4293, Training Loss: 0.070809, Validation Loss: 0.060460\n",
      "Epoch: 4294, Training Loss: 0.070911, Validation Loss: 0.059840\n",
      "Epoch: 4295, Training Loss: 0.070990, Validation Loss: 0.060973\n",
      "Epoch: 4296, Training Loss: 0.071098, Validation Loss: 0.059106\n",
      "Epoch: 4297, Training Loss: 0.071343, Validation Loss: 0.062076\n",
      "Epoch: 4298, Training Loss: 0.071794, Validation Loss: 0.059486\n",
      "Epoch: 4299, Training Loss: 0.072496, Validation Loss: 0.064348\n",
      "Epoch: 4300, Training Loss: 0.073391, Validation Loss: 0.059802\n",
      "Epoch: 4301, Training Loss: 0.074449, Validation Loss: 0.066561\n",
      "Epoch: 4302, Training Loss: 0.075470, Validation Loss: 0.060191\n",
      "Epoch: 4303, Training Loss: 0.076352, Validation Loss: 0.068897\n",
      "Epoch: 4304, Training Loss: 0.076610, Validation Loss: 0.061310\n",
      "Epoch: 4305, Training Loss: 0.076766, Validation Loss: 0.069247\n",
      "Epoch: 4306, Training Loss: 0.075829, Validation Loss: 0.061891\n",
      "Epoch: 4307, Training Loss: 0.074824, Validation Loss: 0.066724\n",
      "Epoch: 4308, Training Loss: 0.073105, Validation Loss: 0.060629\n",
      "Epoch: 4309, Training Loss: 0.071718, Validation Loss: 0.062277\n",
      "Epoch: 4310, Training Loss: 0.070792, Validation Loss: 0.059648\n",
      "Epoch: 4311, Training Loss: 0.070639, Validation Loss: 0.059635\n",
      "Epoch: 4312, Training Loss: 0.071115, Validation Loss: 0.061472\n",
      "Epoch: 4313, Training Loss: 0.071792, Validation Loss: 0.059983\n",
      "Epoch: 4314, Training Loss: 0.072376, Validation Loss: 0.063448\n",
      "Epoch: 4315, Training Loss: 0.072392, Validation Loss: 0.059988\n",
      "Epoch: 4316, Training Loss: 0.072121, Validation Loss: 0.062693\n",
      "Epoch: 4317, Training Loss: 0.071528, Validation Loss: 0.059366\n",
      "Epoch: 4318, Training Loss: 0.071020, Validation Loss: 0.061387\n",
      "Epoch: 4319, Training Loss: 0.070712, Validation Loss: 0.059686\n",
      "Epoch: 4320, Training Loss: 0.070614, Validation Loss: 0.060454\n",
      "Epoch: 4321, Training Loss: 0.070713, Validation Loss: 0.060174\n",
      "Epoch: 4322, Training Loss: 0.070909, Validation Loss: 0.059472\n",
      "Epoch: 4323, Training Loss: 0.071158, Validation Loss: 0.061534\n",
      "Epoch: 4324, Training Loss: 0.071400, Validation Loss: 0.059496\n",
      "Epoch: 4325, Training Loss: 0.071591, Validation Loss: 0.062813\n",
      "Epoch: 4326, Training Loss: 0.071645, Validation Loss: 0.059371\n",
      "Epoch: 4327, Training Loss: 0.071644, Validation Loss: 0.062714\n",
      "Epoch: 4328, Training Loss: 0.071672, Validation Loss: 0.059202\n",
      "Epoch: 4329, Training Loss: 0.071652, Validation Loss: 0.063011\n",
      "Epoch: 4330, Training Loss: 0.071654, Validation Loss: 0.059167\n",
      "Epoch: 4331, Training Loss: 0.071560, Validation Loss: 0.062657\n",
      "Epoch: 4332, Training Loss: 0.071418, Validation Loss: 0.059221\n",
      "Epoch: 4333, Training Loss: 0.071294, Validation Loss: 0.062154\n",
      "Epoch: 4334, Training Loss: 0.071231, Validation Loss: 0.059695\n",
      "Epoch: 4335, Training Loss: 0.071126, Validation Loss: 0.062001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4336, Training Loss: 0.071031, Validation Loss: 0.059561\n",
      "Epoch: 4337, Training Loss: 0.070894, Validation Loss: 0.061194\n",
      "Epoch: 4338, Training Loss: 0.070749, Validation Loss: 0.059455\n",
      "Epoch: 4339, Training Loss: 0.070651, Validation Loss: 0.061069\n",
      "Epoch: 4340, Training Loss: 0.070661, Validation Loss: 0.059642\n",
      "Epoch: 4341, Training Loss: 0.070751, Validation Loss: 0.061482\n",
      "Epoch: 4342, Training Loss: 0.070905, Validation Loss: 0.059361\n",
      "Epoch: 4343, Training Loss: 0.071214, Validation Loss: 0.062082\n",
      "Epoch: 4344, Training Loss: 0.071959, Validation Loss: 0.059011\n",
      "Epoch: 4345, Training Loss: 0.073374, Validation Loss: 0.065267\n",
      "Epoch: 4346, Training Loss: 0.075142, Validation Loss: 0.060682\n",
      "Epoch: 4347, Training Loss: 0.077870, Validation Loss: 0.071177\n",
      "Epoch: 4348, Training Loss: 0.079925, Validation Loss: 0.063729\n",
      "Epoch: 4349, Training Loss: 0.082900, Validation Loss: 0.076830\n",
      "Epoch: 4350, Training Loss: 0.081646, Validation Loss: 0.064988\n",
      "Epoch: 4351, Training Loss: 0.080749, Validation Loss: 0.073335\n",
      "Epoch: 4352, Training Loss: 0.076946, Validation Loss: 0.063020\n",
      "Epoch: 4353, Training Loss: 0.074447, Validation Loss: 0.064762\n",
      "Epoch: 4354, Training Loss: 0.072186, Validation Loss: 0.061485\n",
      "Epoch: 4355, Training Loss: 0.071305, Validation Loss: 0.059768\n",
      "Epoch: 4356, Training Loss: 0.071733, Validation Loss: 0.062925\n",
      "Epoch: 4357, Training Loss: 0.072899, Validation Loss: 0.059545\n",
      "Epoch: 4358, Training Loss: 0.073988, Validation Loss: 0.065216\n",
      "Epoch: 4359, Training Loss: 0.073982, Validation Loss: 0.060606\n",
      "Epoch: 4360, Training Loss: 0.073402, Validation Loss: 0.063592\n",
      "Epoch: 4361, Training Loss: 0.072214, Validation Loss: 0.061435\n",
      "Epoch: 4362, Training Loss: 0.071379, Validation Loss: 0.060856\n",
      "Epoch: 4363, Training Loss: 0.071082, Validation Loss: 0.062231\n",
      "Epoch: 4364, Training Loss: 0.071340, Validation Loss: 0.059626\n",
      "Epoch: 4365, Training Loss: 0.071870, Validation Loss: 0.062751\n",
      "Epoch: 4366, Training Loss: 0.072311, Validation Loss: 0.059983\n",
      "Epoch: 4367, Training Loss: 0.072427, Validation Loss: 0.062433\n",
      "Epoch: 4368, Training Loss: 0.071909, Validation Loss: 0.060388\n",
      "Epoch: 4369, Training Loss: 0.071235, Validation Loss: 0.061208\n",
      "Epoch: 4370, Training Loss: 0.070666, Validation Loss: 0.060006\n",
      "Epoch: 4371, Training Loss: 0.070520, Validation Loss: 0.060259\n",
      "Epoch: 4372, Training Loss: 0.070727, Validation Loss: 0.060009\n",
      "Epoch: 4373, Training Loss: 0.071001, Validation Loss: 0.060084\n",
      "Epoch: 4374, Training Loss: 0.071144, Validation Loss: 0.060850\n",
      "Epoch: 4375, Training Loss: 0.071089, Validation Loss: 0.059632\n",
      "Epoch: 4376, Training Loss: 0.071200, Validation Loss: 0.061979\n",
      "Epoch: 4377, Training Loss: 0.071718, Validation Loss: 0.059740\n",
      "Epoch: 4378, Training Loss: 0.072772, Validation Loss: 0.064813\n",
      "Epoch: 4379, Training Loss: 0.073742, Validation Loss: 0.060174\n",
      "Epoch: 4380, Training Loss: 0.074715, Validation Loss: 0.067107\n",
      "Epoch: 4381, Training Loss: 0.074776, Validation Loss: 0.059651\n",
      "Epoch: 4382, Training Loss: 0.074561, Validation Loss: 0.066297\n",
      "Epoch: 4383, Training Loss: 0.074119, Validation Loss: 0.059950\n",
      "Epoch: 4384, Training Loss: 0.073734, Validation Loss: 0.065198\n",
      "Epoch: 4385, Training Loss: 0.072793, Validation Loss: 0.061049\n",
      "Epoch: 4386, Training Loss: 0.071842, Validation Loss: 0.062729\n",
      "Epoch: 4387, Training Loss: 0.070935, Validation Loss: 0.060046\n",
      "Epoch: 4388, Training Loss: 0.070512, Validation Loss: 0.060344\n",
      "Epoch: 4389, Training Loss: 0.070598, Validation Loss: 0.059975\n",
      "Epoch: 4390, Training Loss: 0.070941, Validation Loss: 0.060553\n",
      "Epoch: 4391, Training Loss: 0.071227, Validation Loss: 0.061348\n",
      "Epoch: 4392, Training Loss: 0.071194, Validation Loss: 0.060496\n",
      "Epoch: 4393, Training Loss: 0.071054, Validation Loss: 0.061700\n",
      "Epoch: 4394, Training Loss: 0.070983, Validation Loss: 0.059443\n",
      "Epoch: 4395, Training Loss: 0.071195, Validation Loss: 0.061951\n",
      "Epoch: 4396, Training Loss: 0.071577, Validation Loss: 0.058702\n",
      "Epoch: 4397, Training Loss: 0.072027, Validation Loss: 0.062966\n",
      "Epoch: 4398, Training Loss: 0.072107, Validation Loss: 0.058731\n",
      "Epoch: 4399, Training Loss: 0.072071, Validation Loss: 0.063253\n",
      "Epoch: 4400, Training Loss: 0.072071, Validation Loss: 0.059283\n",
      "Epoch: 4401, Training Loss: 0.072325, Validation Loss: 0.063775\n",
      "Epoch: 4402, Training Loss: 0.072576, Validation Loss: 0.060103\n",
      "Epoch: 4403, Training Loss: 0.072868, Validation Loss: 0.064484\n",
      "Epoch: 4404, Training Loss: 0.072849, Validation Loss: 0.060179\n",
      "Epoch: 4405, Training Loss: 0.072686, Validation Loss: 0.064326\n",
      "Epoch: 4406, Training Loss: 0.072344, Validation Loss: 0.059694\n",
      "Epoch: 4407, Training Loss: 0.072042, Validation Loss: 0.063750\n",
      "Epoch: 4408, Training Loss: 0.071727, Validation Loss: 0.059421\n",
      "Epoch: 4409, Training Loss: 0.071316, Validation Loss: 0.062627\n",
      "Epoch: 4410, Training Loss: 0.070847, Validation Loss: 0.059352\n",
      "Epoch: 4411, Training Loss: 0.070530, Validation Loss: 0.060788\n",
      "Epoch: 4412, Training Loss: 0.070426, Validation Loss: 0.060293\n",
      "Epoch: 4413, Training Loss: 0.070474, Validation Loss: 0.060051\n",
      "Epoch: 4414, Training Loss: 0.070621, Validation Loss: 0.061426\n",
      "Epoch: 4415, Training Loss: 0.070870, Validation Loss: 0.059354\n",
      "Epoch: 4416, Training Loss: 0.071313, Validation Loss: 0.062233\n",
      "Epoch: 4417, Training Loss: 0.071975, Validation Loss: 0.058876\n",
      "Epoch: 4418, Training Loss: 0.072795, Validation Loss: 0.064277\n",
      "Epoch: 4419, Training Loss: 0.073638, Validation Loss: 0.059498\n",
      "Epoch: 4420, Training Loss: 0.074588, Validation Loss: 0.066652\n",
      "Epoch: 4421, Training Loss: 0.074795, Validation Loss: 0.060364\n",
      "Epoch: 4422, Training Loss: 0.074905, Validation Loss: 0.066959\n",
      "Epoch: 4423, Training Loss: 0.074396, Validation Loss: 0.060330\n",
      "Epoch: 4424, Training Loss: 0.073756, Validation Loss: 0.065743\n",
      "Epoch: 4425, Training Loss: 0.072689, Validation Loss: 0.059747\n",
      "Epoch: 4426, Training Loss: 0.071611, Validation Loss: 0.062774\n",
      "Epoch: 4427, Training Loss: 0.070752, Validation Loss: 0.059749\n",
      "Epoch: 4428, Training Loss: 0.070468, Validation Loss: 0.060090\n",
      "Epoch: 4429, Training Loss: 0.070694, Validation Loss: 0.061377\n",
      "Epoch: 4430, Training Loss: 0.071088, Validation Loss: 0.059682\n",
      "Epoch: 4431, Training Loss: 0.071484, Validation Loss: 0.062841\n",
      "Epoch: 4432, Training Loss: 0.071575, Validation Loss: 0.059758\n",
      "Epoch: 4433, Training Loss: 0.071477, Validation Loss: 0.062399\n",
      "Epoch: 4434, Training Loss: 0.071163, Validation Loss: 0.059591\n",
      "Epoch: 4435, Training Loss: 0.070819, Validation Loss: 0.061154\n",
      "Epoch: 4436, Training Loss: 0.070548, Validation Loss: 0.059694\n",
      "Epoch: 4437, Training Loss: 0.070402, Validation Loss: 0.060316\n",
      "Epoch: 4438, Training Loss: 0.070405, Validation Loss: 0.059572\n",
      "Epoch: 4439, Training Loss: 0.070465, Validation Loss: 0.059880\n",
      "Epoch: 4440, Training Loss: 0.070508, Validation Loss: 0.060078\n",
      "Epoch: 4441, Training Loss: 0.070506, Validation Loss: 0.060110\n",
      "Epoch: 4442, Training Loss: 0.070480, Validation Loss: 0.060608\n",
      "Epoch: 4443, Training Loss: 0.070459, Validation Loss: 0.059842\n",
      "Epoch: 4444, Training Loss: 0.070477, Validation Loss: 0.060797\n",
      "Epoch: 4445, Training Loss: 0.070423, Validation Loss: 0.059961\n",
      "Epoch: 4446, Training Loss: 0.073705, Validation Loss: 0.060806\n",
      "Epoch: 4447, Training Loss: 0.086346, Validation Loss: 0.081847\n",
      "Epoch: 4448, Training Loss: 0.098879, Validation Loss: 0.079458\n",
      "Epoch: 4449, Training Loss: 0.113548, Validation Loss: 0.113250\n",
      "Epoch: 4450, Training Loss: 0.094343, Validation Loss: 0.073411\n",
      "Epoch: 4451, Training Loss: 0.083236, Validation Loss: 0.071226\n",
      "Epoch: 4452, Training Loss: 0.083577, Validation Loss: 0.072888\n",
      "Epoch: 4453, Training Loss: 0.082574, Validation Loss: 0.063566\n",
      "Epoch: 4454, Training Loss: 0.074741, Validation Loss: 0.063803\n",
      "Epoch: 4455, Training Loss: 0.073526, Validation Loss: 0.061683\n",
      "Epoch: 4456, Training Loss: 0.081562, Validation Loss: 0.066157\n",
      "Epoch: 4457, Training Loss: 0.080340, Validation Loss: 0.070933\n",
      "Epoch: 4458, Training Loss: 0.075641, Validation Loss: 0.064227\n",
      "Epoch: 4459, Training Loss: 0.077782, Validation Loss: 0.059774\n",
      "Epoch: 4460, Training Loss: 0.076332, Validation Loss: 0.067132\n",
      "Epoch: 4461, Training Loss: 0.071073, Validation Loss: 0.058748\n",
      "Epoch: 4462, Training Loss: 0.075420, Validation Loss: 0.061336\n",
      "Epoch: 4463, Training Loss: 0.078602, Validation Loss: 0.073196\n",
      "Epoch: 4464, Training Loss: 0.073860, Validation Loss: 0.063162\n",
      "Epoch: 4465, Training Loss: 0.075119, Validation Loss: 0.062576\n",
      "Epoch: 4466, Training Loss: 0.075006, Validation Loss: 0.066245\n",
      "Epoch: 4467, Training Loss: 0.071511, Validation Loss: 0.057924\n",
      "Epoch: 4468, Training Loss: 0.073517, Validation Loss: 0.059677\n",
      "Epoch: 4469, Training Loss: 0.075078, Validation Loss: 0.065813\n",
      "Epoch: 4470, Training Loss: 0.072285, Validation Loss: 0.061001\n",
      "Epoch: 4471, Training Loss: 0.072236, Validation Loss: 0.059273\n",
      "Epoch: 4472, Training Loss: 0.073215, Validation Loss: 0.065113\n",
      "Epoch: 4473, Training Loss: 0.071306, Validation Loss: 0.058919\n",
      "Epoch: 4474, Training Loss: 0.071788, Validation Loss: 0.058058\n",
      "Epoch: 4475, Training Loss: 0.073093, Validation Loss: 0.064072\n",
      "Epoch: 4476, Training Loss: 0.071534, Validation Loss: 0.058462\n",
      "Epoch: 4477, Training Loss: 0.071194, Validation Loss: 0.059509\n",
      "Epoch: 4478, Training Loss: 0.071831, Validation Loss: 0.062900\n",
      "Epoch: 4479, Training Loss: 0.071119, Validation Loss: 0.058986\n",
      "Epoch: 4480, Training Loss: 0.071123, Validation Loss: 0.060176\n",
      "Epoch: 4481, Training Loss: 0.071803, Validation Loss: 0.061596\n",
      "Epoch: 4482, Training Loss: 0.071146, Validation Loss: 0.059517\n",
      "Epoch: 4483, Training Loss: 0.070602, Validation Loss: 0.058849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4484, Training Loss: 0.071201, Validation Loss: 0.061103\n",
      "Epoch: 4485, Training Loss: 0.071099, Validation Loss: 0.058911\n",
      "Epoch: 4486, Training Loss: 0.070618, Validation Loss: 0.059392\n",
      "Epoch: 4487, Training Loss: 0.070993, Validation Loss: 0.061233\n",
      "Epoch: 4488, Training Loss: 0.071309, Validation Loss: 0.059005\n",
      "Epoch: 4489, Training Loss: 0.070850, Validation Loss: 0.061134\n",
      "Epoch: 4490, Training Loss: 0.070508, Validation Loss: 0.059279\n",
      "Epoch: 4491, Training Loss: 0.070792, Validation Loss: 0.058685\n",
      "Epoch: 4492, Training Loss: 0.070922, Validation Loss: 0.060840\n",
      "Epoch: 4493, Training Loss: 0.070638, Validation Loss: 0.059083\n",
      "Epoch: 4494, Training Loss: 0.070688, Validation Loss: 0.060499\n",
      "Epoch: 4495, Training Loss: 0.070896, Validation Loss: 0.060620\n",
      "Epoch: 4496, Training Loss: 0.070727, Validation Loss: 0.060170\n",
      "Epoch: 4497, Training Loss: 0.070431, Validation Loss: 0.059673\n",
      "Epoch: 4498, Training Loss: 0.070475, Validation Loss: 0.060311\n",
      "Epoch: 4499, Training Loss: 0.070601, Validation Loss: 0.059512\n",
      "Epoch: 4500, Training Loss: 0.070496, Validation Loss: 0.060033\n",
      "Epoch: 4501, Training Loss: 0.070408, Validation Loss: 0.060179\n",
      "Epoch: 4502, Training Loss: 0.070507, Validation Loss: 0.059494\n",
      "Epoch: 4503, Training Loss: 0.070617, Validation Loss: 0.061337\n",
      "Epoch: 4504, Training Loss: 0.070557, Validation Loss: 0.059526\n",
      "Epoch: 4505, Training Loss: 0.070425, Validation Loss: 0.060298\n",
      "Epoch: 4506, Training Loss: 0.070424, Validation Loss: 0.059540\n",
      "Epoch: 4507, Training Loss: 0.070453, Validation Loss: 0.059531\n",
      "Epoch: 4508, Training Loss: 0.070379, Validation Loss: 0.060133\n",
      "Epoch: 4509, Training Loss: 0.070306, Validation Loss: 0.059746\n",
      "Epoch: 4510, Training Loss: 0.070354, Validation Loss: 0.060026\n",
      "Epoch: 4511, Training Loss: 0.070424, Validation Loss: 0.059940\n",
      "Epoch: 4512, Training Loss: 0.070395, Validation Loss: 0.059844\n",
      "Epoch: 4513, Training Loss: 0.070316, Validation Loss: 0.059517\n",
      "Epoch: 4514, Training Loss: 0.070314, Validation Loss: 0.060176\n",
      "Epoch: 4515, Training Loss: 0.070353, Validation Loss: 0.059541\n",
      "Epoch: 4516, Training Loss: 0.070339, Validation Loss: 0.060424\n",
      "Epoch: 4517, Training Loss: 0.070269, Validation Loss: 0.059613\n",
      "Epoch: 4518, Training Loss: 0.070247, Validation Loss: 0.059781\n",
      "Epoch: 4519, Training Loss: 0.070268, Validation Loss: 0.059856\n",
      "Epoch: 4520, Training Loss: 0.070268, Validation Loss: 0.059826\n",
      "Epoch: 4521, Training Loss: 0.070248, Validation Loss: 0.060293\n",
      "Epoch: 4522, Training Loss: 0.070228, Validation Loss: 0.059998\n",
      "Epoch: 4523, Training Loss: 0.070234, Validation Loss: 0.060167\n",
      "Epoch: 4524, Training Loss: 0.070272, Validation Loss: 0.059662\n",
      "Epoch: 4525, Training Loss: 0.070340, Validation Loss: 0.060577\n",
      "Epoch: 4526, Training Loss: 0.070466, Validation Loss: 0.059464\n",
      "Epoch: 4527, Training Loss: 0.070640, Validation Loss: 0.061572\n",
      "Epoch: 4528, Training Loss: 0.070735, Validation Loss: 0.059308\n",
      "Epoch: 4529, Training Loss: 0.070949, Validation Loss: 0.062013\n",
      "Epoch: 4530, Training Loss: 0.071174, Validation Loss: 0.059260\n",
      "Epoch: 4531, Training Loss: 0.071386, Validation Loss: 0.062484\n",
      "Epoch: 4532, Training Loss: 0.071585, Validation Loss: 0.059279\n",
      "Epoch: 4533, Training Loss: 0.071848, Validation Loss: 0.063303\n",
      "Epoch: 4534, Training Loss: 0.071983, Validation Loss: 0.059462\n",
      "Epoch: 4535, Training Loss: 0.072077, Validation Loss: 0.063723\n",
      "Epoch: 4536, Training Loss: 0.071977, Validation Loss: 0.059455\n",
      "Epoch: 4537, Training Loss: 0.071666, Validation Loss: 0.063128\n",
      "Epoch: 4538, Training Loss: 0.071282, Validation Loss: 0.059241\n",
      "Epoch: 4539, Training Loss: 0.070816, Validation Loss: 0.061739\n",
      "Epoch: 4540, Training Loss: 0.070470, Validation Loss: 0.059578\n",
      "Epoch: 4541, Training Loss: 0.070275, Validation Loss: 0.060864\n",
      "Epoch: 4542, Training Loss: 0.070193, Validation Loss: 0.060218\n",
      "Epoch: 4543, Training Loss: 0.070186, Validation Loss: 0.060499\n",
      "Epoch: 4544, Training Loss: 0.070200, Validation Loss: 0.060343\n",
      "Epoch: 4545, Training Loss: 0.070223, Validation Loss: 0.059830\n",
      "Epoch: 4546, Training Loss: 0.070293, Validation Loss: 0.060684\n",
      "Epoch: 4547, Training Loss: 0.070420, Validation Loss: 0.059276\n",
      "Epoch: 4548, Training Loss: 0.070693, Validation Loss: 0.061745\n",
      "Epoch: 4549, Training Loss: 0.071165, Validation Loss: 0.059338\n",
      "Epoch: 4550, Training Loss: 0.071859, Validation Loss: 0.063722\n",
      "Epoch: 4551, Training Loss: 0.072569, Validation Loss: 0.059387\n",
      "Epoch: 4552, Training Loss: 0.073320, Validation Loss: 0.065391\n",
      "Epoch: 4553, Training Loss: 0.073915, Validation Loss: 0.059889\n",
      "Epoch: 4554, Training Loss: 0.074401, Validation Loss: 0.066644\n",
      "Epoch: 4555, Training Loss: 0.074221, Validation Loss: 0.060447\n",
      "Epoch: 4556, Training Loss: 0.073923, Validation Loss: 0.066534\n",
      "Epoch: 4557, Training Loss: 0.073041, Validation Loss: 0.060009\n",
      "Epoch: 4558, Training Loss: 0.072054, Validation Loss: 0.064011\n",
      "Epoch: 4559, Training Loss: 0.070979, Validation Loss: 0.059157\n",
      "Epoch: 4560, Training Loss: 0.070304, Validation Loss: 0.060541\n",
      "Epoch: 4561, Training Loss: 0.070257, Validation Loss: 0.060715\n",
      "Epoch: 4562, Training Loss: 0.070707, Validation Loss: 0.059731\n",
      "Epoch: 4563, Training Loss: 0.071186, Validation Loss: 0.062945\n",
      "Epoch: 4564, Training Loss: 0.071406, Validation Loss: 0.059535\n",
      "Epoch: 4565, Training Loss: 0.071396, Validation Loss: 0.062948\n",
      "Epoch: 4566, Training Loss: 0.071144, Validation Loss: 0.059245\n",
      "Epoch: 4567, Training Loss: 0.070732, Validation Loss: 0.061481\n",
      "Epoch: 4568, Training Loss: 0.070349, Validation Loss: 0.059491\n",
      "Epoch: 4569, Training Loss: 0.070161, Validation Loss: 0.060012\n",
      "Epoch: 4570, Training Loss: 0.070284, Validation Loss: 0.061019\n",
      "Epoch: 4571, Training Loss: 0.070705, Validation Loss: 0.059540\n",
      "Epoch: 4572, Training Loss: 0.071234, Validation Loss: 0.062804\n",
      "Epoch: 4573, Training Loss: 0.071755, Validation Loss: 0.059190\n",
      "Epoch: 4574, Training Loss: 0.072182, Validation Loss: 0.063729\n",
      "Epoch: 4575, Training Loss: 0.072625, Validation Loss: 0.059122\n",
      "Epoch: 4576, Training Loss: 0.072876, Validation Loss: 0.064916\n",
      "Epoch: 4577, Training Loss: 0.072918, Validation Loss: 0.059427\n",
      "Epoch: 4578, Training Loss: 0.072752, Validation Loss: 0.064958\n",
      "Epoch: 4579, Training Loss: 0.072422, Validation Loss: 0.059295\n",
      "Epoch: 4580, Training Loss: 0.071997, Validation Loss: 0.063846\n",
      "Epoch: 4581, Training Loss: 0.071496, Validation Loss: 0.059207\n",
      "Epoch: 4582, Training Loss: 0.070983, Validation Loss: 0.062314\n",
      "Epoch: 4583, Training Loss: 0.070542, Validation Loss: 0.059588\n",
      "Epoch: 4584, Training Loss: 0.070235, Validation Loss: 0.060478\n",
      "Epoch: 4585, Training Loss: 0.070187, Validation Loss: 0.060426\n",
      "Epoch: 4586, Training Loss: 0.070343, Validation Loss: 0.059441\n",
      "Epoch: 4587, Training Loss: 0.070571, Validation Loss: 0.061668\n",
      "Epoch: 4588, Training Loss: 0.070729, Validation Loss: 0.059562\n",
      "Epoch: 4589, Training Loss: 0.070803, Validation Loss: 0.061951\n",
      "Epoch: 4590, Training Loss: 0.070818, Validation Loss: 0.059444\n",
      "Epoch: 4591, Training Loss: 0.070740, Validation Loss: 0.061532\n",
      "Epoch: 4592, Training Loss: 0.070649, Validation Loss: 0.059186\n",
      "Epoch: 4593, Training Loss: 0.070542, Validation Loss: 0.061565\n",
      "Epoch: 4594, Training Loss: 0.070622, Validation Loss: 0.059268\n",
      "Epoch: 4595, Training Loss: 0.070627, Validation Loss: 0.061977\n",
      "Epoch: 4596, Training Loss: 0.070722, Validation Loss: 0.058986\n",
      "Epoch: 4597, Training Loss: 0.070776, Validation Loss: 0.061917\n",
      "Epoch: 4598, Training Loss: 0.070842, Validation Loss: 0.059148\n",
      "Epoch: 4599, Training Loss: 0.071009, Validation Loss: 0.062583\n",
      "Epoch: 4600, Training Loss: 0.071323, Validation Loss: 0.059546\n",
      "Epoch: 4601, Training Loss: 0.071752, Validation Loss: 0.063641\n",
      "Epoch: 4602, Training Loss: 0.072368, Validation Loss: 0.059571\n",
      "Epoch: 4603, Training Loss: 0.073427, Validation Loss: 0.066068\n",
      "Epoch: 4604, Training Loss: 0.074423, Validation Loss: 0.060640\n",
      "Epoch: 4605, Training Loss: 0.075839, Validation Loss: 0.069651\n",
      "Epoch: 4606, Training Loss: 0.076848, Validation Loss: 0.061813\n",
      "Epoch: 4607, Training Loss: 0.077948, Validation Loss: 0.071995\n",
      "Epoch: 4608, Training Loss: 0.077426, Validation Loss: 0.061506\n",
      "Epoch: 4609, Training Loss: 0.076242, Validation Loss: 0.069401\n",
      "Epoch: 4610, Training Loss: 0.073739, Validation Loss: 0.059774\n",
      "Epoch: 4611, Training Loss: 0.071693, Validation Loss: 0.062473\n",
      "Epoch: 4612, Training Loss: 0.070656, Validation Loss: 0.060627\n",
      "Epoch: 4613, Training Loss: 0.070996, Validation Loss: 0.059458\n",
      "Epoch: 4614, Training Loss: 0.071984, Validation Loss: 0.064266\n",
      "Epoch: 4615, Training Loss: 0.072747, Validation Loss: 0.059869\n",
      "Epoch: 4616, Training Loss: 0.072938, Validation Loss: 0.064860\n",
      "Epoch: 4617, Training Loss: 0.072119, Validation Loss: 0.059954\n",
      "Epoch: 4618, Training Loss: 0.071441, Validation Loss: 0.061005\n",
      "Epoch: 4619, Training Loss: 0.070980, Validation Loss: 0.061232\n",
      "Epoch: 4620, Training Loss: 0.070664, Validation Loss: 0.059356\n",
      "Epoch: 4621, Training Loss: 0.070596, Validation Loss: 0.061634\n",
      "Epoch: 4622, Training Loss: 0.070939, Validation Loss: 0.059956\n",
      "Epoch: 4623, Training Loss: 0.071339, Validation Loss: 0.061603\n",
      "Epoch: 4624, Training Loss: 0.071287, Validation Loss: 0.060530\n",
      "Epoch: 4625, Training Loss: 0.070863, Validation Loss: 0.060121\n",
      "Epoch: 4626, Training Loss: 0.070336, Validation Loss: 0.060316\n",
      "Epoch: 4627, Training Loss: 0.070191, Validation Loss: 0.059420\n",
      "Epoch: 4628, Training Loss: 0.070520, Validation Loss: 0.061015\n",
      "Epoch: 4629, Training Loss: 0.071012, Validation Loss: 0.059769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4630, Training Loss: 0.071427, Validation Loss: 0.061993\n",
      "Epoch: 4631, Training Loss: 0.071616, Validation Loss: 0.059604\n",
      "Epoch: 4632, Training Loss: 0.072130, Validation Loss: 0.064045\n",
      "Epoch: 4633, Training Loss: 0.072953, Validation Loss: 0.059929\n",
      "Epoch: 4634, Training Loss: 0.073945, Validation Loss: 0.066732\n",
      "Epoch: 4635, Training Loss: 0.073810, Validation Loss: 0.060110\n",
      "Epoch: 4636, Training Loss: 0.073420, Validation Loss: 0.065995\n",
      "Epoch: 4637, Training Loss: 0.072536, Validation Loss: 0.059458\n",
      "Epoch: 4638, Training Loss: 0.071829, Validation Loss: 0.062892\n",
      "Epoch: 4639, Training Loss: 0.071242, Validation Loss: 0.059971\n",
      "Epoch: 4640, Training Loss: 0.070712, Validation Loss: 0.061010\n",
      "Epoch: 4641, Training Loss: 0.070238, Validation Loss: 0.060180\n",
      "Epoch: 4642, Training Loss: 0.070127, Validation Loss: 0.060838\n",
      "Epoch: 4643, Training Loss: 0.070321, Validation Loss: 0.060153\n",
      "Epoch: 4644, Training Loss: 0.070542, Validation Loss: 0.060741\n",
      "Epoch: 4645, Training Loss: 0.070544, Validation Loss: 0.059501\n",
      "Epoch: 4646, Training Loss: 0.070284, Validation Loss: 0.060172\n",
      "Epoch: 4647, Training Loss: 0.070058, Validation Loss: 0.059742\n",
      "Epoch: 4648, Training Loss: 0.070057, Validation Loss: 0.060352\n",
      "Epoch: 4649, Training Loss: 0.070219, Validation Loss: 0.060520\n",
      "Epoch: 4650, Training Loss: 0.070322, Validation Loss: 0.060127\n",
      "Epoch: 4651, Training Loss: 0.070239, Validation Loss: 0.060111\n",
      "Epoch: 4652, Training Loss: 0.070082, Validation Loss: 0.060094\n",
      "Epoch: 4653, Training Loss: 0.070036, Validation Loss: 0.059660\n",
      "Epoch: 4654, Training Loss: 0.070185, Validation Loss: 0.060984\n",
      "Epoch: 4655, Training Loss: 0.070477, Validation Loss: 0.059226\n",
      "Epoch: 4656, Training Loss: 0.070930, Validation Loss: 0.062346\n",
      "Epoch: 4657, Training Loss: 0.071585, Validation Loss: 0.059123\n",
      "Epoch: 4658, Training Loss: 0.072685, Validation Loss: 0.065075\n",
      "Epoch: 4659, Training Loss: 0.074150, Validation Loss: 0.060536\n",
      "Epoch: 4660, Training Loss: 0.076872, Validation Loss: 0.070260\n",
      "Epoch: 4661, Training Loss: 0.079108, Validation Loss: 0.064114\n",
      "Epoch: 4662, Training Loss: 0.082779, Validation Loss: 0.077415\n",
      "Epoch: 4663, Training Loss: 0.081698, Validation Loss: 0.066163\n",
      "Epoch: 4664, Training Loss: 0.080761, Validation Loss: 0.075098\n",
      "Epoch: 4665, Training Loss: 0.075856, Validation Loss: 0.061529\n",
      "Epoch: 4666, Training Loss: 0.072205, Validation Loss: 0.062919\n",
      "Epoch: 4667, Training Loss: 0.070422, Validation Loss: 0.059229\n",
      "Epoch: 4668, Training Loss: 0.071125, Validation Loss: 0.058284\n",
      "Epoch: 4669, Training Loss: 0.072743, Validation Loss: 0.064121\n",
      "Epoch: 4670, Training Loss: 0.073075, Validation Loss: 0.060483\n",
      "Epoch: 4671, Training Loss: 0.072769, Validation Loss: 0.063289\n",
      "Epoch: 4672, Training Loss: 0.071890, Validation Loss: 0.061136\n",
      "Epoch: 4673, Training Loss: 0.071286, Validation Loss: 0.059293\n",
      "Epoch: 4674, Training Loss: 0.071199, Validation Loss: 0.062247\n",
      "Epoch: 4675, Training Loss: 0.071437, Validation Loss: 0.059162\n",
      "Epoch: 4676, Training Loss: 0.071703, Validation Loss: 0.063688\n",
      "Epoch: 4677, Training Loss: 0.071691, Validation Loss: 0.060495\n",
      "Epoch: 4678, Training Loss: 0.071343, Validation Loss: 0.062213\n",
      "Epoch: 4679, Training Loss: 0.070702, Validation Loss: 0.060152\n",
      "Epoch: 4680, Training Loss: 0.070188, Validation Loss: 0.059767\n",
      "Epoch: 4681, Training Loss: 0.070098, Validation Loss: 0.060102\n",
      "Epoch: 4682, Training Loss: 0.070415, Validation Loss: 0.059751\n",
      "Epoch: 4683, Training Loss: 0.070838, Validation Loss: 0.061903\n",
      "Epoch: 4684, Training Loss: 0.071079, Validation Loss: 0.060179\n",
      "Epoch: 4685, Training Loss: 0.071417, Validation Loss: 0.063346\n",
      "Epoch: 4686, Training Loss: 0.071723, Validation Loss: 0.059352\n",
      "Epoch: 4687, Training Loss: 0.071924, Validation Loss: 0.063744\n",
      "Epoch: 4688, Training Loss: 0.071898, Validation Loss: 0.058610\n",
      "Epoch: 4689, Training Loss: 0.071349, Validation Loss: 0.062479\n",
      "Epoch: 4690, Training Loss: 0.070661, Validation Loss: 0.058625\n",
      "Epoch: 4691, Training Loss: 0.070263, Validation Loss: 0.060722\n",
      "Epoch: 4692, Training Loss: 0.070149, Validation Loss: 0.060138\n",
      "Epoch: 4693, Training Loss: 0.070190, Validation Loss: 0.059663\n",
      "Epoch: 4694, Training Loss: 0.070294, Validation Loss: 0.061208\n",
      "Epoch: 4695, Training Loss: 0.070553, Validation Loss: 0.059144\n",
      "Epoch: 4696, Training Loss: 0.070951, Validation Loss: 0.062832\n",
      "Epoch: 4697, Training Loss: 0.071488, Validation Loss: 0.059945\n",
      "Epoch: 4698, Training Loss: 0.072042, Validation Loss: 0.064710\n",
      "Epoch: 4699, Training Loss: 0.072592, Validation Loss: 0.060216\n",
      "Epoch: 4700, Training Loss: 0.073330, Validation Loss: 0.066401\n",
      "Epoch: 4701, Training Loss: 0.073997, Validation Loss: 0.059827\n",
      "Epoch: 4702, Training Loss: 0.074344, Validation Loss: 0.067366\n",
      "Epoch: 4703, Training Loss: 0.074019, Validation Loss: 0.059324\n",
      "Epoch: 4704, Training Loss: 0.072958, Validation Loss: 0.065382\n",
      "Epoch: 4705, Training Loss: 0.071843, Validation Loss: 0.058957\n",
      "Epoch: 4706, Training Loss: 0.070858, Validation Loss: 0.062185\n",
      "Epoch: 4707, Training Loss: 0.070244, Validation Loss: 0.060296\n",
      "Epoch: 4708, Training Loss: 0.070250, Validation Loss: 0.060075\n",
      "Epoch: 4709, Training Loss: 0.070540, Validation Loss: 0.062188\n",
      "Epoch: 4710, Training Loss: 0.070988, Validation Loss: 0.059082\n",
      "Epoch: 4711, Training Loss: 0.071351, Validation Loss: 0.063000\n",
      "Epoch: 4712, Training Loss: 0.071459, Validation Loss: 0.059264\n",
      "Epoch: 4713, Training Loss: 0.071292, Validation Loss: 0.062678\n",
      "Epoch: 4714, Training Loss: 0.070783, Validation Loss: 0.059710\n",
      "Epoch: 4715, Training Loss: 0.070281, Validation Loss: 0.060557\n",
      "Epoch: 4716, Training Loss: 0.070002, Validation Loss: 0.059988\n",
      "Epoch: 4717, Training Loss: 0.070039, Validation Loss: 0.059573\n",
      "Epoch: 4718, Training Loss: 0.070342, Validation Loss: 0.061317\n",
      "Epoch: 4719, Training Loss: 0.070775, Validation Loss: 0.059681\n",
      "Epoch: 4720, Training Loss: 0.071367, Validation Loss: 0.063103\n",
      "Epoch: 4721, Training Loss: 0.071915, Validation Loss: 0.059428\n",
      "Epoch: 4722, Training Loss: 0.072539, Validation Loss: 0.064631\n",
      "Epoch: 4723, Training Loss: 0.072761, Validation Loss: 0.059275\n",
      "Epoch: 4724, Training Loss: 0.072740, Validation Loss: 0.064851\n",
      "Epoch: 4725, Training Loss: 0.071842, Validation Loss: 0.059049\n",
      "Epoch: 4726, Training Loss: 0.071229, Validation Loss: 0.062520\n",
      "Epoch: 4727, Training Loss: 0.070588, Validation Loss: 0.059559\n",
      "Epoch: 4728, Training Loss: 0.070185, Validation Loss: 0.060198\n",
      "Epoch: 4729, Training Loss: 0.070125, Validation Loss: 0.060500\n",
      "Epoch: 4730, Training Loss: 0.070352, Validation Loss: 0.058856\n",
      "Epoch: 4731, Training Loss: 0.071044, Validation Loss: 0.062735\n",
      "Epoch: 4732, Training Loss: 0.071909, Validation Loss: 0.059925\n",
      "Epoch: 4733, Training Loss: 0.073222, Validation Loss: 0.066366\n",
      "Epoch: 4734, Training Loss: 0.074000, Validation Loss: 0.061233\n",
      "Epoch: 4735, Training Loss: 0.074794, Validation Loss: 0.068172\n",
      "Epoch: 4736, Training Loss: 0.073945, Validation Loss: 0.060161\n",
      "Epoch: 4737, Training Loss: 0.072842, Validation Loss: 0.064707\n",
      "Epoch: 4738, Training Loss: 0.071210, Validation Loss: 0.059041\n",
      "Epoch: 4739, Training Loss: 0.070301, Validation Loss: 0.060585\n",
      "Epoch: 4740, Training Loss: 0.070051, Validation Loss: 0.060518\n",
      "Epoch: 4741, Training Loss: 0.070503, Validation Loss: 0.059395\n",
      "Epoch: 4742, Training Loss: 0.071397, Validation Loss: 0.063371\n",
      "Epoch: 4743, Training Loss: 0.072181, Validation Loss: 0.059375\n",
      "Epoch: 4744, Training Loss: 0.072567, Validation Loss: 0.064662\n",
      "Epoch: 4745, Training Loss: 0.072352, Validation Loss: 0.059497\n",
      "Epoch: 4746, Training Loss: 0.071809, Validation Loss: 0.063884\n",
      "Epoch: 4747, Training Loss: 0.071040, Validation Loss: 0.059389\n",
      "Epoch: 4748, Training Loss: 0.070389, Validation Loss: 0.061556\n",
      "Epoch: 4749, Training Loss: 0.070030, Validation Loss: 0.059196\n",
      "Epoch: 4750, Training Loss: 0.069958, Validation Loss: 0.059475\n",
      "Epoch: 4751, Training Loss: 0.070127, Validation Loss: 0.060356\n",
      "Epoch: 4752, Training Loss: 0.070441, Validation Loss: 0.059246\n",
      "Epoch: 4753, Training Loss: 0.070821, Validation Loss: 0.062847\n",
      "Epoch: 4754, Training Loss: 0.070868, Validation Loss: 0.060104\n",
      "Epoch: 4755, Training Loss: 0.070925, Validation Loss: 0.063309\n",
      "Epoch: 4756, Training Loss: 0.070814, Validation Loss: 0.059622\n",
      "Epoch: 4757, Training Loss: 0.070687, Validation Loss: 0.062142\n",
      "Epoch: 4758, Training Loss: 0.070530, Validation Loss: 0.058665\n",
      "Epoch: 4759, Training Loss: 0.070374, Validation Loss: 0.061251\n",
      "Epoch: 4760, Training Loss: 0.070093, Validation Loss: 0.059317\n",
      "Epoch: 4761, Training Loss: 0.069931, Validation Loss: 0.060603\n",
      "Epoch: 4762, Training Loss: 0.069887, Validation Loss: 0.060337\n",
      "Epoch: 4763, Training Loss: 0.069880, Validation Loss: 0.060075\n",
      "Epoch: 4764, Training Loss: 0.069963, Validation Loss: 0.060898\n",
      "Epoch: 4765, Training Loss: 0.070235, Validation Loss: 0.058980\n",
      "Epoch: 4766, Training Loss: 0.070507, Validation Loss: 0.061677\n",
      "Epoch: 4767, Training Loss: 0.071062, Validation Loss: 0.058806\n",
      "Epoch: 4768, Training Loss: 0.071841, Validation Loss: 0.064064\n",
      "Epoch: 4769, Training Loss: 0.072874, Validation Loss: 0.060314\n",
      "Epoch: 4770, Training Loss: 0.074631, Validation Loss: 0.068406\n",
      "Epoch: 4771, Training Loss: 0.076246, Validation Loss: 0.061938\n",
      "Epoch: 4772, Training Loss: 0.078485, Validation Loss: 0.072974\n",
      "Epoch: 4773, Training Loss: 0.078004, Validation Loss: 0.062313\n",
      "Epoch: 4774, Training Loss: 0.077207, Validation Loss: 0.070840\n",
      "Epoch: 4775, Training Loss: 0.074276, Validation Loss: 0.060469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4776, Training Loss: 0.072098, Validation Loss: 0.063689\n",
      "Epoch: 4777, Training Loss: 0.070686, Validation Loss: 0.060454\n",
      "Epoch: 4778, Training Loss: 0.070753, Validation Loss: 0.058995\n",
      "Epoch: 4779, Training Loss: 0.072052, Validation Loss: 0.063969\n",
      "Epoch: 4780, Training Loss: 0.072666, Validation Loss: 0.059554\n",
      "Epoch: 4781, Training Loss: 0.073090, Validation Loss: 0.065449\n",
      "Epoch: 4782, Training Loss: 0.072179, Validation Loss: 0.060626\n",
      "Epoch: 4783, Training Loss: 0.071508, Validation Loss: 0.061681\n",
      "Epoch: 4784, Training Loss: 0.071160, Validation Loss: 0.061134\n",
      "Epoch: 4785, Training Loss: 0.071032, Validation Loss: 0.058214\n",
      "Epoch: 4786, Training Loss: 0.071088, Validation Loss: 0.062133\n",
      "Epoch: 4787, Training Loss: 0.071060, Validation Loss: 0.059133\n",
      "Epoch: 4788, Training Loss: 0.071485, Validation Loss: 0.062963\n",
      "Epoch: 4789, Training Loss: 0.071513, Validation Loss: 0.061523\n",
      "Epoch: 4790, Training Loss: 0.071266, Validation Loss: 0.061782\n",
      "Epoch: 4791, Training Loss: 0.070564, Validation Loss: 0.060832\n",
      "Epoch: 4792, Training Loss: 0.070109, Validation Loss: 0.058898\n",
      "Epoch: 4793, Training Loss: 0.070137, Validation Loss: 0.060414\n",
      "Epoch: 4794, Training Loss: 0.070647, Validation Loss: 0.059793\n",
      "Epoch: 4795, Training Loss: 0.071271, Validation Loss: 0.062931\n",
      "Epoch: 4796, Training Loss: 0.071556, Validation Loss: 0.061019\n",
      "Epoch: 4797, Training Loss: 0.071622, Validation Loss: 0.063656\n",
      "Epoch: 4798, Training Loss: 0.071615, Validation Loss: 0.059241\n",
      "Epoch: 4799, Training Loss: 0.071714, Validation Loss: 0.063532\n",
      "Epoch: 4800, Training Loss: 0.071592, Validation Loss: 0.058689\n",
      "Epoch: 4801, Training Loss: 0.071231, Validation Loss: 0.063264\n",
      "Epoch: 4802, Training Loss: 0.070757, Validation Loss: 0.059517\n",
      "Epoch: 4803, Training Loss: 0.070395, Validation Loss: 0.062163\n",
      "Epoch: 4804, Training Loss: 0.070120, Validation Loss: 0.059800\n",
      "Epoch: 4805, Training Loss: 0.069993, Validation Loss: 0.060253\n",
      "Epoch: 4806, Training Loss: 0.069965, Validation Loss: 0.059871\n",
      "Epoch: 4807, Training Loss: 0.069926, Validation Loss: 0.059404\n",
      "Epoch: 4808, Training Loss: 0.069954, Validation Loss: 0.060883\n",
      "Epoch: 4809, Training Loss: 0.070004, Validation Loss: 0.059949\n",
      "Epoch: 4810, Training Loss: 0.070117, Validation Loss: 0.061364\n",
      "Epoch: 4811, Training Loss: 0.070163, Validation Loss: 0.059969\n",
      "Epoch: 4812, Training Loss: 0.070137, Validation Loss: 0.060522\n",
      "Epoch: 4813, Training Loss: 0.070001, Validation Loss: 0.059428\n",
      "Epoch: 4814, Training Loss: 0.069854, Validation Loss: 0.060125\n",
      "Epoch: 4815, Training Loss: 0.069815, Validation Loss: 0.059652\n",
      "Epoch: 4816, Training Loss: 0.070058, Validation Loss: 0.061546\n",
      "Epoch: 4817, Training Loss: 0.070409, Validation Loss: 0.060087\n",
      "Epoch: 4818, Training Loss: 0.071038, Validation Loss: 0.063230\n",
      "Epoch: 4819, Training Loss: 0.071648, Validation Loss: 0.059367\n",
      "Epoch: 4820, Training Loss: 0.072885, Validation Loss: 0.065343\n",
      "Epoch: 4821, Training Loss: 0.073668, Validation Loss: 0.060091\n",
      "Epoch: 4822, Training Loss: 0.075284, Validation Loss: 0.068534\n",
      "Epoch: 4823, Training Loss: 0.076150, Validation Loss: 0.062621\n",
      "Epoch: 4824, Training Loss: 0.078124, Validation Loss: 0.072012\n",
      "Epoch: 4825, Training Loss: 0.076982, Validation Loss: 0.063086\n",
      "Epoch: 4826, Training Loss: 0.075591, Validation Loss: 0.068441\n",
      "Epoch: 4827, Training Loss: 0.072392, Validation Loss: 0.059596\n",
      "Epoch: 4828, Training Loss: 0.070451, Validation Loss: 0.060375\n",
      "Epoch: 4829, Training Loss: 0.070109, Validation Loss: 0.060264\n",
      "Epoch: 4830, Training Loss: 0.071044, Validation Loss: 0.059003\n",
      "Epoch: 4831, Training Loss: 0.072203, Validation Loss: 0.064111\n",
      "Epoch: 4832, Training Loss: 0.072464, Validation Loss: 0.060267\n",
      "Epoch: 4833, Training Loss: 0.072261, Validation Loss: 0.063801\n",
      "Epoch: 4834, Training Loss: 0.071125, Validation Loss: 0.060269\n",
      "Epoch: 4835, Training Loss: 0.070339, Validation Loss: 0.060197\n",
      "Epoch: 4836, Training Loss: 0.070130, Validation Loss: 0.060661\n",
      "Epoch: 4837, Training Loss: 0.070562, Validation Loss: 0.058694\n",
      "Epoch: 4838, Training Loss: 0.071161, Validation Loss: 0.062461\n",
      "Epoch: 4839, Training Loss: 0.071325, Validation Loss: 0.060051\n",
      "Epoch: 4840, Training Loss: 0.071248, Validation Loss: 0.063003\n",
      "Epoch: 4841, Training Loss: 0.070605, Validation Loss: 0.060964\n",
      "Epoch: 4842, Training Loss: 0.070037, Validation Loss: 0.060660\n",
      "Epoch: 4843, Training Loss: 0.069844, Validation Loss: 0.060464\n",
      "Epoch: 4844, Training Loss: 0.070253, Validation Loss: 0.058754\n",
      "Epoch: 4845, Training Loss: 0.070936, Validation Loss: 0.062616\n",
      "Epoch: 4846, Training Loss: 0.071757, Validation Loss: 0.059946\n",
      "Epoch: 4847, Training Loss: 0.072759, Validation Loss: 0.066118\n",
      "Epoch: 4848, Training Loss: 0.073680, Validation Loss: 0.060910\n",
      "Epoch: 4849, Training Loss: 0.075163, Validation Loss: 0.069182\n",
      "Epoch: 4850, Training Loss: 0.075513, Validation Loss: 0.060565\n",
      "Epoch: 4851, Training Loss: 0.075196, Validation Loss: 0.068946\n",
      "Epoch: 4852, Training Loss: 0.072943, Validation Loss: 0.059013\n",
      "Epoch: 4853, Training Loss: 0.071049, Validation Loss: 0.063008\n",
      "Epoch: 4854, Training Loss: 0.070091, Validation Loss: 0.059377\n",
      "Epoch: 4855, Training Loss: 0.069881, Validation Loss: 0.059839\n",
      "Epoch: 4856, Training Loss: 0.070123, Validation Loss: 0.061446\n",
      "Epoch: 4857, Training Loss: 0.070792, Validation Loss: 0.059093\n",
      "Epoch: 4858, Training Loss: 0.071419, Validation Loss: 0.064003\n",
      "Epoch: 4859, Training Loss: 0.071770, Validation Loss: 0.059123\n",
      "Epoch: 4860, Training Loss: 0.071648, Validation Loss: 0.064207\n",
      "Epoch: 4861, Training Loss: 0.071203, Validation Loss: 0.059116\n",
      "Epoch: 4862, Training Loss: 0.070611, Validation Loss: 0.062239\n",
      "Epoch: 4863, Training Loss: 0.070045, Validation Loss: 0.059357\n",
      "Epoch: 4864, Training Loss: 0.069786, Validation Loss: 0.059839\n",
      "Epoch: 4865, Training Loss: 0.069889, Validation Loss: 0.060609\n",
      "Epoch: 4866, Training Loss: 0.070175, Validation Loss: 0.059068\n",
      "Epoch: 4867, Training Loss: 0.070492, Validation Loss: 0.062508\n",
      "Epoch: 4868, Training Loss: 0.070756, Validation Loss: 0.059889\n",
      "Epoch: 4869, Training Loss: 0.070899, Validation Loss: 0.063556\n",
      "Epoch: 4870, Training Loss: 0.070914, Validation Loss: 0.059893\n",
      "Epoch: 4871, Training Loss: 0.070839, Validation Loss: 0.062962\n",
      "Epoch: 4872, Training Loss: 0.070663, Validation Loss: 0.058987\n",
      "Epoch: 4873, Training Loss: 0.070462, Validation Loss: 0.061816\n",
      "Epoch: 4874, Training Loss: 0.070276, Validation Loss: 0.058891\n",
      "Epoch: 4875, Training Loss: 0.070041, Validation Loss: 0.061224\n",
      "Epoch: 4876, Training Loss: 0.069836, Validation Loss: 0.059559\n",
      "Epoch: 4877, Training Loss: 0.069702, Validation Loss: 0.060401\n",
      "Epoch: 4878, Training Loss: 0.069690, Validation Loss: 0.060357\n",
      "Epoch: 4879, Training Loss: 0.069743, Validation Loss: 0.059651\n",
      "Epoch: 4880, Training Loss: 0.069838, Validation Loss: 0.061089\n",
      "Epoch: 4881, Training Loss: 0.070079, Validation Loss: 0.059470\n",
      "Epoch: 4882, Training Loss: 0.070534, Validation Loss: 0.062355\n",
      "Epoch: 4883, Training Loss: 0.070718, Validation Loss: 0.059566\n",
      "Epoch: 4884, Training Loss: 0.071342, Validation Loss: 0.063614\n",
      "Epoch: 4885, Training Loss: 0.072186, Validation Loss: 0.059693\n",
      "Epoch: 4886, Training Loss: 0.073565, Validation Loss: 0.066408\n",
      "Epoch: 4887, Training Loss: 0.073465, Validation Loss: 0.059912\n",
      "Epoch: 4888, Training Loss: 0.073778, Validation Loss: 0.066579\n",
      "Epoch: 4889, Training Loss: 0.072677, Validation Loss: 0.060238\n",
      "Epoch: 4890, Training Loss: 0.071740, Validation Loss: 0.063486\n",
      "Epoch: 4891, Training Loss: 0.070602, Validation Loss: 0.059676\n",
      "Epoch: 4892, Training Loss: 0.070086, Validation Loss: 0.059769\n",
      "Epoch: 4893, Training Loss: 0.070121, Validation Loss: 0.060745\n",
      "Epoch: 4894, Training Loss: 0.070326, Validation Loss: 0.059024\n",
      "Epoch: 4895, Training Loss: 0.070853, Validation Loss: 0.062444\n",
      "Epoch: 4896, Training Loss: 0.071080, Validation Loss: 0.059982\n",
      "Epoch: 4897, Training Loss: 0.071303, Validation Loss: 0.062817\n",
      "Epoch: 4898, Training Loss: 0.070959, Validation Loss: 0.059900\n",
      "Epoch: 4899, Training Loss: 0.070543, Validation Loss: 0.061178\n",
      "Epoch: 4900, Training Loss: 0.070147, Validation Loss: 0.058896\n",
      "Epoch: 4901, Training Loss: 0.069866, Validation Loss: 0.060511\n",
      "Epoch: 4902, Training Loss: 0.069862, Validation Loss: 0.059621\n",
      "Epoch: 4903, Training Loss: 0.069884, Validation Loss: 0.060929\n",
      "Epoch: 4904, Training Loss: 0.069852, Validation Loss: 0.060831\n",
      "Epoch: 4905, Training Loss: 0.069979, Validation Loss: 0.059711\n",
      "Epoch: 4906, Training Loss: 0.070720, Validation Loss: 0.062820\n",
      "Epoch: 4907, Training Loss: 0.071554, Validation Loss: 0.059493\n",
      "Epoch: 4908, Training Loss: 0.073059, Validation Loss: 0.066444\n",
      "Epoch: 4909, Training Loss: 0.074664, Validation Loss: 0.060835\n",
      "Epoch: 4910, Training Loss: 0.076665, Validation Loss: 0.070966\n",
      "Epoch: 4911, Training Loss: 0.075649, Validation Loss: 0.060998\n",
      "Epoch: 4912, Training Loss: 0.074893, Validation Loss: 0.068193\n",
      "Epoch: 4913, Training Loss: 0.072681, Validation Loss: 0.060800\n",
      "Epoch: 4914, Training Loss: 0.071762, Validation Loss: 0.061948\n",
      "Epoch: 4915, Training Loss: 0.071490, Validation Loss: 0.062214\n",
      "Epoch: 4916, Training Loss: 0.071141, Validation Loss: 0.059415\n",
      "Epoch: 4917, Training Loss: 0.070743, Validation Loss: 0.062319\n",
      "Epoch: 4918, Training Loss: 0.070895, Validation Loss: 0.059859\n",
      "Epoch: 4919, Training Loss: 0.071685, Validation Loss: 0.063035\n",
      "Epoch: 4920, Training Loss: 0.071884, Validation Loss: 0.061344\n",
      "Epoch: 4921, Training Loss: 0.071577, Validation Loss: 0.062641\n",
      "Epoch: 4922, Training Loss: 0.070783, Validation Loss: 0.059667\n",
      "Epoch: 4923, Training Loss: 0.070162, Validation Loss: 0.060961\n",
      "Epoch: 4924, Training Loss: 0.069927, Validation Loss: 0.058761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4925, Training Loss: 0.070030, Validation Loss: 0.060659\n",
      "Epoch: 4926, Training Loss: 0.070092, Validation Loss: 0.060491\n",
      "Epoch: 4927, Training Loss: 0.070135, Validation Loss: 0.060289\n",
      "Epoch: 4928, Training Loss: 0.070320, Validation Loss: 0.062212\n",
      "Epoch: 4929, Training Loss: 0.071034, Validation Loss: 0.059219\n",
      "Epoch: 4930, Training Loss: 0.072312, Validation Loss: 0.065604\n",
      "Epoch: 4931, Training Loss: 0.073014, Validation Loss: 0.060227\n",
      "Epoch: 4932, Training Loss: 0.073673, Validation Loss: 0.067622\n",
      "Epoch: 4933, Training Loss: 0.073825, Validation Loss: 0.060731\n",
      "Epoch: 4934, Training Loss: 0.074356, Validation Loss: 0.068110\n",
      "Epoch: 4935, Training Loss: 0.073702, Validation Loss: 0.060418\n",
      "Epoch: 4936, Training Loss: 0.073096, Validation Loss: 0.065320\n",
      "Epoch: 4937, Training Loss: 0.071711, Validation Loss: 0.060269\n",
      "Epoch: 4938, Training Loss: 0.070831, Validation Loss: 0.061358\n",
      "Epoch: 4939, Training Loss: 0.069995, Validation Loss: 0.060131\n",
      "Epoch: 4940, Training Loss: 0.069689, Validation Loss: 0.059270\n",
      "Epoch: 4941, Training Loss: 0.070095, Validation Loss: 0.061046\n",
      "Epoch: 4942, Training Loss: 0.070982, Validation Loss: 0.059260\n",
      "Epoch: 4943, Training Loss: 0.071969, Validation Loss: 0.064150\n",
      "Epoch: 4944, Training Loss: 0.072480, Validation Loss: 0.060290\n",
      "Epoch: 4945, Training Loss: 0.072855, Validation Loss: 0.066149\n",
      "Epoch: 4946, Training Loss: 0.072681, Validation Loss: 0.060119\n",
      "Epoch: 4947, Training Loss: 0.072501, Validation Loss: 0.065569\n",
      "Epoch: 4948, Training Loss: 0.071774, Validation Loss: 0.058566\n",
      "Epoch: 4949, Training Loss: 0.070787, Validation Loss: 0.062420\n",
      "Epoch: 4950, Training Loss: 0.069815, Validation Loss: 0.059074\n",
      "Epoch: 4951, Training Loss: 0.069680, Validation Loss: 0.059916\n",
      "Epoch: 4952, Training Loss: 0.070175, Validation Loss: 0.062268\n",
      "Epoch: 4953, Training Loss: 0.070463, Validation Loss: 0.059880\n",
      "Epoch: 4954, Training Loss: 0.070797, Validation Loss: 0.063436\n",
      "Epoch: 4955, Training Loss: 0.070867, Validation Loss: 0.059655\n",
      "Epoch: 4956, Training Loss: 0.070924, Validation Loss: 0.063261\n",
      "Epoch: 4957, Training Loss: 0.070776, Validation Loss: 0.059512\n",
      "Epoch: 4958, Training Loss: 0.070594, Validation Loss: 0.062365\n",
      "Epoch: 4959, Training Loss: 0.070463, Validation Loss: 0.058984\n",
      "Epoch: 4960, Training Loss: 0.070184, Validation Loss: 0.061226\n",
      "Epoch: 4961, Training Loss: 0.069795, Validation Loss: 0.059059\n",
      "Epoch: 4962, Training Loss: 0.069623, Validation Loss: 0.059516\n",
      "Epoch: 4963, Training Loss: 0.069666, Validation Loss: 0.060278\n",
      "Epoch: 4964, Training Loss: 0.069888, Validation Loss: 0.059070\n",
      "Epoch: 4965, Training Loss: 0.070528, Validation Loss: 0.062823\n",
      "Epoch: 4966, Training Loss: 0.071044, Validation Loss: 0.059988\n",
      "Epoch: 4967, Training Loss: 0.071710, Validation Loss: 0.064825\n",
      "Epoch: 4968, Training Loss: 0.072255, Validation Loss: 0.060207\n",
      "Epoch: 4969, Training Loss: 0.072847, Validation Loss: 0.065909\n",
      "Epoch: 4970, Training Loss: 0.072956, Validation Loss: 0.059559\n",
      "Epoch: 4971, Training Loss: 0.072751, Validation Loss: 0.065634\n",
      "Epoch: 4972, Training Loss: 0.072031, Validation Loss: 0.059352\n",
      "Epoch: 4973, Training Loss: 0.071249, Validation Loss: 0.063522\n",
      "Epoch: 4974, Training Loss: 0.070140, Validation Loss: 0.059841\n",
      "Epoch: 4975, Training Loss: 0.069702, Validation Loss: 0.061020\n",
      "Epoch: 4976, Training Loss: 0.069522, Validation Loss: 0.060158\n",
      "Epoch: 4977, Training Loss: 0.069680, Validation Loss: 0.059001\n",
      "Epoch: 4978, Training Loss: 0.070320, Validation Loss: 0.061567\n",
      "Epoch: 4979, Training Loss: 0.070945, Validation Loss: 0.058813\n",
      "Epoch: 4980, Training Loss: 0.071764, Validation Loss: 0.064330\n",
      "Epoch: 4981, Training Loss: 0.072374, Validation Loss: 0.060055\n",
      "Epoch: 4982, Training Loss: 0.072840, Validation Loss: 0.065997\n",
      "Epoch: 4983, Training Loss: 0.071623, Validation Loss: 0.059504\n",
      "Epoch: 4984, Training Loss: 0.070838, Validation Loss: 0.062526\n",
      "Epoch: 4985, Training Loss: 0.070174, Validation Loss: 0.059156\n",
      "Epoch: 4986, Training Loss: 0.069679, Validation Loss: 0.060328\n",
      "Epoch: 4987, Training Loss: 0.069553, Validation Loss: 0.060111\n",
      "Epoch: 4988, Training Loss: 0.069667, Validation Loss: 0.059555\n",
      "Epoch: 4989, Training Loss: 0.069790, Validation Loss: 0.061070\n",
      "Epoch: 4990, Training Loss: 0.070001, Validation Loss: 0.059326\n",
      "Epoch: 4991, Training Loss: 0.070142, Validation Loss: 0.061343\n",
      "Epoch: 4992, Training Loss: 0.070070, Validation Loss: 0.059241\n",
      "Epoch: 4993, Training Loss: 0.069990, Validation Loss: 0.061217\n",
      "Epoch: 4994, Training Loss: 0.069744, Validation Loss: 0.060012\n",
      "Epoch: 4995, Training Loss: 0.069546, Validation Loss: 0.060665\n",
      "Epoch: 4996, Training Loss: 0.069502, Validation Loss: 0.060682\n",
      "Epoch: 4997, Training Loss: 0.069594, Validation Loss: 0.059590\n",
      "Epoch: 4998, Training Loss: 0.069830, Validation Loss: 0.061265\n",
      "Epoch: 4999, Training Loss: 0.069951, Validation Loss: 0.059842\n",
      "Epoch: 5000, Training Loss: 0.070174, Validation Loss: 0.062433\n",
      "Epoch: 5001, Training Loss: 0.070341, Validation Loss: 0.060253\n",
      "Epoch: 5002, Training Loss: 0.070515, Validation Loss: 0.062755\n",
      "Epoch: 5003, Training Loss: 0.070934, Validation Loss: 0.058931\n",
      "Epoch: 5004, Training Loss: 0.071794, Validation Loss: 0.064431\n",
      "Epoch: 5005, Training Loss: 0.073668, Validation Loss: 0.059801\n",
      "Epoch: 5006, Training Loss: 0.076648, Validation Loss: 0.071338\n",
      "Epoch: 5007, Training Loss: 0.077208, Validation Loss: 0.062754\n",
      "Epoch: 5008, Training Loss: 0.078100, Validation Loss: 0.073241\n",
      "Epoch: 5009, Training Loss: 0.075870, Validation Loss: 0.061960\n",
      "Epoch: 5010, Training Loss: 0.074311, Validation Loss: 0.066856\n",
      "Epoch: 5011, Training Loss: 0.072326, Validation Loss: 0.060581\n",
      "Epoch: 5012, Training Loss: 0.071503, Validation Loss: 0.060182\n",
      "Epoch: 5013, Training Loss: 0.071191, Validation Loss: 0.062172\n",
      "Epoch: 5014, Training Loss: 0.071167, Validation Loss: 0.059222\n",
      "Epoch: 5015, Training Loss: 0.071396, Validation Loss: 0.063807\n",
      "Epoch: 5016, Training Loss: 0.071627, Validation Loss: 0.060965\n",
      "Epoch: 5017, Training Loss: 0.072186, Validation Loss: 0.063258\n",
      "Epoch: 5018, Training Loss: 0.071711, Validation Loss: 0.061392\n",
      "Epoch: 5019, Training Loss: 0.070733, Validation Loss: 0.061507\n",
      "Epoch: 5020, Training Loss: 0.069792, Validation Loss: 0.060179\n",
      "Epoch: 5021, Training Loss: 0.069596, Validation Loss: 0.061079\n",
      "Epoch: 5022, Training Loss: 0.070076, Validation Loss: 0.060726\n",
      "Epoch: 5023, Training Loss: 0.070388, Validation Loss: 0.061161\n",
      "Epoch: 5024, Training Loss: 0.070425, Validation Loss: 0.061326\n",
      "Epoch: 5025, Training Loss: 0.070563, Validation Loss: 0.059163\n",
      "Epoch: 5026, Training Loss: 0.071556, Validation Loss: 0.064213\n",
      "Epoch: 5027, Training Loss: 0.073344, Validation Loss: 0.060205\n",
      "Epoch: 5028, Training Loss: 0.074715, Validation Loss: 0.068777\n",
      "Epoch: 5029, Training Loss: 0.072536, Validation Loss: 0.060094\n",
      "Epoch: 5030, Training Loss: 0.071098, Validation Loss: 0.063075\n",
      "Epoch: 5031, Training Loss: 0.070351, Validation Loss: 0.059959\n",
      "Epoch: 5032, Training Loss: 0.070479, Validation Loss: 0.059852\n",
      "Epoch: 5033, Training Loss: 0.070631, Validation Loss: 0.061558\n",
      "Epoch: 5034, Training Loss: 0.070003, Validation Loss: 0.058779\n",
      "Epoch: 5035, Training Loss: 0.069891, Validation Loss: 0.060343\n",
      "Epoch: 5036, Training Loss: 0.070465, Validation Loss: 0.060475\n",
      "Epoch: 5037, Training Loss: 0.070821, Validation Loss: 0.060519\n",
      "Epoch: 5038, Training Loss: 0.070284, Validation Loss: 0.060371\n",
      "Epoch: 5039, Training Loss: 0.069708, Validation Loss: 0.060172\n",
      "Epoch: 5040, Training Loss: 0.069611, Validation Loss: 0.059405\n",
      "Epoch: 5041, Training Loss: 0.069833, Validation Loss: 0.060715\n",
      "Epoch: 5042, Training Loss: 0.069943, Validation Loss: 0.059703\n",
      "Epoch: 5043, Training Loss: 0.069726, Validation Loss: 0.059476\n",
      "Epoch: 5044, Training Loss: 0.069734, Validation Loss: 0.060725\n",
      "Epoch: 5045, Training Loss: 0.070178, Validation Loss: 0.059211\n",
      "Epoch: 5046, Training Loss: 0.071265, Validation Loss: 0.064545\n",
      "Epoch: 5047, Training Loss: 0.072945, Validation Loss: 0.061020\n",
      "Epoch: 5048, Training Loss: 0.075153, Validation Loss: 0.069948\n",
      "Epoch: 5049, Training Loss: 0.076030, Validation Loss: 0.062359\n",
      "Epoch: 5050, Training Loss: 0.077762, Validation Loss: 0.072739\n",
      "Epoch: 5051, Training Loss: 0.077133, Validation Loss: 0.063356\n",
      "Epoch: 5052, Training Loss: 0.076687, Validation Loss: 0.069947\n",
      "Epoch: 5053, Training Loss: 0.073499, Validation Loss: 0.061883\n",
      "Epoch: 5054, Training Loss: 0.071062, Validation Loss: 0.061704\n",
      "Epoch: 5055, Training Loss: 0.069662, Validation Loss: 0.059994\n",
      "Epoch: 5056, Training Loss: 0.070140, Validation Loss: 0.059701\n",
      "Epoch: 5057, Training Loss: 0.071648, Validation Loss: 0.063159\n",
      "Epoch: 5058, Training Loss: 0.072413, Validation Loss: 0.061515\n",
      "Epoch: 5059, Training Loss: 0.072071, Validation Loss: 0.062964\n",
      "Epoch: 5060, Training Loss: 0.070608, Validation Loss: 0.060021\n",
      "Epoch: 5061, Training Loss: 0.069587, Validation Loss: 0.060265\n",
      "Epoch: 5062, Training Loss: 0.069612, Validation Loss: 0.060380\n",
      "Epoch: 5063, Training Loss: 0.070267, Validation Loss: 0.061126\n",
      "Epoch: 5064, Training Loss: 0.070735, Validation Loss: 0.062179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5065, Training Loss: 0.070512, Validation Loss: 0.060012\n",
      "Epoch: 5066, Training Loss: 0.070239, Validation Loss: 0.061696\n",
      "Epoch: 5067, Training Loss: 0.069964, Validation Loss: 0.058667\n",
      "Epoch: 5068, Training Loss: 0.069882, Validation Loss: 0.061593\n",
      "Epoch: 5069, Training Loss: 0.069722, Validation Loss: 0.059591\n",
      "Epoch: 5070, Training Loss: 0.069451, Validation Loss: 0.060361\n",
      "Epoch: 5071, Training Loss: 0.069426, Validation Loss: 0.060771\n",
      "Epoch: 5072, Training Loss: 0.069836, Validation Loss: 0.059125\n",
      "Epoch: 5073, Training Loss: 0.070367, Validation Loss: 0.063100\n",
      "Epoch: 5074, Training Loss: 0.070464, Validation Loss: 0.059697\n",
      "Epoch: 5075, Training Loss: 0.070258, Validation Loss: 0.063266\n",
      "Epoch: 5076, Training Loss: 0.070467, Validation Loss: 0.059161\n",
      "Epoch: 5077, Training Loss: 0.070490, Validation Loss: 0.062612\n",
      "Epoch: 5078, Training Loss: 0.070217, Validation Loss: 0.058671\n",
      "Epoch: 5079, Training Loss: 0.069839, Validation Loss: 0.060801\n",
      "Epoch: 5080, Training Loss: 0.069394, Validation Loss: 0.059833\n",
      "Epoch: 5081, Training Loss: 0.070385, Validation Loss: 0.061891\n",
      "Epoch: 5082, Training Loss: 0.079310, Validation Loss: 0.069789\n",
      "Epoch: 5083, Training Loss: 0.096545, Validation Loss: 0.090761\n",
      "Epoch: 5084, Training Loss: 0.094595, Validation Loss: 0.080134\n",
      "Epoch: 5085, Training Loss: 0.090854, Validation Loss: 0.086593\n",
      "Epoch: 5086, Training Loss: 0.078559, Validation Loss: 0.061135\n",
      "Epoch: 5087, Training Loss: 0.072248, Validation Loss: 0.064660\n",
      "Epoch: 5088, Training Loss: 0.075436, Validation Loss: 0.067855\n",
      "Epoch: 5089, Training Loss: 0.081693, Validation Loss: 0.065321\n",
      "Epoch: 5090, Training Loss: 0.081883, Validation Loss: 0.075950\n",
      "Epoch: 5091, Training Loss: 0.071251, Validation Loss: 0.058506\n",
      "Epoch: 5092, Training Loss: 0.071165, Validation Loss: 0.058112\n",
      "Epoch: 5093, Training Loss: 0.077482, Validation Loss: 0.070678\n",
      "Epoch: 5094, Training Loss: 0.074172, Validation Loss: 0.060550\n",
      "Epoch: 5095, Training Loss: 0.072221, Validation Loss: 0.060048\n",
      "Epoch: 5096, Training Loss: 0.072620, Validation Loss: 0.063335\n",
      "Epoch: 5097, Training Loss: 0.070992, Validation Loss: 0.057741\n",
      "Epoch: 5098, Training Loss: 0.072463, Validation Loss: 0.060584\n",
      "Epoch: 5099, Training Loss: 0.073548, Validation Loss: 0.064561\n",
      "Epoch: 5100, Training Loss: 0.070081, Validation Loss: 0.058902\n",
      "Epoch: 5101, Training Loss: 0.070220, Validation Loss: 0.058707\n",
      "Epoch: 5102, Training Loss: 0.072003, Validation Loss: 0.062962\n",
      "Epoch: 5103, Training Loss: 0.070862, Validation Loss: 0.059585\n",
      "Epoch: 5104, Training Loss: 0.069588, Validation Loss: 0.058712\n",
      "Epoch: 5105, Training Loss: 0.070604, Validation Loss: 0.062597\n",
      "Epoch: 5106, Training Loss: 0.070431, Validation Loss: 0.059561\n",
      "Epoch: 5107, Training Loss: 0.069525, Validation Loss: 0.059832\n",
      "Epoch: 5108, Training Loss: 0.070267, Validation Loss: 0.062981\n",
      "Epoch: 5109, Training Loss: 0.070522, Validation Loss: 0.058854\n",
      "Epoch: 5110, Training Loss: 0.069541, Validation Loss: 0.061479\n",
      "Epoch: 5111, Training Loss: 0.069193, Validation Loss: 0.060686\n",
      "Epoch: 5112, Training Loss: 0.070118, Validation Loss: 0.059438\n",
      "Epoch: 5113, Training Loss: 0.070680, Validation Loss: 0.064163\n",
      "Epoch: 5114, Training Loss: 0.070223, Validation Loss: 0.059815\n",
      "Epoch: 5115, Training Loss: 0.069905, Validation Loss: 0.062087\n",
      "Epoch: 5116, Training Loss: 0.069516, Validation Loss: 0.060200\n",
      "Epoch: 5117, Training Loss: 0.069281, Validation Loss: 0.058843\n",
      "Epoch: 5118, Training Loss: 0.069646, Validation Loss: 0.061353\n",
      "Epoch: 5119, Training Loss: 0.070223, Validation Loss: 0.059541\n",
      "Epoch: 5120, Training Loss: 0.070322, Validation Loss: 0.062928\n",
      "Epoch: 5121, Training Loss: 0.069473, Validation Loss: 0.060085\n",
      "Epoch: 5122, Training Loss: 0.069028, Validation Loss: 0.060687\n",
      "Epoch: 5123, Training Loss: 0.069265, Validation Loss: 0.060803\n",
      "Epoch: 5124, Training Loss: 0.069684, Validation Loss: 0.059159\n",
      "Epoch: 5125, Training Loss: 0.069769, Validation Loss: 0.062106\n",
      "Epoch: 5126, Training Loss: 0.070042, Validation Loss: 0.058598\n",
      "Epoch: 5127, Training Loss: 0.070103, Validation Loss: 0.063354\n",
      "Epoch: 5128, Training Loss: 0.069629, Validation Loss: 0.059566\n",
      "Epoch: 5129, Training Loss: 0.069156, Validation Loss: 0.061828\n",
      "Epoch: 5130, Training Loss: 0.069036, Validation Loss: 0.060608\n",
      "Epoch: 5131, Training Loss: 0.069156, Validation Loss: 0.059280\n",
      "Epoch: 5132, Training Loss: 0.069167, Validation Loss: 0.060960\n",
      "Epoch: 5133, Training Loss: 0.069307, Validation Loss: 0.058982\n",
      "Epoch: 5134, Training Loss: 0.069337, Validation Loss: 0.061651\n",
      "Epoch: 5135, Training Loss: 0.069163, Validation Loss: 0.060366\n",
      "Epoch: 5136, Training Loss: 0.069018, Validation Loss: 0.061589\n",
      "Epoch: 5137, Training Loss: 0.069061, Validation Loss: 0.060048\n",
      "Epoch: 5138, Training Loss: 0.069296, Validation Loss: 0.061698\n",
      "Epoch: 5139, Training Loss: 0.069510, Validation Loss: 0.058712\n",
      "Epoch: 5140, Training Loss: 0.069622, Validation Loss: 0.062178\n",
      "Epoch: 5141, Training Loss: 0.070146, Validation Loss: 0.058804\n",
      "Epoch: 5142, Training Loss: 0.070725, Validation Loss: 0.064342\n",
      "Epoch: 5143, Training Loss: 0.070942, Validation Loss: 0.059294\n",
      "Epoch: 5144, Training Loss: 0.070829, Validation Loss: 0.064513\n",
      "Epoch: 5145, Training Loss: 0.070130, Validation Loss: 0.058848\n",
      "Epoch: 5146, Training Loss: 0.069484, Validation Loss: 0.062040\n",
      "Epoch: 5147, Training Loss: 0.068901, Validation Loss: 0.059858\n",
      "Epoch: 5148, Training Loss: 0.069137, Validation Loss: 0.059292\n",
      "Epoch: 5149, Training Loss: 0.070160, Validation Loss: 0.063323\n",
      "Epoch: 5150, Training Loss: 0.071217, Validation Loss: 0.058598\n",
      "Epoch: 5151, Training Loss: 0.071530, Validation Loss: 0.065222\n",
      "Epoch: 5152, Training Loss: 0.070664, Validation Loss: 0.059048\n",
      "Epoch: 5153, Training Loss: 0.069690, Validation Loss: 0.062248\n",
      "Epoch: 5154, Training Loss: 0.068971, Validation Loss: 0.059617\n",
      "Epoch: 5155, Training Loss: 0.068901, Validation Loss: 0.059466\n",
      "Epoch: 5156, Training Loss: 0.069307, Validation Loss: 0.061192\n",
      "Epoch: 5157, Training Loss: 0.069818, Validation Loss: 0.058579\n",
      "Epoch: 5158, Training Loss: 0.069645, Validation Loss: 0.061876\n",
      "Epoch: 5159, Training Loss: 0.069274, Validation Loss: 0.058696\n",
      "Epoch: 5160, Training Loss: 0.068865, Validation Loss: 0.060717\n",
      "Epoch: 5161, Training Loss: 0.068768, Validation Loss: 0.060418\n",
      "Epoch: 5162, Training Loss: 0.069106, Validation Loss: 0.059389\n",
      "Epoch: 5163, Training Loss: 0.070166, Validation Loss: 0.063850\n",
      "Epoch: 5164, Training Loss: 0.071294, Validation Loss: 0.059614\n",
      "Epoch: 5165, Training Loss: 0.072593, Validation Loss: 0.067563\n",
      "Epoch: 5166, Training Loss: 0.073729, Validation Loss: 0.060142\n",
      "Epoch: 5167, Training Loss: 0.074337, Validation Loss: 0.069542\n",
      "Epoch: 5168, Training Loss: 0.072350, Validation Loss: 0.058981\n",
      "Epoch: 5169, Training Loss: 0.070293, Validation Loss: 0.063053\n",
      "Epoch: 5170, Training Loss: 0.069051, Validation Loss: 0.059220\n",
      "Epoch: 5171, Training Loss: 0.068995, Validation Loss: 0.058891\n",
      "Epoch: 5172, Training Loss: 0.070118, Validation Loss: 0.062299\n",
      "Epoch: 5173, Training Loss: 0.070545, Validation Loss: 0.059590\n",
      "Epoch: 5174, Training Loss: 0.069842, Validation Loss: 0.061742\n",
      "Epoch: 5175, Training Loss: 0.069027, Validation Loss: 0.059534\n",
      "Epoch: 5176, Training Loss: 0.069022, Validation Loss: 0.058660\n",
      "Epoch: 5177, Training Loss: 0.069420, Validation Loss: 0.061036\n",
      "Epoch: 5178, Training Loss: 0.069439, Validation Loss: 0.059663\n",
      "Epoch: 5179, Training Loss: 0.069031, Validation Loss: 0.060628\n",
      "Epoch: 5180, Training Loss: 0.068696, Validation Loss: 0.060381\n",
      "Epoch: 5181, Training Loss: 0.068891, Validation Loss: 0.059872\n",
      "Epoch: 5182, Training Loss: 0.069301, Validation Loss: 0.061624\n",
      "Epoch: 5183, Training Loss: 0.069485, Validation Loss: 0.059593\n",
      "Epoch: 5184, Training Loss: 0.069178, Validation Loss: 0.061813\n",
      "Epoch: 5185, Training Loss: 0.068932, Validation Loss: 0.059467\n",
      "Epoch: 5186, Training Loss: 0.069034, Validation Loss: 0.061956\n",
      "Epoch: 5187, Training Loss: 0.068970, Validation Loss: 0.059580\n",
      "Epoch: 5188, Training Loss: 0.069153, Validation Loss: 0.062203\n",
      "Epoch: 5189, Training Loss: 0.069207, Validation Loss: 0.059823\n",
      "Epoch: 5190, Training Loss: 0.069546, Validation Loss: 0.063179\n",
      "Epoch: 5191, Training Loss: 0.069906, Validation Loss: 0.059677\n",
      "Epoch: 5192, Training Loss: 0.070589, Validation Loss: 0.064431\n",
      "Epoch: 5193, Training Loss: 0.070953, Validation Loss: 0.058780\n",
      "Epoch: 5194, Training Loss: 0.071513, Validation Loss: 0.064931\n",
      "Epoch: 5195, Training Loss: 0.071341, Validation Loss: 0.058894\n",
      "Epoch: 5196, Training Loss: 0.070529, Validation Loss: 0.063632\n",
      "Epoch: 5197, Training Loss: 0.069471, Validation Loss: 0.058734\n",
      "Epoch: 5198, Training Loss: 0.068888, Validation Loss: 0.059824\n",
      "Epoch: 5199, Training Loss: 0.068827, Validation Loss: 0.059658\n",
      "Epoch: 5200, Training Loss: 0.069252, Validation Loss: 0.058395\n",
      "Epoch: 5201, Training Loss: 0.069515, Validation Loss: 0.061442\n",
      "Epoch: 5202, Training Loss: 0.069514, Validation Loss: 0.058845\n",
      "Epoch: 5203, Training Loss: 0.069120, Validation Loss: 0.061034\n",
      "Epoch: 5204, Training Loss: 0.068627, Validation Loss: 0.059529\n",
      "Epoch: 5205, Training Loss: 0.068617, Validation Loss: 0.059902\n",
      "Epoch: 5206, Training Loss: 0.068927, Validation Loss: 0.061563\n",
      "Epoch: 5207, Training Loss: 0.069427, Validation Loss: 0.059481\n",
      "Epoch: 5208, Training Loss: 0.069625, Validation Loss: 0.062971\n",
      "Epoch: 5209, Training Loss: 0.070227, Validation Loss: 0.058339\n",
      "Epoch: 5210, Training Loss: 0.071614, Validation Loss: 0.066156\n",
      "Epoch: 5211, Training Loss: 0.079867, Validation Loss: 0.065387\n",
      "Epoch: 5212, Training Loss: 0.119495, Validation Loss: 0.120197\n",
      "Epoch: 5213, Training Loss: 0.143579, Validation Loss: 0.117397\n",
      "Epoch: 5214, Training Loss: 0.124409, Validation Loss: 0.124298\n",
      "Epoch: 5215, Training Loss: 0.077440, Validation Loss: 0.067492\n",
      "Epoch: 5216, Training Loss: 0.103381, Validation Loss: 0.075575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5217, Training Loss: 0.116742, Validation Loss: 0.109336\n",
      "Epoch: 5218, Training Loss: 0.111263, Validation Loss: 0.099085\n",
      "Epoch: 5219, Training Loss: 0.079333, Validation Loss: 0.061090\n",
      "Epoch: 5220, Training Loss: 0.089764, Validation Loss: 0.072108\n",
      "Epoch: 5221, Training Loss: 0.095318, Validation Loss: 0.083084\n",
      "Epoch: 5222, Training Loss: 0.086242, Validation Loss: 0.077883\n",
      "Epoch: 5223, Training Loss: 0.076546, Validation Loss: 0.058163\n",
      "Epoch: 5224, Training Loss: 0.091302, Validation Loss: 0.069276\n",
      "Epoch: 5225, Training Loss: 0.086172, Validation Loss: 0.074803\n",
      "Epoch: 5226, Training Loss: 0.078982, Validation Loss: 0.068479\n",
      "Epoch: 5227, Training Loss: 0.083119, Validation Loss: 0.064722\n",
      "Epoch: 5228, Training Loss: 0.088022, Validation Loss: 0.069415\n",
      "Epoch: 5229, Training Loss: 0.071824, Validation Loss: 0.058830\n",
      "Epoch: 5230, Training Loss: 0.084853, Validation Loss: 0.074479\n",
      "Epoch: 5231, Training Loss: 0.079603, Validation Loss: 0.069458\n",
      "Epoch: 5232, Training Loss: 0.075522, Validation Loss: 0.059749\n",
      "Epoch: 5233, Training Loss: 0.078387, Validation Loss: 0.064470\n",
      "Epoch: 5234, Training Loss: 0.080594, Validation Loss: 0.074005\n",
      "Epoch: 5235, Training Loss: 0.071496, Validation Loss: 0.061074\n",
      "Epoch: 5236, Training Loss: 0.081097, Validation Loss: 0.063592\n",
      "Epoch: 5237, Training Loss: 0.075705, Validation Loss: 0.065293\n",
      "Epoch: 5238, Training Loss: 0.072882, Validation Loss: 0.063623\n",
      "Epoch: 5239, Training Loss: 0.077184, Validation Loss: 0.062949\n",
      "Epoch: 5240, Training Loss: 0.074724, Validation Loss: 0.061406\n",
      "Epoch: 5241, Training Loss: 0.071296, Validation Loss: 0.060016\n",
      "Epoch: 5242, Training Loss: 0.076701, Validation Loss: 0.065476\n",
      "Epoch: 5243, Training Loss: 0.071393, Validation Loss: 0.061331\n",
      "Epoch: 5244, Training Loss: 0.072687, Validation Loss: 0.060650\n",
      "Epoch: 5245, Training Loss: 0.073559, Validation Loss: 0.060661\n",
      "Epoch: 5246, Training Loss: 0.071527, Validation Loss: 0.061504\n",
      "Epoch: 5247, Training Loss: 0.071493, Validation Loss: 0.060507\n",
      "Epoch: 5248, Training Loss: 0.072932, Validation Loss: 0.058563\n",
      "Epoch: 5249, Training Loss: 0.070431, Validation Loss: 0.060342\n",
      "Epoch: 5250, Training Loss: 0.071438, Validation Loss: 0.061781\n",
      "Epoch: 5251, Training Loss: 0.071564, Validation Loss: 0.057171\n",
      "Epoch: 5252, Training Loss: 0.069703, Validation Loss: 0.057554\n",
      "Epoch: 5253, Training Loss: 0.071312, Validation Loss: 0.061886\n",
      "Epoch: 5254, Training Loss: 0.070110, Validation Loss: 0.058171\n",
      "Epoch: 5255, Training Loss: 0.069719, Validation Loss: 0.057927\n",
      "Epoch: 5256, Training Loss: 0.071539, Validation Loss: 0.061762\n",
      "Epoch: 5257, Training Loss: 0.074086, Validation Loss: 0.062291\n",
      "Epoch: 5258, Training Loss: 0.070619, Validation Loss: 0.060205\n",
      "Epoch: 5259, Training Loss: 0.070793, Validation Loss: 0.059827\n",
      "Epoch: 5260, Training Loss: 0.072346, Validation Loss: 0.060526\n",
      "Epoch: 5261, Training Loss: 0.069954, Validation Loss: 0.060477\n",
      "Epoch: 5262, Training Loss: 0.070918, Validation Loss: 0.059844\n",
      "Epoch: 5263, Training Loss: 0.070653, Validation Loss: 0.058477\n",
      "Epoch: 5264, Training Loss: 0.070242, Validation Loss: 0.061423\n",
      "Epoch: 5265, Training Loss: 0.070024, Validation Loss: 0.060080\n",
      "Epoch: 5266, Training Loss: 0.070564, Validation Loss: 0.057832\n",
      "Epoch: 5267, Training Loss: 0.069756, Validation Loss: 0.060312\n",
      "Epoch: 5268, Training Loss: 0.069781, Validation Loss: 0.060728\n",
      "Epoch: 5269, Training Loss: 0.070304, Validation Loss: 0.057789\n",
      "Epoch: 5270, Training Loss: 0.069209, Validation Loss: 0.059073\n",
      "Epoch: 5271, Training Loss: 0.070052, Validation Loss: 0.061300\n",
      "Epoch: 5272, Training Loss: 0.069859, Validation Loss: 0.057647\n",
      "Epoch: 5273, Training Loss: 0.069352, Validation Loss: 0.057974\n",
      "Epoch: 5274, Training Loss: 0.070063, Validation Loss: 0.061292\n",
      "Epoch: 5275, Training Loss: 0.069373, Validation Loss: 0.058296\n",
      "Epoch: 5276, Training Loss: 0.069523, Validation Loss: 0.058116\n",
      "Epoch: 5277, Training Loss: 0.069483, Validation Loss: 0.060821\n",
      "Epoch: 5278, Training Loss: 0.069410, Validation Loss: 0.060099\n",
      "Epoch: 5279, Training Loss: 0.069187, Validation Loss: 0.058980\n",
      "Epoch: 5280, Training Loss: 0.069351, Validation Loss: 0.060414\n",
      "Epoch: 5281, Training Loss: 0.069157, Validation Loss: 0.059554\n",
      "Epoch: 5282, Training Loss: 0.069052, Validation Loss: 0.058473\n",
      "Epoch: 5283, Training Loss: 0.069338, Validation Loss: 0.059967\n",
      "Epoch: 5284, Training Loss: 0.068892, Validation Loss: 0.059853\n",
      "Epoch: 5285, Training Loss: 0.069062, Validation Loss: 0.060477\n",
      "Epoch: 5286, Training Loss: 0.069018, Validation Loss: 0.060369\n",
      "Epoch: 5287, Training Loss: 0.068737, Validation Loss: 0.060345\n",
      "Epoch: 5288, Training Loss: 0.068978, Validation Loss: 0.059843\n",
      "Epoch: 5289, Training Loss: 0.068768, Validation Loss: 0.060572\n",
      "Epoch: 5290, Training Loss: 0.068898, Validation Loss: 0.058664\n",
      "Epoch: 5291, Training Loss: 0.068563, Validation Loss: 0.060520\n",
      "Epoch: 5292, Training Loss: 0.068465, Validation Loss: 0.060464\n",
      "Epoch: 5293, Training Loss: 0.068754, Validation Loss: 0.059126\n",
      "Epoch: 5294, Training Loss: 0.068513, Validation Loss: 0.060633\n",
      "Epoch: 5295, Training Loss: 0.068546, Validation Loss: 0.060114\n",
      "Epoch: 5296, Training Loss: 0.068462, Validation Loss: 0.059968\n",
      "Epoch: 5297, Training Loss: 0.068577, Validation Loss: 0.061166\n",
      "Epoch: 5298, Training Loss: 0.068663, Validation Loss: 0.060016\n",
      "Epoch: 5299, Training Loss: 0.068345, Validation Loss: 0.060393\n",
      "Epoch: 5300, Training Loss: 0.068375, Validation Loss: 0.060233\n",
      "Epoch: 5301, Training Loss: 0.068637, Validation Loss: 0.058357\n",
      "Epoch: 5302, Training Loss: 0.068510, Validation Loss: 0.060448\n",
      "Epoch: 5303, Training Loss: 0.068314, Validation Loss: 0.059574\n",
      "Epoch: 5304, Training Loss: 0.068287, Validation Loss: 0.060606\n",
      "Epoch: 5305, Training Loss: 0.068267, Validation Loss: 0.061150\n",
      "Epoch: 5306, Training Loss: 0.068259, Validation Loss: 0.059995\n",
      "Epoch: 5307, Training Loss: 0.068261, Validation Loss: 0.060520\n",
      "Epoch: 5308, Training Loss: 0.068278, Validation Loss: 0.059080\n",
      "Epoch: 5309, Training Loss: 0.068191, Validation Loss: 0.060134\n",
      "Epoch: 5310, Training Loss: 0.068178, Validation Loss: 0.060746\n",
      "Epoch: 5311, Training Loss: 0.068700, Validation Loss: 0.059157\n",
      "Epoch: 5312, Training Loss: 0.068898, Validation Loss: 0.062583\n",
      "Epoch: 5313, Training Loss: 0.068728, Validation Loss: 0.058280\n",
      "Epoch: 5314, Training Loss: 0.068200, Validation Loss: 0.060429\n",
      "Epoch: 5315, Training Loss: 0.068217, Validation Loss: 0.061053\n",
      "Epoch: 5316, Training Loss: 0.069358, Validation Loss: 0.058952\n",
      "Epoch: 5317, Training Loss: 0.069796, Validation Loss: 0.064724\n",
      "Epoch: 5318, Training Loss: 0.068678, Validation Loss: 0.059375\n",
      "Epoch: 5319, Training Loss: 0.067999, Validation Loss: 0.059883\n",
      "Epoch: 5320, Training Loss: 0.068780, Validation Loss: 0.062057\n",
      "Epoch: 5321, Training Loss: 0.069713, Validation Loss: 0.057894\n",
      "Epoch: 5322, Training Loss: 0.069136, Validation Loss: 0.062866\n",
      "Epoch: 5323, Training Loss: 0.067995, Validation Loss: 0.059286\n",
      "Epoch: 5324, Training Loss: 0.068425, Validation Loss: 0.058832\n",
      "Epoch: 5325, Training Loss: 0.069386, Validation Loss: 0.063197\n",
      "Epoch: 5326, Training Loss: 0.069224, Validation Loss: 0.058594\n",
      "Epoch: 5327, Training Loss: 0.068356, Validation Loss: 0.061362\n",
      "Epoch: 5328, Training Loss: 0.067918, Validation Loss: 0.059634\n",
      "Epoch: 5329, Training Loss: 0.068343, Validation Loss: 0.058357\n",
      "Epoch: 5330, Training Loss: 0.068521, Validation Loss: 0.061411\n",
      "Epoch: 5331, Training Loss: 0.068186, Validation Loss: 0.058741\n",
      "Epoch: 5332, Training Loss: 0.067870, Validation Loss: 0.059580\n",
      "Epoch: 5333, Training Loss: 0.068047, Validation Loss: 0.060558\n",
      "Epoch: 5334, Training Loss: 0.068212, Validation Loss: 0.058681\n",
      "Epoch: 5335, Training Loss: 0.068462, Validation Loss: 0.062316\n",
      "Epoch: 5336, Training Loss: 0.068317, Validation Loss: 0.059862\n",
      "Epoch: 5337, Training Loss: 0.068142, Validation Loss: 0.062045\n",
      "Epoch: 5338, Training Loss: 0.067849, Validation Loss: 0.059480\n",
      "Epoch: 5339, Training Loss: 0.067819, Validation Loss: 0.058812\n",
      "Epoch: 5340, Training Loss: 0.068438, Validation Loss: 0.061660\n",
      "Epoch: 5341, Training Loss: 0.068595, Validation Loss: 0.058432\n",
      "Epoch: 5342, Training Loss: 0.068360, Validation Loss: 0.062130\n",
      "Epoch: 5343, Training Loss: 0.067928, Validation Loss: 0.059446\n",
      "Epoch: 5344, Training Loss: 0.067684, Validation Loss: 0.059767\n",
      "Epoch: 5345, Training Loss: 0.067951, Validation Loss: 0.061003\n",
      "Epoch: 5346, Training Loss: 0.068279, Validation Loss: 0.058133\n",
      "Epoch: 5347, Training Loss: 0.068481, Validation Loss: 0.062085\n",
      "Epoch: 5348, Training Loss: 0.068443, Validation Loss: 0.058746\n",
      "Epoch: 5349, Training Loss: 0.068371, Validation Loss: 0.062718\n",
      "Epoch: 5350, Training Loss: 0.068054, Validation Loss: 0.059207\n",
      "Epoch: 5351, Training Loss: 0.068054, Validation Loss: 0.061302\n",
      "Epoch: 5352, Training Loss: 0.067684, Validation Loss: 0.059108\n",
      "Epoch: 5353, Training Loss: 0.067572, Validation Loss: 0.059691\n",
      "Epoch: 5354, Training Loss: 0.067754, Validation Loss: 0.060876\n",
      "Epoch: 5355, Training Loss: 0.067932, Validation Loss: 0.058733\n",
      "Epoch: 5356, Training Loss: 0.068460, Validation Loss: 0.062499\n",
      "Epoch: 5357, Training Loss: 0.068668, Validation Loss: 0.059044\n",
      "Epoch: 5358, Training Loss: 0.069340, Validation Loss: 0.064183\n",
      "Epoch: 5359, Training Loss: 0.068770, Validation Loss: 0.058892\n",
      "Epoch: 5360, Training Loss: 0.067937, Validation Loss: 0.060993\n",
      "Epoch: 5361, Training Loss: 0.067520, Validation Loss: 0.059136\n",
      "Epoch: 5362, Training Loss: 0.067715, Validation Loss: 0.058269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5363, Training Loss: 0.068434, Validation Loss: 0.061850\n",
      "Epoch: 5364, Training Loss: 0.068915, Validation Loss: 0.058273\n",
      "Epoch: 5365, Training Loss: 0.068490, Validation Loss: 0.062322\n",
      "Epoch: 5366, Training Loss: 0.067759, Validation Loss: 0.058627\n",
      "Epoch: 5367, Training Loss: 0.067440, Validation Loss: 0.059414\n",
      "Epoch: 5368, Training Loss: 0.067717, Validation Loss: 0.060776\n",
      "Epoch: 5369, Training Loss: 0.067868, Validation Loss: 0.058788\n",
      "Epoch: 5370, Training Loss: 0.067845, Validation Loss: 0.061298\n",
      "Epoch: 5371, Training Loss: 0.067559, Validation Loss: 0.059065\n",
      "Epoch: 5372, Training Loss: 0.067411, Validation Loss: 0.059892\n",
      "Epoch: 5373, Training Loss: 0.067352, Validation Loss: 0.059416\n",
      "Epoch: 5374, Training Loss: 0.067393, Validation Loss: 0.058690\n",
      "Epoch: 5375, Training Loss: 0.067386, Validation Loss: 0.059892\n",
      "Epoch: 5376, Training Loss: 0.067343, Validation Loss: 0.059336\n",
      "Epoch: 5377, Training Loss: 0.067528, Validation Loss: 0.061208\n",
      "Epoch: 5378, Training Loss: 0.067735, Validation Loss: 0.059493\n",
      "Epoch: 5379, Training Loss: 0.067891, Validation Loss: 0.062133\n",
      "Epoch: 5380, Training Loss: 0.068016, Validation Loss: 0.058193\n",
      "Epoch: 5381, Training Loss: 0.068803, Validation Loss: 0.063184\n",
      "Epoch: 5382, Training Loss: 0.070461, Validation Loss: 0.058236\n",
      "Epoch: 5383, Training Loss: 0.072119, Validation Loss: 0.068251\n",
      "Epoch: 5384, Training Loss: 0.072461, Validation Loss: 0.059872\n",
      "Epoch: 5385, Training Loss: 0.070523, Validation Loss: 0.065895\n",
      "Epoch: 5386, Training Loss: 0.067993, Validation Loss: 0.058802\n",
      "Epoch: 5387, Training Loss: 0.067653, Validation Loss: 0.058853\n",
      "Epoch: 5388, Training Loss: 0.069384, Validation Loss: 0.063865\n",
      "Epoch: 5389, Training Loss: 0.069601, Validation Loss: 0.058301\n",
      "Epoch: 5390, Training Loss: 0.068114, Validation Loss: 0.061601\n",
      "Epoch: 5391, Training Loss: 0.067294, Validation Loss: 0.059529\n",
      "Epoch: 5392, Training Loss: 0.067711, Validation Loss: 0.058647\n",
      "Epoch: 5393, Training Loss: 0.068093, Validation Loss: 0.061311\n",
      "Epoch: 5394, Training Loss: 0.068346, Validation Loss: 0.057661\n",
      "Epoch: 5395, Training Loss: 0.068022, Validation Loss: 0.061852\n",
      "Epoch: 5396, Training Loss: 0.067773, Validation Loss: 0.059332\n",
      "Epoch: 5397, Training Loss: 0.067290, Validation Loss: 0.060791\n",
      "Epoch: 5398, Training Loss: 0.067167, Validation Loss: 0.060189\n",
      "Epoch: 5399, Training Loss: 0.067372, Validation Loss: 0.059320\n",
      "Epoch: 5400, Training Loss: 0.068145, Validation Loss: 0.063081\n",
      "Epoch: 5401, Training Loss: 0.068888, Validation Loss: 0.058510\n",
      "Epoch: 5402, Training Loss: 0.068465, Validation Loss: 0.063232\n",
      "Epoch: 5403, Training Loss: 0.067933, Validation Loss: 0.058894\n",
      "Epoch: 5404, Training Loss: 0.067369, Validation Loss: 0.061468\n",
      "Epoch: 5405, Training Loss: 0.067218, Validation Loss: 0.059867\n",
      "Epoch: 5406, Training Loss: 0.067512, Validation Loss: 0.059009\n",
      "Epoch: 5407, Training Loss: 0.067975, Validation Loss: 0.063003\n",
      "Epoch: 5408, Training Loss: 0.068135, Validation Loss: 0.059267\n",
      "Epoch: 5409, Training Loss: 0.067635, Validation Loss: 0.061181\n",
      "Epoch: 5410, Training Loss: 0.067416, Validation Loss: 0.058224\n",
      "Epoch: 5411, Training Loss: 0.067028, Validation Loss: 0.060303\n",
      "Epoch: 5412, Training Loss: 0.067106, Validation Loss: 0.060921\n",
      "Epoch: 5413, Training Loss: 0.067141, Validation Loss: 0.059137\n",
      "Epoch: 5414, Training Loss: 0.067454, Validation Loss: 0.060999\n",
      "Epoch: 5415, Training Loss: 0.067320, Validation Loss: 0.058964\n",
      "Epoch: 5416, Training Loss: 0.067715, Validation Loss: 0.062849\n",
      "Epoch: 5417, Training Loss: 0.067531, Validation Loss: 0.059079\n",
      "Epoch: 5418, Training Loss: 0.067740, Validation Loss: 0.061774\n",
      "Epoch: 5419, Training Loss: 0.067898, Validation Loss: 0.058332\n",
      "Epoch: 5420, Training Loss: 0.068359, Validation Loss: 0.063788\n",
      "Epoch: 5421, Training Loss: 0.067991, Validation Loss: 0.059001\n",
      "Epoch: 5422, Training Loss: 0.067943, Validation Loss: 0.061902\n",
      "Epoch: 5423, Training Loss: 0.067788, Validation Loss: 0.058666\n",
      "Epoch: 5424, Training Loss: 0.067695, Validation Loss: 0.062690\n",
      "Epoch: 5425, Training Loss: 0.067524, Validation Loss: 0.058834\n",
      "Epoch: 5426, Training Loss: 0.067177, Validation Loss: 0.060294\n",
      "Epoch: 5427, Training Loss: 0.067019, Validation Loss: 0.058531\n",
      "Epoch: 5428, Training Loss: 0.066913, Validation Loss: 0.060585\n",
      "Epoch: 5429, Training Loss: 0.066892, Validation Loss: 0.060841\n",
      "Epoch: 5430, Training Loss: 0.066964, Validation Loss: 0.058840\n",
      "Epoch: 5431, Training Loss: 0.067324, Validation Loss: 0.061286\n",
      "Epoch: 5432, Training Loss: 0.068246, Validation Loss: 0.059027\n",
      "Epoch: 5433, Training Loss: 0.069305, Validation Loss: 0.065475\n",
      "Epoch: 5434, Training Loss: 0.070898, Validation Loss: 0.058470\n",
      "Epoch: 5435, Training Loss: 0.073760, Validation Loss: 0.068693\n",
      "Epoch: 5436, Training Loss: 0.076807, Validation Loss: 0.060038\n",
      "Epoch: 5437, Training Loss: 0.070886, Validation Loss: 0.066986\n",
      "Epoch: 5438, Training Loss: 0.067783, Validation Loss: 0.060058\n",
      "Epoch: 5439, Training Loss: 0.071398, Validation Loss: 0.057461\n",
      "Epoch: 5440, Training Loss: 0.070412, Validation Loss: 0.063195\n",
      "Epoch: 5441, Training Loss: 0.068181, Validation Loss: 0.060501\n",
      "Epoch: 5442, Training Loss: 0.068854, Validation Loss: 0.059302\n",
      "Epoch: 5443, Training Loss: 0.068994, Validation Loss: 0.062882\n",
      "Epoch: 5444, Training Loss: 0.067483, Validation Loss: 0.058947\n",
      "Epoch: 5445, Training Loss: 0.067334, Validation Loss: 0.058535\n",
      "Epoch: 5446, Training Loss: 0.068269, Validation Loss: 0.061280\n",
      "Epoch: 5447, Training Loss: 0.068096, Validation Loss: 0.057656\n",
      "Epoch: 5448, Training Loss: 0.068773, Validation Loss: 0.062544\n",
      "Epoch: 5449, Training Loss: 0.068550, Validation Loss: 0.059073\n",
      "Epoch: 5450, Training Loss: 0.068263, Validation Loss: 0.063316\n",
      "Epoch: 5451, Training Loss: 0.068637, Validation Loss: 0.059768\n",
      "Epoch: 5452, Training Loss: 0.068406, Validation Loss: 0.064056\n",
      "Epoch: 5453, Training Loss: 0.068452, Validation Loss: 0.057700\n",
      "Epoch: 5454, Training Loss: 0.068474, Validation Loss: 0.063465\n",
      "Epoch: 5455, Training Loss: 0.068052, Validation Loss: 0.058620\n",
      "Epoch: 5456, Training Loss: 0.067715, Validation Loss: 0.063063\n",
      "Epoch: 5457, Training Loss: 0.067156, Validation Loss: 0.059404\n",
      "Epoch: 5458, Training Loss: 0.066731, Validation Loss: 0.059850\n",
      "Epoch: 5459, Training Loss: 0.066806, Validation Loss: 0.059641\n",
      "Epoch: 5460, Training Loss: 0.066754, Validation Loss: 0.058937\n",
      "Epoch: 5461, Training Loss: 0.066795, Validation Loss: 0.059044\n",
      "Epoch: 5462, Training Loss: 0.066858, Validation Loss: 0.058527\n",
      "Epoch: 5463, Training Loss: 0.066701, Validation Loss: 0.060114\n",
      "Epoch: 5464, Training Loss: 0.066672, Validation Loss: 0.060033\n",
      "Epoch: 5465, Training Loss: 0.066581, Validation Loss: 0.059107\n",
      "Epoch: 5466, Training Loss: 0.066620, Validation Loss: 0.060029\n",
      "Epoch: 5467, Training Loss: 0.067005, Validation Loss: 0.059187\n",
      "Epoch: 5468, Training Loss: 0.067478, Validation Loss: 0.063100\n",
      "Epoch: 5469, Training Loss: 0.067937, Validation Loss: 0.058790\n",
      "Epoch: 5470, Training Loss: 0.068494, Validation Loss: 0.063635\n",
      "Epoch: 5471, Training Loss: 0.069947, Validation Loss: 0.057536\n",
      "Epoch: 5472, Training Loss: 0.072835, Validation Loss: 0.069631\n",
      "Epoch: 5473, Training Loss: 0.073979, Validation Loss: 0.059501\n",
      "Epoch: 5474, Training Loss: 0.070101, Validation Loss: 0.065126\n",
      "Epoch: 5475, Training Loss: 0.067263, Validation Loss: 0.058138\n",
      "Epoch: 5476, Training Loss: 0.068738, Validation Loss: 0.057043\n",
      "Epoch: 5477, Training Loss: 0.069737, Validation Loss: 0.064455\n",
      "Epoch: 5478, Training Loss: 0.068013, Validation Loss: 0.056932\n",
      "Epoch: 5479, Training Loss: 0.067386, Validation Loss: 0.056761\n",
      "Epoch: 5480, Training Loss: 0.069084, Validation Loss: 0.063046\n",
      "Epoch: 5481, Training Loss: 0.070208, Validation Loss: 0.058705\n",
      "Epoch: 5482, Training Loss: 0.068011, Validation Loss: 0.063158\n",
      "Epoch: 5483, Training Loss: 0.066538, Validation Loss: 0.058871\n",
      "Epoch: 5484, Training Loss: 0.066840, Validation Loss: 0.058402\n",
      "Epoch: 5485, Training Loss: 0.067740, Validation Loss: 0.062876\n",
      "Epoch: 5486, Training Loss: 0.069699, Validation Loss: 0.058251\n",
      "Epoch: 5487, Training Loss: 0.072101, Validation Loss: 0.068132\n",
      "Epoch: 5488, Training Loss: 0.074343, Validation Loss: 0.059300\n",
      "Epoch: 5489, Training Loss: 0.070417, Validation Loss: 0.066258\n",
      "Epoch: 5490, Training Loss: 0.067308, Validation Loss: 0.058559\n",
      "Epoch: 5491, Training Loss: 0.067505, Validation Loss: 0.058197\n",
      "Epoch: 5492, Training Loss: 0.069098, Validation Loss: 0.064378\n",
      "Epoch: 5493, Training Loss: 0.068125, Validation Loss: 0.058393\n",
      "Epoch: 5494, Training Loss: 0.066778, Validation Loss: 0.059720\n",
      "Epoch: 5495, Training Loss: 0.068199, Validation Loss: 0.063091\n",
      "Epoch: 5496, Training Loss: 0.068814, Validation Loss: 0.057785\n",
      "Epoch: 5497, Training Loss: 0.067147, Validation Loss: 0.060312\n",
      "Epoch: 5498, Training Loss: 0.066634, Validation Loss: 0.057775\n",
      "Epoch: 5499, Training Loss: 0.066943, Validation Loss: 0.056891\n",
      "Epoch: 5500, Training Loss: 0.068029, Validation Loss: 0.061731\n",
      "Epoch: 5501, Training Loss: 0.069229, Validation Loss: 0.058308\n",
      "Epoch: 5502, Training Loss: 0.068286, Validation Loss: 0.064285\n",
      "Epoch: 5503, Training Loss: 0.066898, Validation Loss: 0.059766\n",
      "Epoch: 5504, Training Loss: 0.066449, Validation Loss: 0.059779\n",
      "Epoch: 5505, Training Loss: 0.067157, Validation Loss: 0.061233\n",
      "Epoch: 5506, Training Loss: 0.067495, Validation Loss: 0.058341\n",
      "Epoch: 5507, Training Loss: 0.067097, Validation Loss: 0.062273\n",
      "Epoch: 5508, Training Loss: 0.066258, Validation Loss: 0.059300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5509, Training Loss: 0.066692, Validation Loss: 0.057876\n",
      "Epoch: 5510, Training Loss: 0.067377, Validation Loss: 0.061604\n",
      "Epoch: 5511, Training Loss: 0.067989, Validation Loss: 0.057041\n",
      "Epoch: 5512, Training Loss: 0.068302, Validation Loss: 0.062573\n",
      "Epoch: 5513, Training Loss: 0.068827, Validation Loss: 0.057318\n",
      "Epoch: 5514, Training Loss: 0.067785, Validation Loss: 0.062840\n",
      "Epoch: 5515, Training Loss: 0.066862, Validation Loss: 0.058497\n",
      "Epoch: 5516, Training Loss: 0.066249, Validation Loss: 0.059163\n",
      "Epoch: 5517, Training Loss: 0.066682, Validation Loss: 0.060974\n",
      "Epoch: 5518, Training Loss: 0.067284, Validation Loss: 0.057958\n",
      "Epoch: 5519, Training Loss: 0.067071, Validation Loss: 0.061317\n",
      "Epoch: 5520, Training Loss: 0.066784, Validation Loss: 0.057638\n",
      "Epoch: 5521, Training Loss: 0.066294, Validation Loss: 0.059323\n",
      "Epoch: 5522, Training Loss: 0.066162, Validation Loss: 0.058190\n",
      "Epoch: 5523, Training Loss: 0.066297, Validation Loss: 0.058644\n",
      "Epoch: 5524, Training Loss: 0.066501, Validation Loss: 0.060899\n",
      "Epoch: 5525, Training Loss: 0.066734, Validation Loss: 0.058025\n",
      "Epoch: 5526, Training Loss: 0.067004, Validation Loss: 0.061857\n",
      "Epoch: 5527, Training Loss: 0.067147, Validation Loss: 0.058796\n",
      "Epoch: 5528, Training Loss: 0.066944, Validation Loss: 0.062022\n",
      "Epoch: 5529, Training Loss: 0.066681, Validation Loss: 0.057621\n",
      "Epoch: 5530, Training Loss: 0.066212, Validation Loss: 0.058786\n",
      "Epoch: 5531, Training Loss: 0.066132, Validation Loss: 0.059262\n",
      "Epoch: 5532, Training Loss: 0.066265, Validation Loss: 0.058601\n",
      "Epoch: 5533, Training Loss: 0.066389, Validation Loss: 0.059840\n",
      "Epoch: 5534, Training Loss: 0.067231, Validation Loss: 0.057232\n",
      "Epoch: 5535, Training Loss: 0.067617, Validation Loss: 0.062756\n",
      "Epoch: 5536, Training Loss: 0.068442, Validation Loss: 0.057610\n",
      "Epoch: 5537, Training Loss: 0.068350, Validation Loss: 0.063551\n",
      "Epoch: 5538, Training Loss: 0.067975, Validation Loss: 0.057462\n",
      "Epoch: 5539, Training Loss: 0.066315, Validation Loss: 0.061224\n",
      "Epoch: 5540, Training Loss: 0.066121, Validation Loss: 0.060470\n",
      "Epoch: 5541, Training Loss: 0.066350, Validation Loss: 0.057692\n",
      "Epoch: 5542, Training Loss: 0.066941, Validation Loss: 0.061262\n",
      "Epoch: 5543, Training Loss: 0.067093, Validation Loss: 0.058020\n",
      "Epoch: 5544, Training Loss: 0.067070, Validation Loss: 0.061700\n",
      "Epoch: 5545, Training Loss: 0.067665, Validation Loss: 0.057456\n",
      "Epoch: 5546, Training Loss: 0.067608, Validation Loss: 0.062634\n",
      "Epoch: 5547, Training Loss: 0.067452, Validation Loss: 0.057317\n",
      "Epoch: 5548, Training Loss: 0.066526, Validation Loss: 0.061113\n",
      "Epoch: 5549, Training Loss: 0.065942, Validation Loss: 0.058749\n",
      "Epoch: 5550, Training Loss: 0.065832, Validation Loss: 0.059885\n",
      "Epoch: 5551, Training Loss: 0.065780, Validation Loss: 0.059958\n",
      "Epoch: 5552, Training Loss: 0.065763, Validation Loss: 0.058759\n",
      "Epoch: 5553, Training Loss: 0.065900, Validation Loss: 0.060108\n",
      "Epoch: 5554, Training Loss: 0.066464, Validation Loss: 0.058253\n",
      "Epoch: 5555, Training Loss: 0.067072, Validation Loss: 0.062153\n",
      "Epoch: 5556, Training Loss: 0.070379, Validation Loss: 0.057030\n",
      "Epoch: 5557, Training Loss: 0.074260, Validation Loss: 0.070605\n",
      "Epoch: 5558, Training Loss: 0.080437, Validation Loss: 0.059761\n",
      "Epoch: 5559, Training Loss: 0.068770, Validation Loss: 0.065268\n",
      "Epoch: 5560, Training Loss: 0.070038, Validation Loss: 0.065645\n",
      "Epoch: 5561, Training Loss: 0.076723, Validation Loss: 0.058402\n",
      "Epoch: 5562, Training Loss: 0.070194, Validation Loss: 0.065520\n",
      "Epoch: 5563, Training Loss: 0.072430, Validation Loss: 0.066892\n",
      "Epoch: 5564, Training Loss: 0.075492, Validation Loss: 0.060846\n",
      "Epoch: 5565, Training Loss: 0.072396, Validation Loss: 0.064688\n",
      "Epoch: 5566, Training Loss: 0.072194, Validation Loss: 0.065880\n",
      "Epoch: 5567, Training Loss: 0.073509, Validation Loss: 0.061728\n",
      "Epoch: 5568, Training Loss: 0.073002, Validation Loss: 0.064035\n",
      "Epoch: 5569, Training Loss: 0.069919, Validation Loss: 0.060876\n",
      "Epoch: 5570, Training Loss: 0.075644, Validation Loss: 0.063301\n",
      "Epoch: 5571, Training Loss: 0.073577, Validation Loss: 0.067680\n",
      "Epoch: 5572, Training Loss: 0.066465, Validation Loss: 0.057987\n",
      "Epoch: 5573, Training Loss: 0.078476, Validation Loss: 0.067689\n",
      "Epoch: 5574, Training Loss: 0.087655, Validation Loss: 0.084383\n",
      "Epoch: 5575, Training Loss: 0.083450, Validation Loss: 0.061573\n",
      "Epoch: 5576, Training Loss: 0.072716, Validation Loss: 0.068700\n",
      "Epoch: 5577, Training Loss: 0.082564, Validation Loss: 0.078453\n",
      "Epoch: 5578, Training Loss: 0.085475, Validation Loss: 0.060919\n",
      "Epoch: 5579, Training Loss: 0.073901, Validation Loss: 0.055730\n",
      "Epoch: 5580, Training Loss: 0.089710, Validation Loss: 0.087480\n",
      "Epoch: 5581, Training Loss: 0.069197, Validation Loss: 0.059210\n",
      "Epoch: 5582, Training Loss: 0.082630, Validation Loss: 0.062505\n",
      "Epoch: 5583, Training Loss: 0.071203, Validation Loss: 0.059519\n",
      "Epoch: 5584, Training Loss: 0.078555, Validation Loss: 0.067999\n",
      "Epoch: 5585, Training Loss: 0.072248, Validation Loss: 0.060635\n",
      "Epoch: 5586, Training Loss: 0.076235, Validation Loss: 0.061575\n",
      "Epoch: 5587, Training Loss: 0.071564, Validation Loss: 0.059715\n",
      "Epoch: 5588, Training Loss: 0.074065, Validation Loss: 0.062409\n",
      "Epoch: 5589, Training Loss: 0.069978, Validation Loss: 0.058944\n",
      "Epoch: 5590, Training Loss: 0.072137, Validation Loss: 0.057720\n",
      "Epoch: 5591, Training Loss: 0.069146, Validation Loss: 0.056028\n",
      "Epoch: 5592, Training Loss: 0.072125, Validation Loss: 0.064223\n",
      "Epoch: 5593, Training Loss: 0.066356, Validation Loss: 0.057130\n",
      "Epoch: 5594, Training Loss: 0.070385, Validation Loss: 0.058067\n",
      "Epoch: 5595, Training Loss: 0.068528, Validation Loss: 0.063287\n",
      "Epoch: 5596, Training Loss: 0.067344, Validation Loss: 0.062139\n",
      "Epoch: 5597, Training Loss: 0.067583, Validation Loss: 0.059551\n",
      "Epoch: 5598, Training Loss: 0.067619, Validation Loss: 0.061265\n",
      "Epoch: 5599, Training Loss: 0.067377, Validation Loss: 0.059353\n",
      "Epoch: 5600, Training Loss: 0.066303, Validation Loss: 0.058681\n",
      "Epoch: 5601, Training Loss: 0.067316, Validation Loss: 0.060963\n",
      "Epoch: 5602, Training Loss: 0.066910, Validation Loss: 0.057916\n",
      "Epoch: 5603, Training Loss: 0.066885, Validation Loss: 0.060678\n",
      "Epoch: 5604, Training Loss: 0.066554, Validation Loss: 0.059106\n",
      "Epoch: 5605, Training Loss: 0.066330, Validation Loss: 0.058526\n",
      "Epoch: 5606, Training Loss: 0.067119, Validation Loss: 0.059870\n",
      "Epoch: 5607, Training Loss: 0.066451, Validation Loss: 0.056919\n",
      "Epoch: 5608, Training Loss: 0.066240, Validation Loss: 0.058629\n",
      "Epoch: 5609, Training Loss: 0.066155, Validation Loss: 0.059623\n",
      "Epoch: 5610, Training Loss: 0.066286, Validation Loss: 0.057924\n",
      "Epoch: 5611, Training Loss: 0.066430, Validation Loss: 0.058834\n",
      "Epoch: 5612, Training Loss: 0.065863, Validation Loss: 0.057335\n",
      "Epoch: 5613, Training Loss: 0.066016, Validation Loss: 0.058752\n",
      "Epoch: 5614, Training Loss: 0.065861, Validation Loss: 0.059186\n",
      "Epoch: 5615, Training Loss: 0.066001, Validation Loss: 0.057838\n",
      "Epoch: 5616, Training Loss: 0.065824, Validation Loss: 0.058838\n",
      "Epoch: 5617, Training Loss: 0.065742, Validation Loss: 0.059190\n",
      "Epoch: 5618, Training Loss: 0.065899, Validation Loss: 0.057542\n",
      "Epoch: 5619, Training Loss: 0.065689, Validation Loss: 0.058048\n",
      "Epoch: 5620, Training Loss: 0.065827, Validation Loss: 0.059353\n",
      "Epoch: 5621, Training Loss: 0.066011, Validation Loss: 0.058010\n",
      "Epoch: 5622, Training Loss: 0.065746, Validation Loss: 0.060057\n",
      "Epoch: 5623, Training Loss: 0.065719, Validation Loss: 0.059551\n",
      "Epoch: 5624, Training Loss: 0.065813, Validation Loss: 0.058230\n",
      "Epoch: 5625, Training Loss: 0.065912, Validation Loss: 0.060435\n",
      "Epoch: 5626, Training Loss: 0.065682, Validation Loss: 0.058582\n",
      "Epoch: 5627, Training Loss: 0.065531, Validation Loss: 0.059426\n",
      "Epoch: 5628, Training Loss: 0.065538, Validation Loss: 0.059219\n",
      "Epoch: 5629, Training Loss: 0.065841, Validation Loss: 0.057791\n",
      "Epoch: 5630, Training Loss: 0.066035, Validation Loss: 0.060716\n",
      "Epoch: 5631, Training Loss: 0.065525, Validation Loss: 0.058053\n",
      "Epoch: 5632, Training Loss: 0.065663, Validation Loss: 0.058141\n",
      "Epoch: 5633, Training Loss: 0.066226, Validation Loss: 0.061122\n",
      "Epoch: 5634, Training Loss: 0.066477, Validation Loss: 0.057767\n",
      "Epoch: 5635, Training Loss: 0.065791, Validation Loss: 0.060322\n",
      "Epoch: 5636, Training Loss: 0.065544, Validation Loss: 0.059728\n",
      "Epoch: 5637, Training Loss: 0.066282, Validation Loss: 0.057989\n",
      "Epoch: 5638, Training Loss: 0.066566, Validation Loss: 0.061355\n",
      "Epoch: 5639, Training Loss: 0.066667, Validation Loss: 0.056701\n",
      "Epoch: 5640, Training Loss: 0.065946, Validation Loss: 0.060375\n",
      "Epoch: 5641, Training Loss: 0.065333, Validation Loss: 0.059380\n",
      "Epoch: 5642, Training Loss: 0.065938, Validation Loss: 0.058449\n",
      "Epoch: 5643, Training Loss: 0.066222, Validation Loss: 0.061439\n",
      "Epoch: 5644, Training Loss: 0.066522, Validation Loss: 0.057368\n",
      "Epoch: 5645, Training Loss: 0.065881, Validation Loss: 0.060394\n",
      "Epoch: 5646, Training Loss: 0.065222, Validation Loss: 0.058354\n",
      "Epoch: 5647, Training Loss: 0.065551, Validation Loss: 0.058241\n",
      "Epoch: 5648, Training Loss: 0.066242, Validation Loss: 0.061902\n",
      "Epoch: 5649, Training Loss: 0.066453, Validation Loss: 0.058176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5650, Training Loss: 0.065824, Validation Loss: 0.061141\n",
      "Epoch: 5651, Training Loss: 0.065219, Validation Loss: 0.058731\n",
      "Epoch: 5652, Training Loss: 0.065419, Validation Loss: 0.057877\n",
      "Epoch: 5653, Training Loss: 0.065843, Validation Loss: 0.060888\n",
      "Epoch: 5654, Training Loss: 0.065716, Validation Loss: 0.058469\n",
      "Epoch: 5655, Training Loss: 0.065314, Validation Loss: 0.060487\n",
      "Epoch: 5656, Training Loss: 0.065302, Validation Loss: 0.060152\n",
      "Epoch: 5657, Training Loss: 0.065728, Validation Loss: 0.058494\n",
      "Epoch: 5658, Training Loss: 0.065402, Validation Loss: 0.060698\n",
      "Epoch: 5659, Training Loss: 0.065135, Validation Loss: 0.059778\n",
      "Epoch: 5660, Training Loss: 0.065479, Validation Loss: 0.058100\n",
      "Epoch: 5661, Training Loss: 0.066055, Validation Loss: 0.061324\n",
      "Epoch: 5662, Training Loss: 0.066414, Validation Loss: 0.057490\n",
      "Epoch: 5663, Training Loss: 0.066055, Validation Loss: 0.062299\n",
      "Epoch: 5664, Training Loss: 0.065469, Validation Loss: 0.058082\n",
      "Epoch: 5665, Training Loss: 0.065141, Validation Loss: 0.059128\n",
      "Epoch: 5666, Training Loss: 0.065527, Validation Loss: 0.061565\n",
      "Epoch: 5667, Training Loss: 0.065935, Validation Loss: 0.058228\n",
      "Epoch: 5668, Training Loss: 0.066167, Validation Loss: 0.061156\n",
      "Epoch: 5669, Training Loss: 0.066720, Validation Loss: 0.056880\n",
      "Epoch: 5670, Training Loss: 0.066835, Validation Loss: 0.063020\n",
      "Epoch: 5671, Training Loss: 0.066733, Validation Loss: 0.056753\n",
      "Epoch: 5672, Training Loss: 0.066192, Validation Loss: 0.060776\n",
      "Epoch: 5673, Training Loss: 0.065695, Validation Loss: 0.058487\n",
      "Epoch: 5674, Training Loss: 0.065454, Validation Loss: 0.061363\n",
      "Epoch: 5675, Training Loss: 0.065167, Validation Loss: 0.058891\n",
      "Epoch: 5676, Training Loss: 0.065782, Validation Loss: 0.057343\n",
      "Epoch: 5677, Training Loss: 0.065516, Validation Loss: 0.061234\n",
      "Epoch: 5678, Training Loss: 0.065366, Validation Loss: 0.058789\n",
      "Epoch: 5679, Training Loss: 0.065043, Validation Loss: 0.059590\n",
      "Epoch: 5680, Training Loss: 0.065106, Validation Loss: 0.059304\n",
      "Epoch: 5681, Training Loss: 0.065083, Validation Loss: 0.059056\n",
      "Epoch: 5682, Training Loss: 0.065364, Validation Loss: 0.061874\n",
      "Epoch: 5683, Training Loss: 0.065079, Validation Loss: 0.058880\n",
      "Epoch: 5684, Training Loss: 0.065167, Validation Loss: 0.060026\n",
      "Epoch: 5685, Training Loss: 0.065003, Validation Loss: 0.058497\n",
      "Epoch: 5686, Training Loss: 0.065090, Validation Loss: 0.060648\n",
      "Epoch: 5687, Training Loss: 0.065338, Validation Loss: 0.058474\n",
      "Epoch: 5688, Training Loss: 0.065949, Validation Loss: 0.061546\n",
      "Epoch: 5689, Training Loss: 0.067607, Validation Loss: 0.056897\n",
      "Epoch: 5690, Training Loss: 0.066651, Validation Loss: 0.063442\n",
      "Epoch: 5691, Training Loss: 0.065473, Validation Loss: 0.057971\n",
      "Epoch: 5692, Training Loss: 0.065074, Validation Loss: 0.059457\n",
      "Epoch: 5693, Training Loss: 0.065483, Validation Loss: 0.061894\n",
      "Epoch: 5694, Training Loss: 0.067242, Validation Loss: 0.057706\n",
      "Epoch: 5695, Training Loss: 0.068808, Validation Loss: 0.062480\n",
      "Epoch: 5696, Training Loss: 0.070096, Validation Loss: 0.056279\n",
      "Epoch: 5697, Training Loss: 0.065917, Validation Loss: 0.060759\n",
      "Epoch: 5698, Training Loss: 0.065402, Validation Loss: 0.060711\n",
      "Epoch: 5699, Training Loss: 0.066168, Validation Loss: 0.057743\n",
      "Epoch: 5700, Training Loss: 0.066453, Validation Loss: 0.061230\n",
      "Epoch: 5701, Training Loss: 0.065690, Validation Loss: 0.058379\n",
      "Epoch: 5702, Training Loss: 0.065104, Validation Loss: 0.060562\n",
      "Epoch: 5703, Training Loss: 0.065150, Validation Loss: 0.059995\n",
      "Epoch: 5704, Training Loss: 0.065831, Validation Loss: 0.058036\n",
      "Epoch: 5705, Training Loss: 0.066395, Validation Loss: 0.062000\n",
      "Epoch: 5706, Training Loss: 0.066828, Validation Loss: 0.056862\n",
      "Epoch: 5707, Training Loss: 0.066157, Validation Loss: 0.061473\n",
      "Epoch: 5708, Training Loss: 0.064959, Validation Loss: 0.058410\n",
      "Epoch: 5709, Training Loss: 0.064954, Validation Loss: 0.059196\n",
      "Epoch: 5710, Training Loss: 0.065533, Validation Loss: 0.061302\n",
      "Epoch: 5711, Training Loss: 0.065614, Validation Loss: 0.058468\n",
      "Epoch: 5712, Training Loss: 0.065923, Validation Loss: 0.060836\n",
      "Epoch: 5713, Training Loss: 0.066774, Validation Loss: 0.057230\n",
      "Epoch: 5714, Training Loss: 0.066297, Validation Loss: 0.061789\n",
      "Epoch: 5715, Training Loss: 0.065769, Validation Loss: 0.056867\n",
      "Epoch: 5716, Training Loss: 0.065172, Validation Loss: 0.059549\n",
      "Epoch: 5717, Training Loss: 0.065018, Validation Loss: 0.060718\n",
      "Epoch: 5718, Training Loss: 0.065495, Validation Loss: 0.059562\n",
      "Epoch: 5719, Training Loss: 0.065454, Validation Loss: 0.060233\n",
      "Epoch: 5720, Training Loss: 0.065315, Validation Loss: 0.057661\n",
      "Epoch: 5721, Training Loss: 0.064974, Validation Loss: 0.060114\n",
      "Epoch: 5722, Training Loss: 0.064806, Validation Loss: 0.059024\n",
      "Epoch: 5723, Training Loss: 0.064933, Validation Loss: 0.057648\n",
      "Epoch: 5724, Training Loss: 0.065394, Validation Loss: 0.060206\n",
      "Epoch: 5725, Training Loss: 0.065963, Validation Loss: 0.058564\n",
      "Epoch: 5726, Training Loss: 0.067330, Validation Loss: 0.063998\n",
      "Epoch: 5727, Training Loss: 0.070000, Validation Loss: 0.058009\n",
      "Epoch: 5728, Training Loss: 0.068375, Validation Loss: 0.063301\n",
      "Epoch: 5729, Training Loss: 0.066046, Validation Loss: 0.057863\n",
      "Epoch: 5730, Training Loss: 0.065363, Validation Loss: 0.060135\n",
      "Epoch: 5731, Training Loss: 0.066302, Validation Loss: 0.062280\n",
      "Epoch: 5732, Training Loss: 0.068177, Validation Loss: 0.057513\n",
      "Epoch: 5733, Training Loss: 0.067018, Validation Loss: 0.061104\n",
      "Epoch: 5734, Training Loss: 0.065334, Validation Loss: 0.059734\n",
      "Epoch: 5735, Training Loss: 0.064854, Validation Loss: 0.059290\n",
      "Epoch: 5736, Training Loss: 0.065959, Validation Loss: 0.060032\n",
      "Epoch: 5737, Training Loss: 0.067301, Validation Loss: 0.056085\n",
      "Epoch: 5738, Training Loss: 0.067319, Validation Loss: 0.062823\n",
      "Epoch: 5739, Training Loss: 0.066442, Validation Loss: 0.057675\n",
      "Epoch: 5740, Training Loss: 0.064981, Validation Loss: 0.060167\n",
      "Epoch: 5741, Training Loss: 0.065039, Validation Loss: 0.060514\n",
      "Epoch: 5742, Training Loss: 0.066451, Validation Loss: 0.057288\n",
      "Epoch: 5743, Training Loss: 0.066940, Validation Loss: 0.062095\n",
      "Epoch: 5744, Training Loss: 0.067537, Validation Loss: 0.057287\n",
      "Epoch: 5745, Training Loss: 0.065749, Validation Loss: 0.061880\n",
      "Epoch: 5746, Training Loss: 0.064762, Validation Loss: 0.057891\n",
      "Epoch: 5747, Training Loss: 0.064950, Validation Loss: 0.057372\n",
      "Epoch: 5748, Training Loss: 0.067260, Validation Loss: 0.063272\n",
      "Epoch: 5749, Training Loss: 0.070830, Validation Loss: 0.058144\n",
      "Epoch: 5750, Training Loss: 0.067856, Validation Loss: 0.063354\n",
      "Epoch: 5751, Training Loss: 0.065239, Validation Loss: 0.059564\n",
      "Epoch: 5752, Training Loss: 0.065361, Validation Loss: 0.058837\n",
      "Epoch: 5753, Training Loss: 0.067813, Validation Loss: 0.062227\n",
      "Epoch: 5754, Training Loss: 0.070987, Validation Loss: 0.056700\n",
      "Epoch: 5755, Training Loss: 0.066274, Validation Loss: 0.062145\n",
      "Epoch: 5756, Training Loss: 0.064790, Validation Loss: 0.059657\n",
      "Epoch: 5757, Training Loss: 0.065686, Validation Loss: 0.056622\n",
      "Epoch: 5758, Training Loss: 0.066632, Validation Loss: 0.060808\n",
      "Epoch: 5759, Training Loss: 0.067389, Validation Loss: 0.057499\n",
      "Epoch: 5760, Training Loss: 0.065782, Validation Loss: 0.061714\n",
      "Epoch: 5761, Training Loss: 0.065124, Validation Loss: 0.057788\n",
      "Epoch: 5762, Training Loss: 0.064654, Validation Loss: 0.058197\n",
      "Epoch: 5763, Training Loss: 0.065454, Validation Loss: 0.061822\n",
      "Epoch: 5764, Training Loss: 0.065614, Validation Loss: 0.057211\n",
      "Epoch: 5765, Training Loss: 0.066846, Validation Loss: 0.061053\n",
      "Epoch: 5766, Training Loss: 0.068290, Validation Loss: 0.058070\n",
      "Epoch: 5767, Training Loss: 0.065640, Validation Loss: 0.062403\n",
      "Epoch: 5768, Training Loss: 0.065209, Validation Loss: 0.058852\n",
      "Epoch: 5769, Training Loss: 0.066304, Validation Loss: 0.056993\n",
      "Epoch: 5770, Training Loss: 0.066635, Validation Loss: 0.064317\n",
      "Epoch: 5771, Training Loss: 0.065895, Validation Loss: 0.058329\n",
      "Epoch: 5772, Training Loss: 0.064913, Validation Loss: 0.059846\n",
      "Epoch: 5773, Training Loss: 0.064618, Validation Loss: 0.058875\n",
      "Epoch: 5774, Training Loss: 0.064549, Validation Loss: 0.059080\n",
      "Epoch: 5775, Training Loss: 0.064808, Validation Loss: 0.059383\n",
      "Epoch: 5776, Training Loss: 0.065433, Validation Loss: 0.058473\n",
      "Epoch: 5777, Training Loss: 0.065335, Validation Loss: 0.060799\n",
      "Epoch: 5778, Training Loss: 0.065542, Validation Loss: 0.057735\n",
      "Epoch: 5779, Training Loss: 0.065440, Validation Loss: 0.061362\n",
      "Epoch: 5780, Training Loss: 0.065223, Validation Loss: 0.057949\n",
      "Epoch: 5781, Training Loss: 0.064879, Validation Loss: 0.062043\n",
      "Epoch: 5782, Training Loss: 0.064531, Validation Loss: 0.060910\n",
      "Epoch: 5783, Training Loss: 0.064554, Validation Loss: 0.058919\n",
      "Epoch: 5784, Training Loss: 0.064841, Validation Loss: 0.060408\n",
      "Epoch: 5785, Training Loss: 0.065064, Validation Loss: 0.058304\n",
      "Epoch: 5786, Training Loss: 0.066068, Validation Loss: 0.062224\n",
      "Epoch: 5787, Training Loss: 0.068139, Validation Loss: 0.057889\n",
      "Epoch: 5788, Training Loss: 0.065475, Validation Loss: 0.062840\n",
      "Epoch: 5789, Training Loss: 0.064661, Validation Loss: 0.059919\n",
      "Epoch: 5790, Training Loss: 0.065903, Validation Loss: 0.057434\n",
      "Epoch: 5791, Training Loss: 0.067817, Validation Loss: 0.063811\n",
      "Epoch: 5792, Training Loss: 0.068247, Validation Loss: 0.058960\n",
      "Epoch: 5793, Training Loss: 0.065718, Validation Loss: 0.061290\n",
      "Epoch: 5794, Training Loss: 0.064453, Validation Loss: 0.059722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5795, Training Loss: 0.066146, Validation Loss: 0.056772\n",
      "Epoch: 5796, Training Loss: 0.068081, Validation Loss: 0.062443\n",
      "Epoch: 5797, Training Loss: 0.067978, Validation Loss: 0.056949\n",
      "Epoch: 5798, Training Loss: 0.065065, Validation Loss: 0.060655\n",
      "Epoch: 5799, Training Loss: 0.065437, Validation Loss: 0.061265\n",
      "Epoch: 5800, Training Loss: 0.067718, Validation Loss: 0.056157\n",
      "Epoch: 5801, Training Loss: 0.066692, Validation Loss: 0.061989\n",
      "Epoch: 5802, Training Loss: 0.064779, Validation Loss: 0.058565\n",
      "Epoch: 5803, Training Loss: 0.064839, Validation Loss: 0.060222\n",
      "Epoch: 5804, Training Loss: 0.066072, Validation Loss: 0.063170\n",
      "Epoch: 5805, Training Loss: 0.066825, Validation Loss: 0.056535\n",
      "Epoch: 5806, Training Loss: 0.066584, Validation Loss: 0.060095\n",
      "Epoch: 5807, Training Loss: 0.065254, Validation Loss: 0.058124\n",
      "Epoch: 5808, Training Loss: 0.064609, Validation Loss: 0.061193\n",
      "Epoch: 5809, Training Loss: 0.065304, Validation Loss: 0.060763\n",
      "Epoch: 5810, Training Loss: 0.065614, Validation Loss: 0.058336\n",
      "Epoch: 5811, Training Loss: 0.065754, Validation Loss: 0.062648\n",
      "Epoch: 5812, Training Loss: 0.064610, Validation Loss: 0.059275\n",
      "Epoch: 5813, Training Loss: 0.065223, Validation Loss: 0.058574\n",
      "Epoch: 5814, Training Loss: 0.066409, Validation Loss: 0.063805\n",
      "Epoch: 5815, Training Loss: 0.068031, Validation Loss: 0.058073\n",
      "Epoch: 5816, Training Loss: 0.068569, Validation Loss: 0.061722\n",
      "Epoch: 5817, Training Loss: 0.068542, Validation Loss: 0.056128\n",
      "Epoch: 5818, Training Loss: 0.064820, Validation Loss: 0.061292\n",
      "Epoch: 5819, Training Loss: 0.066234, Validation Loss: 0.062624\n",
      "Epoch: 5820, Training Loss: 0.069147, Validation Loss: 0.056906\n",
      "Epoch: 5821, Training Loss: 0.066246, Validation Loss: 0.061894\n",
      "Epoch: 5822, Training Loss: 0.065406, Validation Loss: 0.061439\n",
      "Epoch: 5823, Training Loss: 0.066973, Validation Loss: 0.056841\n",
      "Epoch: 5824, Training Loss: 0.067406, Validation Loss: 0.060992\n",
      "Epoch: 5825, Training Loss: 0.065265, Validation Loss: 0.057716\n",
      "Epoch: 5826, Training Loss: 0.064862, Validation Loss: 0.060157\n",
      "Epoch: 5827, Training Loss: 0.064992, Validation Loss: 0.060920\n",
      "Epoch: 5828, Training Loss: 0.065024, Validation Loss: 0.057431\n",
      "Epoch: 5829, Training Loss: 0.065155, Validation Loss: 0.060233\n",
      "Epoch: 5830, Training Loss: 0.064522, Validation Loss: 0.058681\n",
      "Epoch: 5831, Training Loss: 0.064399, Validation Loss: 0.060568\n",
      "Epoch: 5832, Training Loss: 0.064440, Validation Loss: 0.060267\n",
      "Epoch: 5833, Training Loss: 0.064146, Validation Loss: 0.059334\n",
      "Epoch: 5834, Training Loss: 0.064418, Validation Loss: 0.060200\n",
      "Epoch: 5835, Training Loss: 0.064511, Validation Loss: 0.059263\n",
      "Epoch: 5836, Training Loss: 0.065799, Validation Loss: 0.063232\n",
      "Epoch: 5837, Training Loss: 0.070087, Validation Loss: 0.058125\n",
      "Epoch: 5838, Training Loss: 0.071566, Validation Loss: 0.067288\n",
      "Epoch: 5839, Training Loss: 0.070566, Validation Loss: 0.058520\n",
      "Epoch: 5840, Training Loss: 0.065890, Validation Loss: 0.061562\n",
      "Epoch: 5841, Training Loss: 0.066509, Validation Loss: 0.063487\n",
      "Epoch: 5842, Training Loss: 0.069051, Validation Loss: 0.059213\n",
      "Epoch: 5843, Training Loss: 0.066318, Validation Loss: 0.061931\n",
      "Epoch: 5844, Training Loss: 0.065832, Validation Loss: 0.061205\n",
      "Epoch: 5845, Training Loss: 0.067663, Validation Loss: 0.060097\n",
      "Epoch: 5846, Training Loss: 0.065810, Validation Loss: 0.061496\n",
      "Epoch: 5847, Training Loss: 0.064659, Validation Loss: 0.059428\n",
      "Epoch: 5848, Training Loss: 0.066957, Validation Loss: 0.057288\n",
      "Epoch: 5849, Training Loss: 0.067142, Validation Loss: 0.061618\n",
      "Epoch: 5850, Training Loss: 0.067963, Validation Loss: 0.058597\n",
      "Epoch: 5851, Training Loss: 0.064718, Validation Loss: 0.060462\n",
      "Epoch: 5852, Training Loss: 0.066598, Validation Loss: 0.060647\n",
      "Epoch: 5853, Training Loss: 0.069985, Validation Loss: 0.057939\n",
      "Epoch: 5854, Training Loss: 0.066384, Validation Loss: 0.065198\n",
      "Epoch: 5855, Training Loss: 0.065969, Validation Loss: 0.060658\n",
      "Epoch: 5856, Training Loss: 0.066883, Validation Loss: 0.058315\n",
      "Epoch: 5857, Training Loss: 0.065526, Validation Loss: 0.062169\n",
      "Epoch: 5858, Training Loss: 0.065413, Validation Loss: 0.061407\n",
      "Epoch: 5859, Training Loss: 0.065656, Validation Loss: 0.056982\n",
      "Epoch: 5860, Training Loss: 0.067599, Validation Loss: 0.061551\n",
      "Epoch: 5861, Training Loss: 0.066519, Validation Loss: 0.058299\n",
      "Epoch: 5862, Training Loss: 0.065237, Validation Loss: 0.059107\n",
      "Epoch: 5863, Training Loss: 0.064834, Validation Loss: 0.058804\n",
      "Epoch: 5864, Training Loss: 0.065261, Validation Loss: 0.058436\n",
      "Epoch: 5865, Training Loss: 0.067345, Validation Loss: 0.062101\n",
      "Epoch: 5866, Training Loss: 0.066321, Validation Loss: 0.058031\n",
      "Epoch: 5867, Training Loss: 0.064895, Validation Loss: 0.060322\n",
      "Epoch: 5868, Training Loss: 0.065731, Validation Loss: 0.062234\n",
      "Epoch: 5869, Training Loss: 0.066725, Validation Loss: 0.057210\n",
      "Epoch: 5870, Training Loss: 0.067164, Validation Loss: 0.062206\n",
      "Epoch: 5871, Training Loss: 0.065140, Validation Loss: 0.059011\n",
      "Epoch: 5872, Training Loss: 0.064894, Validation Loss: 0.060166\n",
      "Epoch: 5873, Training Loss: 0.064472, Validation Loss: 0.061032\n",
      "Epoch: 5874, Training Loss: 0.065009, Validation Loss: 0.058364\n",
      "Epoch: 5875, Training Loss: 0.067000, Validation Loss: 0.060676\n",
      "Epoch: 5876, Training Loss: 0.067524, Validation Loss: 0.057195\n",
      "Epoch: 5877, Training Loss: 0.065996, Validation Loss: 0.063123\n",
      "Epoch: 5878, Training Loss: 0.070164, Validation Loss: 0.062734\n",
      "Epoch: 5879, Training Loss: 0.068004, Validation Loss: 0.057330\n",
      "Epoch: 5880, Training Loss: 0.068262, Validation Loss: 0.065778\n",
      "Epoch: 5881, Training Loss: 0.067860, Validation Loss: 0.059041\n",
      "Epoch: 5882, Training Loss: 0.066410, Validation Loss: 0.058521\n",
      "Epoch: 5883, Training Loss: 0.070305, Validation Loss: 0.067576\n",
      "Epoch: 5884, Training Loss: 0.065496, Validation Loss: 0.056890\n",
      "Epoch: 5885, Training Loss: 0.066146, Validation Loss: 0.057656\n",
      "Epoch: 5886, Training Loss: 0.068329, Validation Loss: 0.066086\n",
      "Epoch: 5887, Training Loss: 0.065958, Validation Loss: 0.057986\n",
      "Epoch: 5888, Training Loss: 0.066752, Validation Loss: 0.061351\n",
      "Epoch: 5889, Training Loss: 0.065318, Validation Loss: 0.063320\n",
      "Epoch: 5890, Training Loss: 0.064690, Validation Loss: 0.060188\n",
      "Epoch: 5891, Training Loss: 0.066784, Validation Loss: 0.060363\n",
      "Epoch: 5892, Training Loss: 0.067845, Validation Loss: 0.058152\n",
      "Epoch: 5893, Training Loss: 0.067542, Validation Loss: 0.066574\n",
      "Epoch: 5894, Training Loss: 0.066799, Validation Loss: 0.059109\n",
      "Epoch: 5895, Training Loss: 0.064944, Validation Loss: 0.058302\n",
      "Epoch: 5896, Training Loss: 0.069114, Validation Loss: 0.068206\n",
      "Epoch: 5897, Training Loss: 0.066893, Validation Loss: 0.058734\n",
      "Epoch: 5898, Training Loss: 0.069323, Validation Loss: 0.057351\n",
      "Epoch: 5899, Training Loss: 0.067036, Validation Loss: 0.059843\n",
      "Epoch: 5900, Training Loss: 0.065762, Validation Loss: 0.062932\n",
      "Epoch: 5901, Training Loss: 0.068106, Validation Loss: 0.057475\n",
      "Epoch: 5902, Training Loss: 0.067595, Validation Loss: 0.055485\n",
      "Epoch: 5903, Training Loss: 0.066064, Validation Loss: 0.063291\n",
      "Epoch: 5904, Training Loss: 0.066645, Validation Loss: 0.063434\n",
      "Epoch: 5905, Training Loss: 0.066602, Validation Loss: 0.058366\n",
      "Epoch: 5906, Training Loss: 0.066538, Validation Loss: 0.061418\n",
      "Epoch: 5907, Training Loss: 0.065947, Validation Loss: 0.062238\n",
      "Epoch: 5908, Training Loss: 0.067335, Validation Loss: 0.058386\n",
      "Epoch: 5909, Training Loss: 0.069699, Validation Loss: 0.061596\n",
      "Epoch: 5910, Training Loss: 0.068167, Validation Loss: 0.058446\n",
      "Epoch: 5911, Training Loss: 0.066122, Validation Loss: 0.063293\n",
      "Epoch: 5912, Training Loss: 0.072725, Validation Loss: 0.062621\n",
      "Epoch: 5913, Training Loss: 0.072351, Validation Loss: 0.057078\n",
      "Epoch: 5914, Training Loss: 0.065888, Validation Loss: 0.062742\n",
      "Epoch: 5915, Training Loss: 0.072000, Validation Loss: 0.063799\n",
      "Epoch: 5916, Training Loss: 0.069359, Validation Loss: 0.058568\n",
      "Epoch: 5917, Training Loss: 0.068472, Validation Loss: 0.059015\n",
      "Epoch: 5918, Training Loss: 0.069611, Validation Loss: 0.065964\n",
      "Epoch: 5919, Training Loss: 0.067765, Validation Loss: 0.059577\n",
      "Epoch: 5920, Training Loss: 0.068279, Validation Loss: 0.054574\n",
      "Epoch: 5921, Training Loss: 0.067127, Validation Loss: 0.057832\n",
      "Epoch: 5922, Training Loss: 0.067641, Validation Loss: 0.059978\n",
      "Epoch: 5923, Training Loss: 0.067027, Validation Loss: 0.060303\n",
      "Epoch: 5924, Training Loss: 0.068430, Validation Loss: 0.061200\n",
      "Epoch: 5925, Training Loss: 0.065307, Validation Loss: 0.057774\n",
      "Epoch: 5926, Training Loss: 0.067584, Validation Loss: 0.060669\n",
      "Epoch: 5927, Training Loss: 0.066583, Validation Loss: 0.059011\n",
      "Epoch: 5928, Training Loss: 0.066357, Validation Loss: 0.057505\n",
      "Epoch: 5929, Training Loss: 0.066144, Validation Loss: 0.060134\n",
      "Epoch: 5930, Training Loss: 0.068265, Validation Loss: 0.069420\n",
      "Epoch: 5931, Training Loss: 0.065972, Validation Loss: 0.060912\n",
      "Epoch: 5932, Training Loss: 0.066747, Validation Loss: 0.059550\n",
      "Epoch: 5933, Training Loss: 0.064719, Validation Loss: 0.060188\n",
      "Epoch: 5934, Training Loss: 0.065370, Validation Loss: 0.060700\n",
      "Epoch: 5935, Training Loss: 0.065764, Validation Loss: 0.058743\n",
      "Epoch: 5936, Training Loss: 0.064860, Validation Loss: 0.058832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5937, Training Loss: 0.066335, Validation Loss: 0.065397\n",
      "Epoch: 5938, Training Loss: 0.064338, Validation Loss: 0.059012\n",
      "Epoch: 5939, Training Loss: 0.065525, Validation Loss: 0.058815\n",
      "Epoch: 5940, Training Loss: 0.064359, Validation Loss: 0.060006\n",
      "Epoch: 5941, Training Loss: 0.064779, Validation Loss: 0.062437\n",
      "Epoch: 5942, Training Loss: 0.064822, Validation Loss: 0.058794\n",
      "Epoch: 5943, Training Loss: 0.064578, Validation Loss: 0.058953\n",
      "Epoch: 5944, Training Loss: 0.064658, Validation Loss: 0.060614\n",
      "Epoch: 5945, Training Loss: 0.064275, Validation Loss: 0.060482\n",
      "Epoch: 5946, Training Loss: 0.064551, Validation Loss: 0.058208\n",
      "Epoch: 5947, Training Loss: 0.064061, Validation Loss: 0.060410\n",
      "Epoch: 5948, Training Loss: 0.064559, Validation Loss: 0.062277\n",
      "Epoch: 5949, Training Loss: 0.064160, Validation Loss: 0.059458\n",
      "Epoch: 5950, Training Loss: 0.064442, Validation Loss: 0.057713\n",
      "Epoch: 5951, Training Loss: 0.064516, Validation Loss: 0.061069\n",
      "Epoch: 5952, Training Loss: 0.064183, Validation Loss: 0.059559\n",
      "Epoch: 5953, Training Loss: 0.064218, Validation Loss: 0.059297\n",
      "Epoch: 5954, Training Loss: 0.064091, Validation Loss: 0.061259\n",
      "Epoch: 5955, Training Loss: 0.065018, Validation Loss: 0.060514\n",
      "Epoch: 5956, Training Loss: 0.064596, Validation Loss: 0.060370\n",
      "Epoch: 5957, Training Loss: 0.065668, Validation Loss: 0.057801\n",
      "Epoch: 5958, Training Loss: 0.067352, Validation Loss: 0.063561\n",
      "Epoch: 5959, Training Loss: 0.067269, Validation Loss: 0.058033\n",
      "Epoch: 5960, Training Loss: 0.066701, Validation Loss: 0.060391\n",
      "Epoch: 5961, Training Loss: 0.065598, Validation Loss: 0.057911\n",
      "Epoch: 5962, Training Loss: 0.064598, Validation Loss: 0.059351\n",
      "Epoch: 5963, Training Loss: 0.064577, Validation Loss: 0.057506\n",
      "Epoch: 5964, Training Loss: 0.065742, Validation Loss: 0.057556\n",
      "Epoch: 5965, Training Loss: 0.064788, Validation Loss: 0.059504\n",
      "Epoch: 5966, Training Loss: 0.064131, Validation Loss: 0.058610\n",
      "Epoch: 5967, Training Loss: 0.064608, Validation Loss: 0.056142\n",
      "Epoch: 5968, Training Loss: 0.064105, Validation Loss: 0.057860\n",
      "Epoch: 5969, Training Loss: 0.064179, Validation Loss: 0.059670\n",
      "Epoch: 5970, Training Loss: 0.063938, Validation Loss: 0.058530\n",
      "Epoch: 5971, Training Loss: 0.064947, Validation Loss: 0.060136\n",
      "Epoch: 5972, Training Loss: 0.066686, Validation Loss: 0.058113\n",
      "Epoch: 5973, Training Loss: 0.068872, Validation Loss: 0.064238\n",
      "Epoch: 5974, Training Loss: 0.071699, Validation Loss: 0.057420\n",
      "Epoch: 5975, Training Loss: 0.068902, Validation Loss: 0.066281\n",
      "Epoch: 5976, Training Loss: 0.065118, Validation Loss: 0.061522\n",
      "Epoch: 5977, Training Loss: 0.066857, Validation Loss: 0.058331\n",
      "Epoch: 5978, Training Loss: 0.068725, Validation Loss: 0.065339\n",
      "Epoch: 5979, Training Loss: 0.065190, Validation Loss: 0.058515\n",
      "Epoch: 5980, Training Loss: 0.065682, Validation Loss: 0.058599\n",
      "Epoch: 5981, Training Loss: 0.067495, Validation Loss: 0.063715\n",
      "Epoch: 5982, Training Loss: 0.064823, Validation Loss: 0.055795\n",
      "Epoch: 5983, Training Loss: 0.065233, Validation Loss: 0.055993\n",
      "Epoch: 5984, Training Loss: 0.066685, Validation Loss: 0.062171\n",
      "Epoch: 5985, Training Loss: 0.064165, Validation Loss: 0.057555\n",
      "Epoch: 5986, Training Loss: 0.065580, Validation Loss: 0.058630\n",
      "Epoch: 5987, Training Loss: 0.064792, Validation Loss: 0.062991\n",
      "Epoch: 5988, Training Loss: 0.063875, Validation Loss: 0.059653\n",
      "Epoch: 5989, Training Loss: 0.065173, Validation Loss: 0.058323\n",
      "Epoch: 5990, Training Loss: 0.064151, Validation Loss: 0.059009\n",
      "Epoch: 5991, Training Loss: 0.064637, Validation Loss: 0.062126\n",
      "Epoch: 5992, Training Loss: 0.065535, Validation Loss: 0.059862\n",
      "Epoch: 5993, Training Loss: 0.067931, Validation Loss: 0.061601\n",
      "Epoch: 5994, Training Loss: 0.070787, Validation Loss: 0.059529\n",
      "Epoch: 5995, Training Loss: 0.064533, Validation Loss: 0.061201\n",
      "Epoch: 5996, Training Loss: 0.070220, Validation Loss: 0.060207\n",
      "Epoch: 5997, Training Loss: 0.073297, Validation Loss: 0.055274\n",
      "Epoch: 5998, Training Loss: 0.069618, Validation Loss: 0.068654\n",
      "Epoch: 5999, Training Loss: 0.069880, Validation Loss: 0.058248\n",
      "Epoch: 6000, Training Loss: 0.069490, Validation Loss: 0.059524\n",
      "Epoch: 6001, Training Loss: 0.069540, Validation Loss: 0.064222\n",
      "Epoch: 6002, Training Loss: 0.066110, Validation Loss: 0.058677\n",
      "Epoch: 6003, Training Loss: 0.069414, Validation Loss: 0.059947\n",
      "Epoch: 6004, Training Loss: 0.066493, Validation Loss: 0.059347\n",
      "Epoch: 6005, Training Loss: 0.066595, Validation Loss: 0.060337\n",
      "Epoch: 6006, Training Loss: 0.068092, Validation Loss: 0.058963\n",
      "Epoch: 6007, Training Loss: 0.064121, Validation Loss: 0.056478\n",
      "Epoch: 6008, Training Loss: 0.066528, Validation Loss: 0.061530\n",
      "Epoch: 6009, Training Loss: 0.065295, Validation Loss: 0.060357\n",
      "Epoch: 6010, Training Loss: 0.064627, Validation Loss: 0.061948\n",
      "Epoch: 6011, Training Loss: 0.066292, Validation Loss: 0.064275\n",
      "Epoch: 6012, Training Loss: 0.063920, Validation Loss: 0.059167\n",
      "Epoch: 6013, Training Loss: 0.065218, Validation Loss: 0.059243\n",
      "Epoch: 6014, Training Loss: 0.065107, Validation Loss: 0.059690\n",
      "Epoch: 6015, Training Loss: 0.064350, Validation Loss: 0.059294\n",
      "Epoch: 6016, Training Loss: 0.064996, Validation Loss: 0.061053\n",
      "Epoch: 6017, Training Loss: 0.064223, Validation Loss: 0.062845\n",
      "Epoch: 6018, Training Loss: 0.064387, Validation Loss: 0.061238\n",
      "Epoch: 6019, Training Loss: 0.064246, Validation Loss: 0.059546\n",
      "Epoch: 6020, Training Loss: 0.064479, Validation Loss: 0.062309\n",
      "Epoch: 6021, Training Loss: 0.064774, Validation Loss: 0.060546\n",
      "Epoch: 6022, Training Loss: 0.064450, Validation Loss: 0.059953\n",
      "Epoch: 6023, Training Loss: 0.064113, Validation Loss: 0.059092\n",
      "Epoch: 6024, Training Loss: 0.064528, Validation Loss: 0.061668\n",
      "Epoch: 6025, Training Loss: 0.063699, Validation Loss: 0.059027\n",
      "Epoch: 6026, Training Loss: 0.064278, Validation Loss: 0.058579\n",
      "Epoch: 6027, Training Loss: 0.064005, Validation Loss: 0.062022\n",
      "Epoch: 6028, Training Loss: 0.063603, Validation Loss: 0.058897\n",
      "Epoch: 6029, Training Loss: 0.064425, Validation Loss: 0.057299\n",
      "Epoch: 6030, Training Loss: 0.063918, Validation Loss: 0.057912\n",
      "Epoch: 6031, Training Loss: 0.064675, Validation Loss: 0.062112\n",
      "Epoch: 6032, Training Loss: 0.064714, Validation Loss: 0.059985\n",
      "Epoch: 6033, Training Loss: 0.063828, Validation Loss: 0.059966\n",
      "Epoch: 6034, Training Loss: 0.064013, Validation Loss: 0.060479\n",
      "Epoch: 6035, Training Loss: 0.063714, Validation Loss: 0.058171\n",
      "Epoch: 6036, Training Loss: 0.064096, Validation Loss: 0.057632\n",
      "Epoch: 6037, Training Loss: 0.064745, Validation Loss: 0.062628\n",
      "Epoch: 6038, Training Loss: 0.065821, Validation Loss: 0.059179\n",
      "Epoch: 6039, Training Loss: 0.067166, Validation Loss: 0.059343\n",
      "Epoch: 6040, Training Loss: 0.067709, Validation Loss: 0.058501\n",
      "Epoch: 6041, Training Loss: 0.064029, Validation Loss: 0.062012\n",
      "Epoch: 6042, Training Loss: 0.065676, Validation Loss: 0.059598\n",
      "Epoch: 6043, Training Loss: 0.066631, Validation Loss: 0.058149\n",
      "Epoch: 6044, Training Loss: 0.064804, Validation Loss: 0.062173\n",
      "Epoch: 6045, Training Loss: 0.064577, Validation Loss: 0.057875\n",
      "Epoch: 6046, Training Loss: 0.068565, Validation Loss: 0.056629\n",
      "Epoch: 6047, Training Loss: 0.068666, Validation Loss: 0.060987\n",
      "Epoch: 6048, Training Loss: 0.064869, Validation Loss: 0.057501\n",
      "Epoch: 6049, Training Loss: 0.064363, Validation Loss: 0.058520\n",
      "Epoch: 6050, Training Loss: 0.066433, Validation Loss: 0.060512\n",
      "Epoch: 6051, Training Loss: 0.065936, Validation Loss: 0.057691\n",
      "Epoch: 6052, Training Loss: 0.063884, Validation Loss: 0.057327\n",
      "Epoch: 6053, Training Loss: 0.065695, Validation Loss: 0.058702\n",
      "Epoch: 6054, Training Loss: 0.068133, Validation Loss: 0.058059\n",
      "Epoch: 6055, Training Loss: 0.064148, Validation Loss: 0.059399\n",
      "Epoch: 6056, Training Loss: 0.064828, Validation Loss: 0.058425\n",
      "Epoch: 6057, Training Loss: 0.067252, Validation Loss: 0.059108\n",
      "Epoch: 6058, Training Loss: 0.070227, Validation Loss: 0.065762\n",
      "Epoch: 6059, Training Loss: 0.073466, Validation Loss: 0.060876\n",
      "Epoch: 6060, Training Loss: 0.069032, Validation Loss: 0.064325\n",
      "Epoch: 6061, Training Loss: 0.065039, Validation Loss: 0.062247\n",
      "Epoch: 6062, Training Loss: 0.069978, Validation Loss: 0.060986\n",
      "Epoch: 6063, Training Loss: 0.067111, Validation Loss: 0.059398\n",
      "Epoch: 6064, Training Loss: 0.065872, Validation Loss: 0.058888\n",
      "Epoch: 6065, Training Loss: 0.067173, Validation Loss: 0.059018\n",
      "Epoch: 6066, Training Loss: 0.066021, Validation Loss: 0.059378\n",
      "Epoch: 6067, Training Loss: 0.067788, Validation Loss: 0.061518\n",
      "Epoch: 6068, Training Loss: 0.064195, Validation Loss: 0.057957\n",
      "Epoch: 6069, Training Loss: 0.067821, Validation Loss: 0.058091\n",
      "Epoch: 6070, Training Loss: 0.067503, Validation Loss: 0.058155\n",
      "Epoch: 6071, Training Loss: 0.064218, Validation Loss: 0.061819\n",
      "Epoch: 6072, Training Loss: 0.067665, Validation Loss: 0.057489\n",
      "Epoch: 6073, Training Loss: 0.065964, Validation Loss: 0.058924\n",
      "Epoch: 6074, Training Loss: 0.066167, Validation Loss: 0.063565\n",
      "Epoch: 6075, Training Loss: 0.065035, Validation Loss: 0.056908\n",
      "Epoch: 6076, Training Loss: 0.065214, Validation Loss: 0.056551\n",
      "Epoch: 6077, Training Loss: 0.066461, Validation Loss: 0.063702\n",
      "Epoch: 6078, Training Loss: 0.063747, Validation Loss: 0.057289\n",
      "Epoch: 6079, Training Loss: 0.064735, Validation Loss: 0.058891\n",
      "Epoch: 6080, Training Loss: 0.064889, Validation Loss: 0.063143\n",
      "Epoch: 6081, Training Loss: 0.064245, Validation Loss: 0.058827\n",
      "Epoch: 6082, Training Loss: 0.065235, Validation Loss: 0.060877\n",
      "Epoch: 6083, Training Loss: 0.064662, Validation Loss: 0.060640\n",
      "Epoch: 6084, Training Loss: 0.065492, Validation Loss: 0.064984\n",
      "Epoch: 6085, Training Loss: 0.067852, Validation Loss: 0.059316\n",
      "Epoch: 6086, Training Loss: 0.065741, Validation Loss: 0.063655\n",
      "Epoch: 6087, Training Loss: 0.066380, Validation Loss: 0.060718\n",
      "Epoch: 6088, Training Loss: 0.063660, Validation Loss: 0.060400\n",
      "Epoch: 6089, Training Loss: 0.065862, Validation Loss: 0.061180\n",
      "Epoch: 6090, Training Loss: 0.069099, Validation Loss: 0.058499\n",
      "Epoch: 6091, Training Loss: 0.065403, Validation Loss: 0.065063\n",
      "Epoch: 6092, Training Loss: 0.067365, Validation Loss: 0.057759\n",
      "Epoch: 6093, Training Loss: 0.071764, Validation Loss: 0.059335\n",
      "Epoch: 6094, Training Loss: 0.068997, Validation Loss: 0.068306\n",
      "Epoch: 6095, Training Loss: 0.066855, Validation Loss: 0.056711\n",
      "Epoch: 6096, Training Loss: 0.071651, Validation Loss: 0.057728\n",
      "Epoch: 6097, Training Loss: 0.067949, Validation Loss: 0.064946\n",
      "Epoch: 6098, Training Loss: 0.064904, Validation Loss: 0.061470\n",
      "Epoch: 6099, Training Loss: 0.069786, Validation Loss: 0.059040\n",
      "Epoch: 6100, Training Loss: 0.065666, Validation Loss: 0.057559\n",
      "Epoch: 6101, Training Loss: 0.065563, Validation Loss: 0.061004\n",
      "Epoch: 6102, Training Loss: 0.066914, Validation Loss: 0.058608\n",
      "Epoch: 6103, Training Loss: 0.064139, Validation Loss: 0.057944\n",
      "Epoch: 6104, Training Loss: 0.066384, Validation Loss: 0.062879\n",
      "Epoch: 6105, Training Loss: 0.063986, Validation Loss: 0.058891\n",
      "Epoch: 6106, Training Loss: 0.064945, Validation Loss: 0.059078\n",
      "Epoch: 6107, Training Loss: 0.067042, Validation Loss: 0.060682\n",
      "Epoch: 6108, Training Loss: 0.064855, Validation Loss: 0.063981\n",
      "Epoch: 6109, Training Loss: 0.067181, Validation Loss: 0.061474\n",
      "Epoch: 6110, Training Loss: 0.064554, Validation Loss: 0.063435\n",
      "Epoch: 6111, Training Loss: 0.064795, Validation Loss: 0.060132\n",
      "Epoch: 6112, Training Loss: 0.065120, Validation Loss: 0.060905\n",
      "Epoch: 6113, Training Loss: 0.064925, Validation Loss: 0.059340\n",
      "Epoch: 6114, Training Loss: 0.067427, Validation Loss: 0.064741\n",
      "Epoch: 6115, Training Loss: 0.068049, Validation Loss: 0.058742\n",
      "Epoch: 6116, Training Loss: 0.066254, Validation Loss: 0.060242\n",
      "Epoch: 6117, Training Loss: 0.064267, Validation Loss: 0.061952\n",
      "Epoch: 6118, Training Loss: 0.065159, Validation Loss: 0.058537\n",
      "Epoch: 6119, Training Loss: 0.066673, Validation Loss: 0.057433\n",
      "Epoch: 6120, Training Loss: 0.064448, Validation Loss: 0.060728\n",
      "Epoch: 6121, Training Loss: 0.064761, Validation Loss: 0.059147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6122, Training Loss: 0.065624, Validation Loss: 0.056505\n",
      "Epoch: 6123, Training Loss: 0.064043, Validation Loss: 0.057056\n",
      "Epoch: 6124, Training Loss: 0.064511, Validation Loss: 0.058759\n",
      "Epoch: 6125, Training Loss: 0.065602, Validation Loss: 0.055583\n",
      "Epoch: 6126, Training Loss: 0.065013, Validation Loss: 0.056941\n",
      "Epoch: 6127, Training Loss: 0.064634, Validation Loss: 0.061543\n",
      "Epoch: 6128, Training Loss: 0.064063, Validation Loss: 0.057810\n",
      "Epoch: 6129, Training Loss: 0.065432, Validation Loss: 0.057428\n",
      "Epoch: 6130, Training Loss: 0.065211, Validation Loss: 0.064017\n",
      "Epoch: 6131, Training Loss: 0.063605, Validation Loss: 0.058755\n",
      "Epoch: 6132, Training Loss: 0.066096, Validation Loss: 0.056560\n",
      "Epoch: 6133, Training Loss: 0.067168, Validation Loss: 0.063610\n",
      "Epoch: 6134, Training Loss: 0.066000, Validation Loss: 0.056960\n",
      "Epoch: 6135, Training Loss: 0.064409, Validation Loss: 0.058086\n",
      "Epoch: 6136, Training Loss: 0.063847, Validation Loss: 0.059929\n",
      "Epoch: 6137, Training Loss: 0.063145, Validation Loss: 0.058681\n",
      "Epoch: 6138, Training Loss: 0.063501, Validation Loss: 0.059283\n",
      "Epoch: 6139, Training Loss: 0.064143, Validation Loss: 0.058763\n",
      "Epoch: 6140, Training Loss: 0.064170, Validation Loss: 0.060277\n",
      "Epoch: 6141, Training Loss: 0.064478, Validation Loss: 0.058857\n",
      "Epoch: 6142, Training Loss: 0.064545, Validation Loss: 0.061081\n",
      "Epoch: 6143, Training Loss: 0.065002, Validation Loss: 0.058178\n",
      "Epoch: 6144, Training Loss: 0.065722, Validation Loss: 0.061322\n",
      "Epoch: 6145, Training Loss: 0.066325, Validation Loss: 0.058519\n",
      "Epoch: 6146, Training Loss: 0.064727, Validation Loss: 0.059900\n",
      "Epoch: 6147, Training Loss: 0.063262, Validation Loss: 0.059490\n",
      "Epoch: 6148, Training Loss: 0.063602, Validation Loss: 0.057874\n",
      "Epoch: 6149, Training Loss: 0.066707, Validation Loss: 0.059040\n",
      "Epoch: 6150, Training Loss: 0.069190, Validation Loss: 0.058651\n",
      "Epoch: 6151, Training Loss: 0.063560, Validation Loss: 0.060265\n",
      "Epoch: 6152, Training Loss: 0.065638, Validation Loss: 0.057293\n",
      "Epoch: 6153, Training Loss: 0.070369, Validation Loss: 0.057103\n",
      "Epoch: 6154, Training Loss: 0.065179, Validation Loss: 0.062172\n",
      "Epoch: 6155, Training Loss: 0.064881, Validation Loss: 0.057236\n",
      "Epoch: 6156, Training Loss: 0.066966, Validation Loss: 0.057307\n",
      "Epoch: 6157, Training Loss: 0.065601, Validation Loss: 0.062852\n",
      "Epoch: 6158, Training Loss: 0.063997, Validation Loss: 0.056272\n",
      "Epoch: 6159, Training Loss: 0.066495, Validation Loss: 0.057203\n",
      "Epoch: 6160, Training Loss: 0.066084, Validation Loss: 0.062734\n",
      "Epoch: 6161, Training Loss: 0.063444, Validation Loss: 0.057093\n",
      "Epoch: 6162, Training Loss: 0.064564, Validation Loss: 0.058101\n",
      "Epoch: 6163, Training Loss: 0.064698, Validation Loss: 0.063087\n",
      "Epoch: 6164, Training Loss: 0.063139, Validation Loss: 0.058436\n",
      "Epoch: 6165, Training Loss: 0.064052, Validation Loss: 0.057859\n",
      "Epoch: 6166, Training Loss: 0.066455, Validation Loss: 0.061907\n",
      "Epoch: 6167, Training Loss: 0.068486, Validation Loss: 0.059087\n",
      "Epoch: 6168, Training Loss: 0.065197, Validation Loss: 0.058950\n",
      "Epoch: 6169, Training Loss: 0.063105, Validation Loss: 0.060534\n",
      "Epoch: 6170, Training Loss: 0.064213, Validation Loss: 0.058763\n",
      "Epoch: 6171, Training Loss: 0.067399, Validation Loss: 0.058148\n",
      "Epoch: 6172, Training Loss: 0.065304, Validation Loss: 0.058701\n",
      "Epoch: 6173, Training Loss: 0.063691, Validation Loss: 0.060985\n",
      "Epoch: 6174, Training Loss: 0.065060, Validation Loss: 0.057932\n",
      "Epoch: 6175, Training Loss: 0.066417, Validation Loss: 0.057062\n",
      "Epoch: 6176, Training Loss: 0.063539, Validation Loss: 0.058657\n",
      "Epoch: 6177, Training Loss: 0.063711, Validation Loss: 0.058032\n",
      "Epoch: 6178, Training Loss: 0.066188, Validation Loss: 0.057104\n",
      "Epoch: 6179, Training Loss: 0.063957, Validation Loss: 0.059228\n",
      "Epoch: 6180, Training Loss: 0.062953, Validation Loss: 0.059342\n",
      "Epoch: 6181, Training Loss: 0.064231, Validation Loss: 0.058454\n",
      "Epoch: 6182, Training Loss: 0.064511, Validation Loss: 0.059135\n",
      "Epoch: 6183, Training Loss: 0.063866, Validation Loss: 0.059749\n",
      "Epoch: 6184, Training Loss: 0.063195, Validation Loss: 0.058102\n",
      "Epoch: 6185, Training Loss: 0.063556, Validation Loss: 0.059423\n",
      "Epoch: 6186, Training Loss: 0.065372, Validation Loss: 0.059742\n",
      "Epoch: 6187, Training Loss: 0.063542, Validation Loss: 0.058737\n",
      "Epoch: 6188, Training Loss: 0.062862, Validation Loss: 0.059146\n",
      "Epoch: 6189, Training Loss: 0.064539, Validation Loss: 0.061756\n",
      "Epoch: 6190, Training Loss: 0.064604, Validation Loss: 0.058756\n",
      "Epoch: 6191, Training Loss: 0.064273, Validation Loss: 0.059275\n",
      "Epoch: 6192, Training Loss: 0.065284, Validation Loss: 0.063990\n",
      "Epoch: 6193, Training Loss: 0.063177, Validation Loss: 0.057910\n",
      "Epoch: 6194, Training Loss: 0.063103, Validation Loss: 0.057243\n",
      "Epoch: 6195, Training Loss: 0.064115, Validation Loss: 0.061667\n",
      "Epoch: 6196, Training Loss: 0.062968, Validation Loss: 0.058086\n",
      "Epoch: 6197, Training Loss: 0.063116, Validation Loss: 0.058139\n",
      "Epoch: 6198, Training Loss: 0.063637, Validation Loss: 0.061516\n",
      "Epoch: 6199, Training Loss: 0.063035, Validation Loss: 0.058564\n",
      "Epoch: 6200, Training Loss: 0.062946, Validation Loss: 0.058582\n",
      "Epoch: 6201, Training Loss: 0.063252, Validation Loss: 0.061711\n",
      "Epoch: 6202, Training Loss: 0.062938, Validation Loss: 0.059424\n",
      "Epoch: 6203, Training Loss: 0.064919, Validation Loss: 0.059650\n",
      "Epoch: 6204, Training Loss: 0.068445, Validation Loss: 0.059859\n",
      "Epoch: 6205, Training Loss: 0.066697, Validation Loss: 0.060462\n",
      "Epoch: 6206, Training Loss: 0.064920, Validation Loss: 0.059804\n",
      "Epoch: 6207, Training Loss: 0.063510, Validation Loss: 0.061918\n",
      "Epoch: 6208, Training Loss: 0.063293, Validation Loss: 0.058835\n",
      "Epoch: 6209, Training Loss: 0.065046, Validation Loss: 0.056647\n",
      "Epoch: 6210, Training Loss: 0.064835, Validation Loss: 0.059814\n",
      "Epoch: 6211, Training Loss: 0.063174, Validation Loss: 0.056041\n",
      "Epoch: 6212, Training Loss: 0.062822, Validation Loss: 0.056880\n",
      "Epoch: 6213, Training Loss: 0.063370, Validation Loss: 0.060342\n",
      "Epoch: 6214, Training Loss: 0.063384, Validation Loss: 0.058204\n",
      "Epoch: 6215, Training Loss: 0.062866, Validation Loss: 0.059601\n",
      "Epoch: 6216, Training Loss: 0.063151, Validation Loss: 0.061987\n",
      "Epoch: 6217, Training Loss: 0.063191, Validation Loss: 0.058863\n",
      "Epoch: 6218, Training Loss: 0.063508, Validation Loss: 0.060546\n",
      "Epoch: 6219, Training Loss: 0.066822, Validation Loss: 0.058354\n",
      "Epoch: 6220, Training Loss: 0.068002, Validation Loss: 0.060461\n",
      "Epoch: 6221, Training Loss: 0.069207, Validation Loss: 0.058224\n",
      "Epoch: 6222, Training Loss: 0.066154, Validation Loss: 0.061952\n",
      "Epoch: 6223, Training Loss: 0.063966, Validation Loss: 0.056790\n",
      "Epoch: 6224, Training Loss: 0.067287, Validation Loss: 0.058087\n",
      "Epoch: 6225, Training Loss: 0.065604, Validation Loss: 0.059150\n",
      "Epoch: 6226, Training Loss: 0.065886, Validation Loss: 0.054904\n",
      "Epoch: 6227, Training Loss: 0.065983, Validation Loss: 0.055417\n",
      "Epoch: 6228, Training Loss: 0.067596, Validation Loss: 0.061782\n",
      "Epoch: 6229, Training Loss: 0.063783, Validation Loss: 0.055197\n",
      "Epoch: 6230, Training Loss: 0.065452, Validation Loss: 0.055001\n",
      "Epoch: 6231, Training Loss: 0.064092, Validation Loss: 0.058007\n",
      "Epoch: 6232, Training Loss: 0.063356, Validation Loss: 0.058904\n",
      "Epoch: 6233, Training Loss: 0.064478, Validation Loss: 0.058735\n",
      "Epoch: 6234, Training Loss: 0.063719, Validation Loss: 0.060476\n",
      "Epoch: 6235, Training Loss: 0.063620, Validation Loss: 0.057643\n",
      "Epoch: 6236, Training Loss: 0.065145, Validation Loss: 0.055388\n",
      "Epoch: 6237, Training Loss: 0.066305, Validation Loss: 0.063923\n",
      "Epoch: 6238, Training Loss: 0.067457, Validation Loss: 0.060747\n",
      "Epoch: 6239, Training Loss: 0.067290, Validation Loss: 0.060983\n",
      "Epoch: 6240, Training Loss: 0.064771, Validation Loss: 0.063957\n",
      "Epoch: 6241, Training Loss: 0.063702, Validation Loss: 0.059459\n",
      "Epoch: 6242, Training Loss: 0.065510, Validation Loss: 0.059227\n",
      "Epoch: 6243, Training Loss: 0.064253, Validation Loss: 0.056935\n",
      "Epoch: 6244, Training Loss: 0.063460, Validation Loss: 0.059019\n",
      "Epoch: 6245, Training Loss: 0.064065, Validation Loss: 0.056280\n",
      "Epoch: 6246, Training Loss: 0.063703, Validation Loss: 0.056613\n",
      "Epoch: 6247, Training Loss: 0.063383, Validation Loss: 0.058101\n",
      "Epoch: 6248, Training Loss: 0.064494, Validation Loss: 0.058257\n",
      "Epoch: 6249, Training Loss: 0.066611, Validation Loss: 0.055464\n",
      "Epoch: 6250, Training Loss: 0.065992, Validation Loss: 0.061479\n",
      "Epoch: 6251, Training Loss: 0.063968, Validation Loss: 0.060690\n",
      "Epoch: 6252, Training Loss: 0.063538, Validation Loss: 0.059649\n",
      "Epoch: 6253, Training Loss: 0.063664, Validation Loss: 0.060736\n",
      "Epoch: 6254, Training Loss: 0.063407, Validation Loss: 0.058250\n",
      "Epoch: 6255, Training Loss: 0.063701, Validation Loss: 0.057150\n",
      "Epoch: 6256, Training Loss: 0.063383, Validation Loss: 0.057125\n",
      "Epoch: 6257, Training Loss: 0.062838, Validation Loss: 0.057554\n",
      "Epoch: 6258, Training Loss: 0.062882, Validation Loss: 0.059890\n",
      "Epoch: 6259, Training Loss: 0.062602, Validation Loss: 0.059335\n",
      "Epoch: 6260, Training Loss: 0.062408, Validation Loss: 0.059004\n",
      "Epoch: 6261, Training Loss: 0.062576, Validation Loss: 0.059846\n",
      "Epoch: 6262, Training Loss: 0.062131, Validation Loss: 0.060219\n",
      "Epoch: 6263, Training Loss: 0.062683, Validation Loss: 0.062331\n",
      "Epoch: 6264, Training Loss: 0.062785, Validation Loss: 0.060777\n",
      "Epoch: 6265, Training Loss: 0.065310, Validation Loss: 0.060224\n",
      "Epoch: 6266, Training Loss: 0.072768, Validation Loss: 0.057859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6267, Training Loss: 0.071173, Validation Loss: 0.065038\n",
      "Epoch: 6268, Training Loss: 0.066322, Validation Loss: 0.059200\n",
      "Epoch: 6269, Training Loss: 0.067105, Validation Loss: 0.058085\n",
      "Epoch: 6270, Training Loss: 0.069586, Validation Loss: 0.065951\n",
      "Epoch: 6271, Training Loss: 0.067552, Validation Loss: 0.057504\n",
      "Epoch: 6272, Training Loss: 0.067316, Validation Loss: 0.054770\n",
      "Epoch: 6273, Training Loss: 0.067371, Validation Loss: 0.058837\n",
      "Epoch: 6274, Training Loss: 0.064924, Validation Loss: 0.057740\n",
      "Epoch: 6275, Training Loss: 0.067247, Validation Loss: 0.055257\n",
      "Epoch: 6276, Training Loss: 0.064273, Validation Loss: 0.054844\n",
      "Epoch: 6277, Training Loss: 0.067467, Validation Loss: 0.059545\n",
      "Epoch: 6278, Training Loss: 0.065171, Validation Loss: 0.053606\n",
      "Epoch: 6279, Training Loss: 0.063404, Validation Loss: 0.054741\n",
      "Epoch: 6280, Training Loss: 0.067895, Validation Loss: 0.061794\n",
      "Epoch: 6281, Training Loss: 0.063915, Validation Loss: 0.057491\n",
      "Epoch: 6282, Training Loss: 0.065360, Validation Loss: 0.059635\n",
      "Epoch: 6283, Training Loss: 0.067364, Validation Loss: 0.061432\n",
      "Epoch: 6284, Training Loss: 0.064305, Validation Loss: 0.064457\n",
      "Epoch: 6285, Training Loss: 0.067214, Validation Loss: 0.064958\n",
      "Epoch: 6286, Training Loss: 0.063403, Validation Loss: 0.065717\n",
      "Epoch: 6287, Training Loss: 0.063610, Validation Loss: 0.062575\n",
      "Epoch: 6288, Training Loss: 0.063883, Validation Loss: 0.060049\n",
      "Epoch: 6289, Training Loss: 0.063200, Validation Loss: 0.060599\n",
      "Epoch: 6290, Training Loss: 0.064043, Validation Loss: 0.063288\n",
      "Epoch: 6291, Training Loss: 0.064039, Validation Loss: 0.060847\n",
      "Epoch: 6292, Training Loss: 0.063081, Validation Loss: 0.061256\n",
      "Epoch: 6293, Training Loss: 0.063175, Validation Loss: 0.061123\n",
      "Epoch: 6294, Training Loss: 0.063276, Validation Loss: 0.058662\n",
      "Epoch: 6295, Training Loss: 0.062180, Validation Loss: 0.058260\n",
      "Epoch: 6296, Training Loss: 0.064129, Validation Loss: 0.059328\n",
      "Epoch: 6297, Training Loss: 0.064545, Validation Loss: 0.054621\n",
      "Epoch: 6298, Training Loss: 0.063129, Validation Loss: 0.054890\n",
      "Epoch: 6299, Training Loss: 0.065708, Validation Loss: 0.061746\n",
      "Epoch: 6300, Training Loss: 0.065181, Validation Loss: 0.057905\n",
      "Epoch: 6301, Training Loss: 0.064218, Validation Loss: 0.058866\n",
      "Epoch: 6302, Training Loss: 0.066320, Validation Loss: 0.065569\n",
      "Epoch: 6303, Training Loss: 0.063130, Validation Loss: 0.059340\n",
      "Epoch: 6304, Training Loss: 0.064160, Validation Loss: 0.057112\n",
      "Epoch: 6305, Training Loss: 0.065657, Validation Loss: 0.060993\n",
      "Epoch: 6306, Training Loss: 0.064462, Validation Loss: 0.057176\n",
      "Epoch: 6307, Training Loss: 0.064980, Validation Loss: 0.057176\n",
      "Epoch: 6308, Training Loss: 0.068921, Validation Loss: 0.065844\n",
      "Epoch: 6309, Training Loss: 0.068111, Validation Loss: 0.059962\n",
      "Epoch: 6310, Training Loss: 0.065173, Validation Loss: 0.058279\n",
      "Epoch: 6311, Training Loss: 0.064541, Validation Loss: 0.062586\n",
      "Epoch: 6312, Training Loss: 0.064385, Validation Loss: 0.058973\n",
      "Epoch: 6313, Training Loss: 0.065526, Validation Loss: 0.056956\n",
      "Epoch: 6314, Training Loss: 0.063906, Validation Loss: 0.055574\n",
      "Epoch: 6315, Training Loss: 0.063841, Validation Loss: 0.058198\n",
      "Epoch: 6316, Training Loss: 0.065681, Validation Loss: 0.056199\n",
      "Epoch: 6317, Training Loss: 0.064276, Validation Loss: 0.056800\n",
      "Epoch: 6318, Training Loss: 0.063749, Validation Loss: 0.056766\n",
      "Epoch: 6319, Training Loss: 0.064267, Validation Loss: 0.056660\n",
      "Epoch: 6320, Training Loss: 0.063984, Validation Loss: 0.056496\n",
      "Epoch: 6321, Training Loss: 0.063251, Validation Loss: 0.056606\n",
      "Epoch: 6322, Training Loss: 0.064355, Validation Loss: 0.056004\n",
      "Epoch: 6323, Training Loss: 0.067196, Validation Loss: 0.062821\n",
      "Epoch: 6324, Training Loss: 0.063822, Validation Loss: 0.055562\n",
      "Epoch: 6325, Training Loss: 0.063186, Validation Loss: 0.056359\n",
      "Epoch: 6326, Training Loss: 0.066897, Validation Loss: 0.057613\n",
      "Epoch: 6327, Training Loss: 0.064283, Validation Loss: 0.057260\n",
      "Epoch: 6328, Training Loss: 0.062938, Validation Loss: 0.057076\n",
      "Epoch: 6329, Training Loss: 0.066846, Validation Loss: 0.060179\n",
      "Epoch: 6330, Training Loss: 0.064976, Validation Loss: 0.057634\n",
      "Epoch: 6331, Training Loss: 0.062650, Validation Loss: 0.057078\n",
      "Epoch: 6332, Training Loss: 0.066721, Validation Loss: 0.060283\n",
      "Epoch: 6333, Training Loss: 0.067349, Validation Loss: 0.057983\n",
      "Epoch: 6334, Training Loss: 0.065407, Validation Loss: 0.058430\n",
      "Epoch: 6335, Training Loss: 0.066696, Validation Loss: 0.064810\n",
      "Epoch: 6336, Training Loss: 0.064066, Validation Loss: 0.056560\n",
      "Epoch: 6337, Training Loss: 0.064574, Validation Loss: 0.055171\n",
      "Epoch: 6338, Training Loss: 0.066381, Validation Loss: 0.060767\n",
      "Epoch: 6339, Training Loss: 0.062894, Validation Loss: 0.054692\n",
      "Epoch: 6340, Training Loss: 0.063862, Validation Loss: 0.056350\n",
      "Epoch: 6341, Training Loss: 0.065320, Validation Loss: 0.061875\n",
      "Epoch: 6342, Training Loss: 0.062863, Validation Loss: 0.058028\n",
      "Epoch: 6343, Training Loss: 0.064138, Validation Loss: 0.057367\n",
      "Epoch: 6344, Training Loss: 0.065419, Validation Loss: 0.062638\n",
      "Epoch: 6345, Training Loss: 0.063402, Validation Loss: 0.057901\n",
      "Epoch: 6346, Training Loss: 0.065091, Validation Loss: 0.056272\n",
      "Epoch: 6347, Training Loss: 0.064857, Validation Loss: 0.058988\n",
      "Epoch: 6348, Training Loss: 0.064239, Validation Loss: 0.059708\n",
      "Epoch: 6349, Training Loss: 0.065266, Validation Loss: 0.058382\n",
      "Epoch: 6350, Training Loss: 0.064516, Validation Loss: 0.062913\n",
      "Epoch: 6351, Training Loss: 0.062780, Validation Loss: 0.059697\n",
      "Epoch: 6352, Training Loss: 0.062840, Validation Loss: 0.059004\n",
      "Epoch: 6353, Training Loss: 0.063059, Validation Loss: 0.060035\n",
      "Epoch: 6354, Training Loss: 0.062931, Validation Loss: 0.057384\n",
      "Epoch: 6355, Training Loss: 0.064574, Validation Loss: 0.056911\n",
      "Epoch: 6356, Training Loss: 0.062514, Validation Loss: 0.058650\n",
      "Epoch: 6357, Training Loss: 0.062203, Validation Loss: 0.060911\n",
      "Epoch: 6358, Training Loss: 0.065455, Validation Loss: 0.057606\n",
      "Epoch: 6359, Training Loss: 0.066624, Validation Loss: 0.060133\n",
      "Epoch: 6360, Training Loss: 0.064294, Validation Loss: 0.059397\n",
      "Epoch: 6361, Training Loss: 0.062307, Validation Loss: 0.057351\n",
      "Epoch: 6362, Training Loss: 0.064374, Validation Loss: 0.058122\n",
      "Epoch: 6363, Training Loss: 0.065217, Validation Loss: 0.057703\n",
      "Epoch: 6364, Training Loss: 0.062892, Validation Loss: 0.057411\n",
      "Epoch: 6365, Training Loss: 0.062814, Validation Loss: 0.058691\n",
      "Epoch: 6366, Training Loss: 0.064429, Validation Loss: 0.058030\n",
      "Epoch: 6367, Training Loss: 0.065176, Validation Loss: 0.056554\n",
      "Epoch: 6368, Training Loss: 0.066235, Validation Loss: 0.061870\n",
      "Epoch: 6369, Training Loss: 0.063071, Validation Loss: 0.059299\n",
      "Epoch: 6370, Training Loss: 0.062883, Validation Loss: 0.058756\n",
      "Epoch: 6371, Training Loss: 0.064272, Validation Loss: 0.059709\n",
      "Epoch: 6372, Training Loss: 0.062360, Validation Loss: 0.056520\n",
      "Epoch: 6373, Training Loss: 0.062212, Validation Loss: 0.056128\n",
      "Epoch: 6374, Training Loss: 0.064048, Validation Loss: 0.059350\n",
      "Epoch: 6375, Training Loss: 0.063099, Validation Loss: 0.057949\n",
      "Epoch: 6376, Training Loss: 0.062213, Validation Loss: 0.058910\n",
      "Epoch: 6377, Training Loss: 0.066460, Validation Loss: 0.059195\n",
      "Epoch: 6378, Training Loss: 0.066972, Validation Loss: 0.058357\n",
      "Epoch: 6379, Training Loss: 0.064159, Validation Loss: 0.057793\n",
      "Epoch: 6380, Training Loss: 0.061845, Validation Loss: 0.056406\n",
      "Epoch: 6381, Training Loss: 0.066179, Validation Loss: 0.057033\n",
      "Epoch: 6382, Training Loss: 0.066010, Validation Loss: 0.057328\n",
      "Epoch: 6383, Training Loss: 0.061864, Validation Loss: 0.055913\n",
      "Epoch: 6384, Training Loss: 0.066648, Validation Loss: 0.057590\n",
      "Epoch: 6385, Training Loss: 0.065205, Validation Loss: 0.056176\n",
      "Epoch: 6386, Training Loss: 0.062369, Validation Loss: 0.055361\n",
      "Epoch: 6387, Training Loss: 0.065521, Validation Loss: 0.057813\n",
      "Epoch: 6388, Training Loss: 0.063697, Validation Loss: 0.055845\n",
      "Epoch: 6389, Training Loss: 0.062313, Validation Loss: 0.055595\n",
      "Epoch: 6390, Training Loss: 0.066048, Validation Loss: 0.057601\n",
      "Epoch: 6391, Training Loss: 0.064668, Validation Loss: 0.057044\n",
      "Epoch: 6392, Training Loss: 0.061716, Validation Loss: 0.056035\n",
      "Epoch: 6393, Training Loss: 0.064980, Validation Loss: 0.058827\n",
      "Epoch: 6394, Training Loss: 0.066599, Validation Loss: 0.057786\n",
      "Epoch: 6395, Training Loss: 0.061931, Validation Loss: 0.056203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6396, Training Loss: 0.062765, Validation Loss: 0.056600\n",
      "Epoch: 6397, Training Loss: 0.065437, Validation Loss: 0.057857\n",
      "Epoch: 6398, Training Loss: 0.062629, Validation Loss: 0.056786\n",
      "Epoch: 6399, Training Loss: 0.061600, Validation Loss: 0.056557\n",
      "Epoch: 6400, Training Loss: 0.062556, Validation Loss: 0.058171\n",
      "Epoch: 6401, Training Loss: 0.063000, Validation Loss: 0.058506\n",
      "Epoch: 6402, Training Loss: 0.064559, Validation Loss: 0.060437\n",
      "Epoch: 6403, Training Loss: 0.062089, Validation Loss: 0.058783\n",
      "Epoch: 6404, Training Loss: 0.061352, Validation Loss: 0.057399\n",
      "Epoch: 6405, Training Loss: 0.062357, Validation Loss: 0.057592\n",
      "Epoch: 6406, Training Loss: 0.063142, Validation Loss: 0.057014\n",
      "Epoch: 6407, Training Loss: 0.063089, Validation Loss: 0.058788\n",
      "Epoch: 6408, Training Loss: 0.066835, Validation Loss: 0.061328\n",
      "Epoch: 6409, Training Loss: 0.063230, Validation Loss: 0.057979\n",
      "Epoch: 6410, Training Loss: 0.061399, Validation Loss: 0.059410\n",
      "Epoch: 6411, Training Loss: 0.064187, Validation Loss: 0.060959\n",
      "Epoch: 6412, Training Loss: 0.063615, Validation Loss: 0.057189\n",
      "Epoch: 6413, Training Loss: 0.062057, Validation Loss: 0.056610\n",
      "Epoch: 6414, Training Loss: 0.065244, Validation Loss: 0.060937\n",
      "Epoch: 6415, Training Loss: 0.063856, Validation Loss: 0.057519\n",
      "Epoch: 6416, Training Loss: 0.061287, Validation Loss: 0.057847\n",
      "Epoch: 6417, Training Loss: 0.066686, Validation Loss: 0.064956\n",
      "Epoch: 6418, Training Loss: 0.066427, Validation Loss: 0.059002\n",
      "Epoch: 6419, Training Loss: 0.063169, Validation Loss: 0.059515\n",
      "Epoch: 6420, Training Loss: 0.072622, Validation Loss: 0.071285\n",
      "Epoch: 6421, Training Loss: 0.063881, Validation Loss: 0.059816\n",
      "Epoch: 6422, Training Loss: 0.065598, Validation Loss: 0.056276\n",
      "Epoch: 6423, Training Loss: 0.063574, Validation Loss: 0.059091\n",
      "Epoch: 6424, Training Loss: 0.063973, Validation Loss: 0.056019\n",
      "Epoch: 6425, Training Loss: 0.064978, Validation Loss: 0.055321\n",
      "Epoch: 6426, Training Loss: 0.062407, Validation Loss: 0.055800\n",
      "Epoch: 6427, Training Loss: 0.067766, Validation Loss: 0.060290\n",
      "Epoch: 6428, Training Loss: 0.066020, Validation Loss: 0.056194\n",
      "Epoch: 6429, Training Loss: 0.063516, Validation Loss: 0.055801\n",
      "Epoch: 6430, Training Loss: 0.069623, Validation Loss: 0.060955\n",
      "Epoch: 6431, Training Loss: 0.063358, Validation Loss: 0.056295\n",
      "Epoch: 6432, Training Loss: 0.063818, Validation Loss: 0.056148\n",
      "Epoch: 6433, Training Loss: 0.064579, Validation Loss: 0.059677\n",
      "Epoch: 6434, Training Loss: 0.061761, Validation Loss: 0.057501\n",
      "Epoch: 6435, Training Loss: 0.065344, Validation Loss: 0.056425\n",
      "Epoch: 6436, Training Loss: 0.062422, Validation Loss: 0.057140\n",
      "Epoch: 6437, Training Loss: 0.063364, Validation Loss: 0.059536\n",
      "Epoch: 6438, Training Loss: 0.064401, Validation Loss: 0.058652\n",
      "Epoch: 6439, Training Loss: 0.061667, Validation Loss: 0.058805\n",
      "Epoch: 6440, Training Loss: 0.063160, Validation Loss: 0.060733\n",
      "Epoch: 6441, Training Loss: 0.066779, Validation Loss: 0.058633\n",
      "Epoch: 6442, Training Loss: 0.069501, Validation Loss: 0.062814\n",
      "Epoch: 6443, Training Loss: 0.066458, Validation Loss: 0.062144\n",
      "Epoch: 6444, Training Loss: 0.064001, Validation Loss: 0.057225\n",
      "Epoch: 6445, Training Loss: 0.066793, Validation Loss: 0.060501\n",
      "Epoch: 6446, Training Loss: 0.064187, Validation Loss: 0.058443\n",
      "Epoch: 6447, Training Loss: 0.064307, Validation Loss: 0.055429\n",
      "Epoch: 6448, Training Loss: 0.064084, Validation Loss: 0.058676\n",
      "Epoch: 6449, Training Loss: 0.063052, Validation Loss: 0.058218\n",
      "Epoch: 6450, Training Loss: 0.063669, Validation Loss: 0.056515\n",
      "Epoch: 6451, Training Loss: 0.064715, Validation Loss: 0.058385\n",
      "Epoch: 6452, Training Loss: 0.063892, Validation Loss: 0.056703\n",
      "Epoch: 6453, Training Loss: 0.062986, Validation Loss: 0.056290\n",
      "Epoch: 6454, Training Loss: 0.062193, Validation Loss: 0.057787\n",
      "Epoch: 6455, Training Loss: 0.063758, Validation Loss: 0.059051\n",
      "Epoch: 6456, Training Loss: 0.062807, Validation Loss: 0.058734\n",
      "Epoch: 6457, Training Loss: 0.061708, Validation Loss: 0.059821\n",
      "Epoch: 6458, Training Loss: 0.063499, Validation Loss: 0.060084\n",
      "Epoch: 6459, Training Loss: 0.065247, Validation Loss: 0.060724\n",
      "Epoch: 6460, Training Loss: 0.068523, Validation Loss: 0.062995\n",
      "Epoch: 6461, Training Loss: 0.064141, Validation Loss: 0.058724\n",
      "Epoch: 6462, Training Loss: 0.062557, Validation Loss: 0.056496\n",
      "Epoch: 6463, Training Loss: 0.063289, Validation Loss: 0.059571\n",
      "Epoch: 6464, Training Loss: 0.062674, Validation Loss: 0.056637\n",
      "Epoch: 6465, Training Loss: 0.064306, Validation Loss: 0.056877\n",
      "Epoch: 6466, Training Loss: 0.061544, Validation Loss: 0.057403\n",
      "Epoch: 6467, Training Loss: 0.064028, Validation Loss: 0.057546\n",
      "Epoch: 6468, Training Loss: 0.064940, Validation Loss: 0.056124\n",
      "Epoch: 6469, Training Loss: 0.061575, Validation Loss: 0.056492\n",
      "Epoch: 6470, Training Loss: 0.066158, Validation Loss: 0.060586\n",
      "Epoch: 6471, Training Loss: 0.067395, Validation Loss: 0.056210\n",
      "Epoch: 6472, Training Loss: 0.063195, Validation Loss: 0.056264\n",
      "Epoch: 6473, Training Loss: 0.069055, Validation Loss: 0.062469\n",
      "Epoch: 6474, Training Loss: 0.063363, Validation Loss: 0.058205\n",
      "Epoch: 6475, Training Loss: 0.070833, Validation Loss: 0.056834\n",
      "Epoch: 6476, Training Loss: 0.062970, Validation Loss: 0.055506\n",
      "Epoch: 6477, Training Loss: 0.075154, Validation Loss: 0.073393\n",
      "Epoch: 6478, Training Loss: 0.069869, Validation Loss: 0.062634\n",
      "Epoch: 6479, Training Loss: 0.066688, Validation Loss: 0.058069\n",
      "Epoch: 6480, Training Loss: 0.068484, Validation Loss: 0.058293\n",
      "Epoch: 6481, Training Loss: 0.066431, Validation Loss: 0.062927\n",
      "Epoch: 6482, Training Loss: 0.065688, Validation Loss: 0.060947\n",
      "Epoch: 6483, Training Loss: 0.066414, Validation Loss: 0.057982\n",
      "Epoch: 6484, Training Loss: 0.064489, Validation Loss: 0.057491\n",
      "Epoch: 6485, Training Loss: 0.067666, Validation Loss: 0.063707\n",
      "Epoch: 6486, Training Loss: 0.065835, Validation Loss: 0.063256\n",
      "Epoch: 6487, Training Loss: 0.071162, Validation Loss: 0.057666\n",
      "Epoch: 6488, Training Loss: 0.068017, Validation Loss: 0.058803\n",
      "Epoch: 6489, Training Loss: 0.074268, Validation Loss: 0.071899\n",
      "Epoch: 6490, Training Loss: 0.066908, Validation Loss: 0.058806\n",
      "Epoch: 6491, Training Loss: 0.065519, Validation Loss: 0.058452\n",
      "Epoch: 6492, Training Loss: 0.067150, Validation Loss: 0.060000\n",
      "Epoch: 6493, Training Loss: 0.067456, Validation Loss: 0.063323\n",
      "Epoch: 6494, Training Loss: 0.064279, Validation Loss: 0.057419\n",
      "Epoch: 6495, Training Loss: 0.066570, Validation Loss: 0.056266\n",
      "Epoch: 6496, Training Loss: 0.063357, Validation Loss: 0.055631\n",
      "Epoch: 6497, Training Loss: 0.065780, Validation Loss: 0.057843\n",
      "Epoch: 6498, Training Loss: 0.063254, Validation Loss: 0.054594\n",
      "Epoch: 6499, Training Loss: 0.064199, Validation Loss: 0.053975\n",
      "Epoch: 6500, Training Loss: 0.064640, Validation Loss: 0.055006\n",
      "Epoch: 6501, Training Loss: 0.063248, Validation Loss: 0.056156\n",
      "Epoch: 6502, Training Loss: 0.063284, Validation Loss: 0.056462\n",
      "Epoch: 6503, Training Loss: 0.062106, Validation Loss: 0.058441\n",
      "Epoch: 6504, Training Loss: 0.064176, Validation Loss: 0.063283\n",
      "Epoch: 6505, Training Loss: 0.062685, Validation Loss: 0.060823\n",
      "Epoch: 6506, Training Loss: 0.062478, Validation Loss: 0.061112\n",
      "Epoch: 6507, Training Loss: 0.062979, Validation Loss: 0.062134\n",
      "Epoch: 6508, Training Loss: 0.062206, Validation Loss: 0.059264\n",
      "Epoch: 6509, Training Loss: 0.063567, Validation Loss: 0.060668\n",
      "Epoch: 6510, Training Loss: 0.066743, Validation Loss: 0.060388\n",
      "Epoch: 6511, Training Loss: 0.066331, Validation Loss: 0.064269\n",
      "Epoch: 6512, Training Loss: 0.063661, Validation Loss: 0.061645\n",
      "Epoch: 6513, Training Loss: 0.062320, Validation Loss: 0.060190\n",
      "Epoch: 6514, Training Loss: 0.063393, Validation Loss: 0.059020\n",
      "Epoch: 6515, Training Loss: 0.062694, Validation Loss: 0.055487\n",
      "Epoch: 6516, Training Loss: 0.062381, Validation Loss: 0.054144\n",
      "Epoch: 6517, Training Loss: 0.063011, Validation Loss: 0.057925\n",
      "Epoch: 6518, Training Loss: 0.062620, Validation Loss: 0.055352\n",
      "Epoch: 6519, Training Loss: 0.062710, Validation Loss: 0.055595\n",
      "Epoch: 6520, Training Loss: 0.062175, Validation Loss: 0.057377\n",
      "Epoch: 6521, Training Loss: 0.061860, Validation Loss: 0.057281\n",
      "Epoch: 6522, Training Loss: 0.061892, Validation Loss: 0.057415\n",
      "Epoch: 6523, Training Loss: 0.061606, Validation Loss: 0.057854\n",
      "Epoch: 6524, Training Loss: 0.061068, Validation Loss: 0.057134\n",
      "Epoch: 6525, Training Loss: 0.061403, Validation Loss: 0.058130\n",
      "Epoch: 6526, Training Loss: 0.062522, Validation Loss: 0.060832\n",
      "Epoch: 6527, Training Loss: 0.062657, Validation Loss: 0.059896\n",
      "Epoch: 6528, Training Loss: 0.064686, Validation Loss: 0.061724\n",
      "Epoch: 6529, Training Loss: 0.068593, Validation Loss: 0.060025\n",
      "Epoch: 6530, Training Loss: 0.066156, Validation Loss: 0.062699\n",
      "Epoch: 6531, Training Loss: 0.061983, Validation Loss: 0.060906\n",
      "Epoch: 6532, Training Loss: 0.062962, Validation Loss: 0.058382\n",
      "Epoch: 6533, Training Loss: 0.067187, Validation Loss: 0.060115\n",
      "Epoch: 6534, Training Loss: 0.062590, Validation Loss: 0.055839\n",
      "Epoch: 6535, Training Loss: 0.062873, Validation Loss: 0.053856\n",
      "Epoch: 6536, Training Loss: 0.064442, Validation Loss: 0.057733\n",
      "Epoch: 6537, Training Loss: 0.062677, Validation Loss: 0.055522\n",
      "Epoch: 6538, Training Loss: 0.063201, Validation Loss: 0.053925\n",
      "Epoch: 6539, Training Loss: 0.062478, Validation Loss: 0.056195\n",
      "Epoch: 6540, Training Loss: 0.061830, Validation Loss: 0.055728\n",
      "Epoch: 6541, Training Loss: 0.063301, Validation Loss: 0.054482\n",
      "Epoch: 6542, Training Loss: 0.062243, Validation Loss: 0.055869\n",
      "Epoch: 6543, Training Loss: 0.062560, Validation Loss: 0.056471\n",
      "Epoch: 6544, Training Loss: 0.063633, Validation Loss: 0.056056\n",
      "Epoch: 6545, Training Loss: 0.062213, Validation Loss: 0.056568\n",
      "Epoch: 6546, Training Loss: 0.061654, Validation Loss: 0.057339\n",
      "Epoch: 6547, Training Loss: 0.061988, Validation Loss: 0.057466\n",
      "Epoch: 6548, Training Loss: 0.062245, Validation Loss: 0.059847\n",
      "Epoch: 6549, Training Loss: 0.062081, Validation Loss: 0.059826\n",
      "Epoch: 6550, Training Loss: 0.061779, Validation Loss: 0.060997\n",
      "Epoch: 6551, Training Loss: 0.062065, Validation Loss: 0.061608\n",
      "Epoch: 6552, Training Loss: 0.066183, Validation Loss: 0.059034\n",
      "Epoch: 6553, Training Loss: 0.077874, Validation Loss: 0.072219\n",
      "Epoch: 6554, Training Loss: 0.075485, Validation Loss: 0.064774\n",
      "Epoch: 6555, Training Loss: 0.062744, Validation Loss: 0.059076\n",
      "Epoch: 6556, Training Loss: 0.069814, Validation Loss: 0.063066\n",
      "Epoch: 6557, Training Loss: 0.066875, Validation Loss: 0.058874\n",
      "Epoch: 6558, Training Loss: 0.067175, Validation Loss: 0.055286\n",
      "Epoch: 6559, Training Loss: 0.066763, Validation Loss: 0.056898\n",
      "Epoch: 6560, Training Loss: 0.067011, Validation Loss: 0.057262\n",
      "Epoch: 6561, Training Loss: 0.064598, Validation Loss: 0.052663\n",
      "Epoch: 6562, Training Loss: 0.067523, Validation Loss: 0.054719\n",
      "Epoch: 6563, Training Loss: 0.064438, Validation Loss: 0.052348\n",
      "Epoch: 6564, Training Loss: 0.066844, Validation Loss: 0.054188\n",
      "Epoch: 6565, Training Loss: 0.063823, Validation Loss: 0.052309\n",
      "Epoch: 6566, Training Loss: 0.065174, Validation Loss: 0.053796\n",
      "Epoch: 6567, Training Loss: 0.064377, Validation Loss: 0.053517\n",
      "Epoch: 6568, Training Loss: 0.063821, Validation Loss: 0.056826\n",
      "Epoch: 6569, Training Loss: 0.062200, Validation Loss: 0.056401\n",
      "Epoch: 6570, Training Loss: 0.064010, Validation Loss: 0.054955\n",
      "Epoch: 6571, Training Loss: 0.063253, Validation Loss: 0.060504\n",
      "Epoch: 6572, Training Loss: 0.061590, Validation Loss: 0.058036\n",
      "Epoch: 6573, Training Loss: 0.066927, Validation Loss: 0.055860\n",
      "Epoch: 6574, Training Loss: 0.073518, Validation Loss: 0.067345\n",
      "Epoch: 6575, Training Loss: 0.070433, Validation Loss: 0.062333\n",
      "Epoch: 6576, Training Loss: 0.063772, Validation Loss: 0.058697\n",
      "Epoch: 6577, Training Loss: 0.062754, Validation Loss: 0.059054\n",
      "Epoch: 6578, Training Loss: 0.064964, Validation Loss: 0.058250\n",
      "Epoch: 6579, Training Loss: 0.064686, Validation Loss: 0.056176\n",
      "Epoch: 6580, Training Loss: 0.062432, Validation Loss: 0.057017\n",
      "Epoch: 6581, Training Loss: 0.064133, Validation Loss: 0.056462\n",
      "Epoch: 6582, Training Loss: 0.062656, Validation Loss: 0.055405\n",
      "Epoch: 6583, Training Loss: 0.061974, Validation Loss: 0.055446\n",
      "Epoch: 6584, Training Loss: 0.062051, Validation Loss: 0.054315\n",
      "Epoch: 6585, Training Loss: 0.064624, Validation Loss: 0.058345\n",
      "Epoch: 6586, Training Loss: 0.064283, Validation Loss: 0.057250\n",
      "Epoch: 6587, Training Loss: 0.061927, Validation Loss: 0.060683\n",
      "Epoch: 6588, Training Loss: 0.063366, Validation Loss: 0.060779\n",
      "Epoch: 6589, Training Loss: 0.061543, Validation Loss: 0.059997\n",
      "Epoch: 6590, Training Loss: 0.061825, Validation Loss: 0.060701\n",
      "Epoch: 6591, Training Loss: 0.062688, Validation Loss: 0.058635\n",
      "Epoch: 6592, Training Loss: 0.063806, Validation Loss: 0.061997\n",
      "Epoch: 6593, Training Loss: 0.061202, Validation Loss: 0.057511\n",
      "Epoch: 6594, Training Loss: 0.061123, Validation Loss: 0.058113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6595, Training Loss: 0.062909, Validation Loss: 0.059872\n",
      "Epoch: 6596, Training Loss: 0.061640, Validation Loss: 0.058028\n",
      "Epoch: 6597, Training Loss: 0.061768, Validation Loss: 0.057386\n",
      "Epoch: 6598, Training Loss: 0.061800, Validation Loss: 0.059279\n",
      "Epoch: 6599, Training Loss: 0.062031, Validation Loss: 0.057595\n",
      "Epoch: 6600, Training Loss: 0.062625, Validation Loss: 0.058425\n",
      "Epoch: 6601, Training Loss: 0.063157, Validation Loss: 0.058228\n",
      "Epoch: 6602, Training Loss: 0.061625, Validation Loss: 0.058178\n",
      "Epoch: 6603, Training Loss: 0.062124, Validation Loss: 0.056882\n",
      "Epoch: 6604, Training Loss: 0.063640, Validation Loss: 0.059558\n",
      "Epoch: 6605, Training Loss: 0.065948, Validation Loss: 0.057465\n",
      "Epoch: 6606, Training Loss: 0.065284, Validation Loss: 0.059067\n",
      "Epoch: 6607, Training Loss: 0.061733, Validation Loss: 0.057304\n",
      "Epoch: 6608, Training Loss: 0.061480, Validation Loss: 0.056855\n",
      "Epoch: 6609, Training Loss: 0.063231, Validation Loss: 0.058525\n",
      "Epoch: 6610, Training Loss: 0.061185, Validation Loss: 0.055181\n",
      "Epoch: 6611, Training Loss: 0.062163, Validation Loss: 0.054899\n",
      "Epoch: 6612, Training Loss: 0.062756, Validation Loss: 0.058390\n",
      "Epoch: 6613, Training Loss: 0.061469, Validation Loss: 0.055485\n",
      "Epoch: 6614, Training Loss: 0.061316, Validation Loss: 0.055694\n",
      "Epoch: 6615, Training Loss: 0.062626, Validation Loss: 0.057410\n",
      "Epoch: 6616, Training Loss: 0.062762, Validation Loss: 0.056987\n",
      "Epoch: 6617, Training Loss: 0.061069, Validation Loss: 0.056594\n",
      "Epoch: 6618, Training Loss: 0.060654, Validation Loss: 0.057197\n",
      "Epoch: 6619, Training Loss: 0.060710, Validation Loss: 0.057127\n",
      "Epoch: 6620, Training Loss: 0.062524, Validation Loss: 0.058848\n",
      "Epoch: 6621, Training Loss: 0.061033, Validation Loss: 0.057288\n",
      "Epoch: 6622, Training Loss: 0.060754, Validation Loss: 0.058511\n",
      "Epoch: 6623, Training Loss: 0.060760, Validation Loss: 0.060111\n",
      "Epoch: 6624, Training Loss: 0.064143, Validation Loss: 0.060758\n",
      "Epoch: 6625, Training Loss: 0.066929, Validation Loss: 0.064255\n",
      "Epoch: 6626, Training Loss: 0.071364, Validation Loss: 0.063089\n",
      "Epoch: 6627, Training Loss: 0.068951, Validation Loss: 0.064304\n",
      "Epoch: 6628, Training Loss: 0.061630, Validation Loss: 0.057694\n",
      "Epoch: 6629, Training Loss: 0.066692, Validation Loss: 0.057035\n",
      "Epoch: 6630, Training Loss: 0.064289, Validation Loss: 0.059023\n",
      "Epoch: 6631, Training Loss: 0.064613, Validation Loss: 0.057573\n",
      "Epoch: 6632, Training Loss: 0.063712, Validation Loss: 0.052937\n",
      "Epoch: 6633, Training Loss: 0.063906, Validation Loss: 0.053638\n",
      "Epoch: 6634, Training Loss: 0.063481, Validation Loss: 0.053984\n",
      "Epoch: 6635, Training Loss: 0.065069, Validation Loss: 0.054666\n",
      "Epoch: 6636, Training Loss: 0.063894, Validation Loss: 0.051588\n",
      "Epoch: 6637, Training Loss: 0.064116, Validation Loss: 0.054281\n",
      "Epoch: 6638, Training Loss: 0.065248, Validation Loss: 0.056648\n",
      "Epoch: 6639, Training Loss: 0.061889, Validation Loss: 0.054652\n",
      "Epoch: 6640, Training Loss: 0.062427, Validation Loss: 0.055786\n",
      "Epoch: 6641, Training Loss: 0.060847, Validation Loss: 0.055655\n",
      "Epoch: 6642, Training Loss: 0.062256, Validation Loss: 0.057229\n",
      "Epoch: 6643, Training Loss: 0.061529, Validation Loss: 0.057008\n",
      "Epoch: 6644, Training Loss: 0.061611, Validation Loss: 0.058126\n",
      "Epoch: 6645, Training Loss: 0.061202, Validation Loss: 0.059345\n",
      "Epoch: 6646, Training Loss: 0.060728, Validation Loss: 0.059398\n",
      "Epoch: 6647, Training Loss: 0.061439, Validation Loss: 0.060333\n",
      "Epoch: 6648, Training Loss: 0.060851, Validation Loss: 0.059199\n",
      "Epoch: 6649, Training Loss: 0.062066, Validation Loss: 0.060967\n",
      "Epoch: 6650, Training Loss: 0.061312, Validation Loss: 0.058510\n",
      "Epoch: 6651, Training Loss: 0.061192, Validation Loss: 0.059086\n",
      "Epoch: 6652, Training Loss: 0.061168, Validation Loss: 0.057693\n",
      "Epoch: 6653, Training Loss: 0.061069, Validation Loss: 0.058209\n",
      "Epoch: 6654, Training Loss: 0.061031, Validation Loss: 0.057470\n",
      "Epoch: 6655, Training Loss: 0.060313, Validation Loss: 0.057368\n",
      "Epoch: 6656, Training Loss: 0.061435, Validation Loss: 0.058018\n",
      "Epoch: 6657, Training Loss: 0.064438, Validation Loss: 0.059427\n",
      "Epoch: 6658, Training Loss: 0.063265, Validation Loss: 0.059530\n",
      "Epoch: 6659, Training Loss: 0.062215, Validation Loss: 0.058973\n",
      "Epoch: 6660, Training Loss: 0.061011, Validation Loss: 0.058523\n",
      "Epoch: 6661, Training Loss: 0.061895, Validation Loss: 0.058379\n",
      "Epoch: 6662, Training Loss: 0.061067, Validation Loss: 0.057538\n",
      "Epoch: 6663, Training Loss: 0.060268, Validation Loss: 0.057442\n",
      "Epoch: 6664, Training Loss: 0.059741, Validation Loss: 0.057278\n",
      "Epoch: 6665, Training Loss: 0.060154, Validation Loss: 0.057783\n",
      "Epoch: 6666, Training Loss: 0.062020, Validation Loss: 0.059163\n",
      "Epoch: 6667, Training Loss: 0.065320, Validation Loss: 0.057823\n",
      "Epoch: 6668, Training Loss: 0.068602, Validation Loss: 0.063008\n",
      "Epoch: 6669, Training Loss: 0.066217, Validation Loss: 0.059504\n",
      "Epoch: 6670, Training Loss: 0.060909, Validation Loss: 0.056245\n",
      "Epoch: 6671, Training Loss: 0.062361, Validation Loss: 0.058058\n",
      "Epoch: 6672, Training Loss: 0.064178, Validation Loss: 0.056976\n",
      "Epoch: 6673, Training Loss: 0.063002, Validation Loss: 0.054631\n",
      "Epoch: 6674, Training Loss: 0.061623, Validation Loss: 0.055853\n",
      "Epoch: 6675, Training Loss: 0.068213, Validation Loss: 0.059724\n",
      "Epoch: 6676, Training Loss: 0.060368, Validation Loss: 0.054256\n",
      "Epoch: 6677, Training Loss: 0.065785, Validation Loss: 0.057403\n",
      "Epoch: 6678, Training Loss: 0.068247, Validation Loss: 0.058970\n",
      "Epoch: 6679, Training Loss: 0.060621, Validation Loss: 0.057804\n",
      "Epoch: 6680, Training Loss: 0.063656, Validation Loss: 0.057862\n",
      "Epoch: 6681, Training Loss: 0.069023, Validation Loss: 0.059952\n",
      "Epoch: 6682, Training Loss: 0.062374, Validation Loss: 0.061658\n",
      "Epoch: 6683, Training Loss: 0.067180, Validation Loss: 0.058951\n",
      "Epoch: 6684, Training Loss: 0.065151, Validation Loss: 0.057619\n",
      "Epoch: 6685, Training Loss: 0.070188, Validation Loss: 0.066311\n",
      "Epoch: 6686, Training Loss: 0.061552, Validation Loss: 0.056786\n",
      "Epoch: 6687, Training Loss: 0.066147, Validation Loss: 0.056319\n",
      "Epoch: 6688, Training Loss: 0.062899, Validation Loss: 0.058736\n",
      "Epoch: 6689, Training Loss: 0.061428, Validation Loss: 0.057827\n",
      "Epoch: 6690, Training Loss: 0.065817, Validation Loss: 0.056953\n",
      "Epoch: 6691, Training Loss: 0.060804, Validation Loss: 0.055435\n",
      "Epoch: 6692, Training Loss: 0.065647, Validation Loss: 0.062714\n",
      "Epoch: 6693, Training Loss: 0.066026, Validation Loss: 0.059729\n",
      "Epoch: 6694, Training Loss: 0.062529, Validation Loss: 0.057908\n",
      "Epoch: 6695, Training Loss: 0.069015, Validation Loss: 0.065201\n",
      "Epoch: 6696, Training Loss: 0.063146, Validation Loss: 0.056807\n",
      "Epoch: 6697, Training Loss: 0.067981, Validation Loss: 0.062182\n",
      "Epoch: 6698, Training Loss: 0.068797, Validation Loss: 0.059874\n",
      "Epoch: 6699, Training Loss: 0.060784, Validation Loss: 0.055839\n",
      "Epoch: 6700, Training Loss: 0.070126, Validation Loss: 0.060966\n",
      "Epoch: 6701, Training Loss: 0.062828, Validation Loss: 0.055454\n",
      "Epoch: 6702, Training Loss: 0.064163, Validation Loss: 0.058148\n",
      "Epoch: 6703, Training Loss: 0.063114, Validation Loss: 0.057194\n",
      "Epoch: 6704, Training Loss: 0.062114, Validation Loss: 0.056943\n",
      "Epoch: 6705, Training Loss: 0.062215, Validation Loss: 0.056116\n",
      "Epoch: 6706, Training Loss: 0.062950, Validation Loss: 0.057003\n",
      "Epoch: 6707, Training Loss: 0.061050, Validation Loss: 0.055417\n",
      "Epoch: 6708, Training Loss: 0.060990, Validation Loss: 0.055310\n",
      "Epoch: 6709, Training Loss: 0.063277, Validation Loss: 0.057122\n",
      "Epoch: 6710, Training Loss: 0.061521, Validation Loss: 0.054721\n",
      "Epoch: 6711, Training Loss: 0.061372, Validation Loss: 0.057235\n",
      "Epoch: 6712, Training Loss: 0.059857, Validation Loss: 0.055288\n",
      "Epoch: 6713, Training Loss: 0.062818, Validation Loss: 0.056770\n",
      "Epoch: 6714, Training Loss: 0.061810, Validation Loss: 0.057002\n",
      "Epoch: 6715, Training Loss: 0.060570, Validation Loss: 0.057117\n",
      "Epoch: 6716, Training Loss: 0.060218, Validation Loss: 0.057310\n",
      "Epoch: 6717, Training Loss: 0.060329, Validation Loss: 0.057943\n",
      "Epoch: 6718, Training Loss: 0.063485, Validation Loss: 0.060584\n",
      "Epoch: 6719, Training Loss: 0.062155, Validation Loss: 0.059500\n",
      "Epoch: 6720, Training Loss: 0.062174, Validation Loss: 0.057980\n",
      "Epoch: 6721, Training Loss: 0.064297, Validation Loss: 0.062304\n",
      "Epoch: 6722, Training Loss: 0.065839, Validation Loss: 0.058437\n",
      "Epoch: 6723, Training Loss: 0.066794, Validation Loss: 0.061991\n",
      "Epoch: 6724, Training Loss: 0.062512, Validation Loss: 0.057717\n",
      "Epoch: 6725, Training Loss: 0.063239, Validation Loss: 0.055363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6726, Training Loss: 0.063477, Validation Loss: 0.058485\n",
      "Epoch: 6727, Training Loss: 0.062194, Validation Loss: 0.057356\n",
      "Epoch: 6728, Training Loss: 0.063226, Validation Loss: 0.054164\n",
      "Epoch: 6729, Training Loss: 0.060616, Validation Loss: 0.054931\n",
      "Epoch: 6730, Training Loss: 0.060668, Validation Loss: 0.054963\n",
      "Epoch: 6731, Training Loss: 0.062826, Validation Loss: 0.054534\n",
      "Epoch: 6732, Training Loss: 0.061228, Validation Loss: 0.054191\n",
      "Epoch: 6733, Training Loss: 0.060818, Validation Loss: 0.054721\n",
      "Epoch: 6734, Training Loss: 0.060179, Validation Loss: 0.055243\n",
      "Epoch: 6735, Training Loss: 0.059940, Validation Loss: 0.056896\n",
      "Epoch: 6736, Training Loss: 0.060151, Validation Loss: 0.057660\n",
      "Epoch: 6737, Training Loss: 0.060662, Validation Loss: 0.058193\n",
      "Epoch: 6738, Training Loss: 0.061316, Validation Loss: 0.059638\n",
      "Epoch: 6739, Training Loss: 0.060867, Validation Loss: 0.060244\n",
      "Epoch: 6740, Training Loss: 0.060119, Validation Loss: 0.061150\n",
      "Epoch: 6741, Training Loss: 0.059537, Validation Loss: 0.060654\n",
      "Epoch: 6742, Training Loss: 0.060488, Validation Loss: 0.061646\n",
      "Epoch: 6743, Training Loss: 0.063012, Validation Loss: 0.059552\n",
      "Epoch: 6744, Training Loss: 0.070381, Validation Loss: 0.065655\n",
      "Epoch: 6745, Training Loss: 0.075423, Validation Loss: 0.061162\n",
      "Epoch: 6746, Training Loss: 0.062576, Validation Loss: 0.060465\n",
      "Epoch: 6747, Training Loss: 0.064074, Validation Loss: 0.059469\n",
      "Epoch: 6748, Training Loss: 0.067428, Validation Loss: 0.055268\n",
      "Epoch: 6749, Training Loss: 0.062306, Validation Loss: 0.052737\n",
      "Epoch: 6750, Training Loss: 0.064817, Validation Loss: 0.055241\n",
      "Epoch: 6751, Training Loss: 0.063754, Validation Loss: 0.053796\n",
      "Epoch: 6752, Training Loss: 0.062918, Validation Loss: 0.051580\n",
      "Epoch: 6753, Training Loss: 0.063792, Validation Loss: 0.055558\n",
      "Epoch: 6754, Training Loss: 0.061785, Validation Loss: 0.053654\n",
      "Epoch: 6755, Training Loss: 0.063847, Validation Loss: 0.053923\n",
      "Epoch: 6756, Training Loss: 0.063075, Validation Loss: 0.054984\n",
      "Epoch: 6757, Training Loss: 0.060609, Validation Loss: 0.054795\n",
      "Epoch: 6758, Training Loss: 0.063221, Validation Loss: 0.056498\n",
      "Epoch: 6759, Training Loss: 0.060634, Validation Loss: 0.056576\n",
      "Epoch: 6760, Training Loss: 0.062131, Validation Loss: 0.058483\n",
      "Epoch: 6761, Training Loss: 0.060929, Validation Loss: 0.057753\n",
      "Epoch: 6762, Training Loss: 0.060659, Validation Loss: 0.060770\n",
      "Epoch: 6763, Training Loss: 0.060470, Validation Loss: 0.060532\n",
      "Epoch: 6764, Training Loss: 0.059597, Validation Loss: 0.061355\n",
      "Epoch: 6765, Training Loss: 0.061534, Validation Loss: 0.063280\n",
      "Epoch: 6766, Training Loss: 0.061208, Validation Loss: 0.062718\n",
      "Epoch: 6767, Training Loss: 0.066424, Validation Loss: 0.060303\n",
      "Epoch: 6768, Training Loss: 0.071431, Validation Loss: 0.070895\n",
      "Epoch: 6769, Training Loss: 0.079729, Validation Loss: 0.062425\n",
      "Epoch: 6770, Training Loss: 0.067627, Validation Loss: 0.068262\n",
      "Epoch: 6771, Training Loss: 0.067379, Validation Loss: 0.061890\n",
      "Epoch: 6772, Training Loss: 0.068038, Validation Loss: 0.057873\n",
      "Epoch: 6773, Training Loss: 0.067318, Validation Loss: 0.055455\n",
      "Epoch: 6774, Training Loss: 0.065992, Validation Loss: 0.054877\n",
      "Epoch: 6775, Training Loss: 0.067169, Validation Loss: 0.055366\n",
      "Epoch: 6776, Training Loss: 0.063776, Validation Loss: 0.051190\n",
      "Epoch: 6777, Training Loss: 0.066881, Validation Loss: 0.053870\n",
      "Epoch: 6778, Training Loss: 0.061984, Validation Loss: 0.050360\n",
      "Epoch: 6779, Training Loss: 0.064884, Validation Loss: 0.054732\n",
      "Epoch: 6780, Training Loss: 0.063506, Validation Loss: 0.053276\n",
      "Epoch: 6781, Training Loss: 0.063693, Validation Loss: 0.051406\n",
      "Epoch: 6782, Training Loss: 0.063420, Validation Loss: 0.056081\n",
      "Epoch: 6783, Training Loss: 0.065279, Validation Loss: 0.057880\n",
      "Epoch: 6784, Training Loss: 0.062132, Validation Loss: 0.055664\n",
      "Epoch: 6785, Training Loss: 0.061548, Validation Loss: 0.058096\n",
      "Epoch: 6786, Training Loss: 0.064594, Validation Loss: 0.059884\n",
      "Epoch: 6787, Training Loss: 0.060894, Validation Loss: 0.058095\n",
      "Epoch: 6788, Training Loss: 0.062508, Validation Loss: 0.059153\n",
      "Epoch: 6789, Training Loss: 0.063465, Validation Loss: 0.060053\n",
      "Epoch: 6790, Training Loss: 0.063911, Validation Loss: 0.063417\n",
      "Epoch: 6791, Training Loss: 0.059732, Validation Loss: 0.059213\n",
      "Epoch: 6792, Training Loss: 0.060598, Validation Loss: 0.060414\n",
      "Epoch: 6793, Training Loss: 0.061310, Validation Loss: 0.060135\n",
      "Epoch: 6794, Training Loss: 0.061083, Validation Loss: 0.060101\n",
      "Epoch: 6795, Training Loss: 0.060292, Validation Loss: 0.059694\n",
      "Epoch: 6796, Training Loss: 0.059750, Validation Loss: 0.058248\n",
      "Epoch: 6797, Training Loss: 0.061489, Validation Loss: 0.060913\n",
      "Epoch: 6798, Training Loss: 0.062062, Validation Loss: 0.057250\n",
      "Epoch: 6799, Training Loss: 0.062899, Validation Loss: 0.059392\n",
      "Epoch: 6800, Training Loss: 0.060262, Validation Loss: 0.058342\n",
      "Epoch: 6801, Training Loss: 0.059759, Validation Loss: 0.057477\n",
      "Epoch: 6802, Training Loss: 0.059979, Validation Loss: 0.058465\n",
      "Epoch: 6803, Training Loss: 0.061936, Validation Loss: 0.059907\n",
      "Epoch: 6804, Training Loss: 0.061205, Validation Loss: 0.058147\n",
      "Epoch: 6805, Training Loss: 0.059885, Validation Loss: 0.058296\n",
      "Epoch: 6806, Training Loss: 0.062607, Validation Loss: 0.058508\n",
      "Epoch: 6807, Training Loss: 0.061994, Validation Loss: 0.056498\n",
      "Epoch: 6808, Training Loss: 0.060048, Validation Loss: 0.057281\n",
      "Epoch: 6809, Training Loss: 0.065959, Validation Loss: 0.058992\n",
      "Epoch: 6810, Training Loss: 0.063072, Validation Loss: 0.056609\n",
      "Epoch: 6811, Training Loss: 0.061811, Validation Loss: 0.057564\n",
      "Epoch: 6812, Training Loss: 0.069077, Validation Loss: 0.063205\n",
      "Epoch: 6813, Training Loss: 0.060149, Validation Loss: 0.057300\n",
      "Epoch: 6814, Training Loss: 0.072809, Validation Loss: 0.060097\n",
      "Epoch: 6815, Training Loss: 0.063811, Validation Loss: 0.057656\n",
      "Epoch: 6816, Training Loss: 0.075257, Validation Loss: 0.070692\n",
      "Epoch: 6817, Training Loss: 0.070885, Validation Loss: 0.066414\n",
      "Epoch: 6818, Training Loss: 0.065966, Validation Loss: 0.058018\n",
      "Epoch: 6819, Training Loss: 0.069092, Validation Loss: 0.059018\n",
      "Epoch: 6820, Training Loss: 0.063439, Validation Loss: 0.059764\n",
      "Epoch: 6821, Training Loss: 0.065157, Validation Loss: 0.059183\n",
      "Epoch: 6822, Training Loss: 0.062706, Validation Loss: 0.056496\n",
      "Epoch: 6823, Training Loss: 0.062564, Validation Loss: 0.057123\n",
      "Epoch: 6824, Training Loss: 0.065239, Validation Loss: 0.059589\n",
      "Epoch: 6825, Training Loss: 0.060872, Validation Loss: 0.058861\n",
      "Epoch: 6826, Training Loss: 0.067516, Validation Loss: 0.060121\n",
      "Epoch: 6827, Training Loss: 0.063707, Validation Loss: 0.053901\n",
      "Epoch: 6828, Training Loss: 0.079950, Validation Loss: 0.080163\n",
      "Epoch: 6829, Training Loss: 0.064806, Validation Loss: 0.059486\n",
      "Epoch: 6830, Training Loss: 0.065889, Validation Loss: 0.058702\n",
      "Epoch: 6831, Training Loss: 0.068161, Validation Loss: 0.069163\n",
      "Epoch: 6832, Training Loss: 0.062593, Validation Loss: 0.059690\n",
      "Epoch: 6833, Training Loss: 0.065510, Validation Loss: 0.055777\n",
      "Epoch: 6834, Training Loss: 0.064642, Validation Loss: 0.058285\n",
      "Epoch: 6835, Training Loss: 0.065233, Validation Loss: 0.062142\n",
      "Epoch: 6836, Training Loss: 0.065537, Validation Loss: 0.059870\n",
      "Epoch: 6837, Training Loss: 0.062243, Validation Loss: 0.054855\n",
      "Epoch: 6838, Training Loss: 0.062166, Validation Loss: 0.056669\n",
      "Epoch: 6839, Training Loss: 0.062524, Validation Loss: 0.057400\n",
      "Epoch: 6840, Training Loss: 0.062639, Validation Loss: 0.055491\n",
      "Epoch: 6841, Training Loss: 0.062595, Validation Loss: 0.058449\n",
      "Epoch: 6842, Training Loss: 0.059171, Validation Loss: 0.056387\n",
      "Epoch: 6843, Training Loss: 0.065273, Validation Loss: 0.061308\n",
      "Epoch: 6844, Training Loss: 0.061795, Validation Loss: 0.059343\n",
      "Epoch: 6845, Training Loss: 0.061191, Validation Loss: 0.059398\n",
      "Epoch: 6846, Training Loss: 0.059034, Validation Loss: 0.060231\n",
      "Epoch: 6847, Training Loss: 0.061110, Validation Loss: 0.062376\n",
      "Epoch: 6848, Training Loss: 0.067181, Validation Loss: 0.059268\n",
      "Epoch: 6849, Training Loss: 0.062020, Validation Loss: 0.063139\n",
      "Epoch: 6850, Training Loss: 0.060219, Validation Loss: 0.060495\n",
      "Epoch: 6851, Training Loss: 0.060214, Validation Loss: 0.057994\n",
      "Epoch: 6852, Training Loss: 0.062893, Validation Loss: 0.060902\n",
      "Epoch: 6853, Training Loss: 0.060482, Validation Loss: 0.056508\n",
      "Epoch: 6854, Training Loss: 0.059241, Validation Loss: 0.058197\n",
      "Epoch: 6855, Training Loss: 0.059236, Validation Loss: 0.058014\n",
      "Epoch: 6856, Training Loss: 0.060993, Validation Loss: 0.054492\n",
      "Epoch: 6857, Training Loss: 0.060325, Validation Loss: 0.057611\n",
      "Epoch: 6858, Training Loss: 0.060364, Validation Loss: 0.057458\n",
      "Epoch: 6859, Training Loss: 0.059978, Validation Loss: 0.055479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6860, Training Loss: 0.058651, Validation Loss: 0.057299\n",
      "Epoch: 6861, Training Loss: 0.058727, Validation Loss: 0.056223\n",
      "Epoch: 6862, Training Loss: 0.059184, Validation Loss: 0.055671\n",
      "Epoch: 6863, Training Loss: 0.059943, Validation Loss: 0.057803\n",
      "Epoch: 6864, Training Loss: 0.059253, Validation Loss: 0.056966\n",
      "Epoch: 6865, Training Loss: 0.059501, Validation Loss: 0.055981\n",
      "Epoch: 6866, Training Loss: 0.060854, Validation Loss: 0.059101\n",
      "Epoch: 6867, Training Loss: 0.061545, Validation Loss: 0.054635\n",
      "Epoch: 6868, Training Loss: 0.060026, Validation Loss: 0.059131\n",
      "Epoch: 6869, Training Loss: 0.058870, Validation Loss: 0.056606\n",
      "Epoch: 6870, Training Loss: 0.058410, Validation Loss: 0.056838\n",
      "Epoch: 6871, Training Loss: 0.059237, Validation Loss: 0.060486\n",
      "Epoch: 6872, Training Loss: 0.059264, Validation Loss: 0.059020\n",
      "Epoch: 6873, Training Loss: 0.059539, Validation Loss: 0.060960\n",
      "Epoch: 6874, Training Loss: 0.059257, Validation Loss: 0.059830\n",
      "Epoch: 6875, Training Loss: 0.058713, Validation Loss: 0.058833\n",
      "Epoch: 6876, Training Loss: 0.058500, Validation Loss: 0.058990\n",
      "Epoch: 6877, Training Loss: 0.059536, Validation Loss: 0.056462\n",
      "Epoch: 6878, Training Loss: 0.062481, Validation Loss: 0.061276\n",
      "Epoch: 6879, Training Loss: 0.065652, Validation Loss: 0.056070\n",
      "Epoch: 6880, Training Loss: 0.063464, Validation Loss: 0.061625\n",
      "Epoch: 6881, Training Loss: 0.060544, Validation Loss: 0.055055\n",
      "Epoch: 6882, Training Loss: 0.058352, Validation Loss: 0.056067\n",
      "Epoch: 6883, Training Loss: 0.061767, Validation Loss: 0.060726\n",
      "Epoch: 6884, Training Loss: 0.061999, Validation Loss: 0.055618\n",
      "Epoch: 6885, Training Loss: 0.059337, Validation Loss: 0.055398\n",
      "Epoch: 6886, Training Loss: 0.062493, Validation Loss: 0.059840\n",
      "Epoch: 6887, Training Loss: 0.065267, Validation Loss: 0.054610\n",
      "Epoch: 6888, Training Loss: 0.059749, Validation Loss: 0.053395\n",
      "Epoch: 6889, Training Loss: 0.063338, Validation Loss: 0.060833\n",
      "Epoch: 6890, Training Loss: 0.068046, Validation Loss: 0.057787\n",
      "Epoch: 6891, Training Loss: 0.060312, Validation Loss: 0.053875\n",
      "Epoch: 6892, Training Loss: 0.066851, Validation Loss: 0.063540\n",
      "Epoch: 6893, Training Loss: 0.065875, Validation Loss: 0.056841\n",
      "Epoch: 6894, Training Loss: 0.062548, Validation Loss: 0.052300\n",
      "Epoch: 6895, Training Loss: 0.065547, Validation Loss: 0.062889\n",
      "Epoch: 6896, Training Loss: 0.066463, Validation Loss: 0.062322\n",
      "Epoch: 6897, Training Loss: 0.072953, Validation Loss: 0.059361\n",
      "Epoch: 6898, Training Loss: 0.067029, Validation Loss: 0.059527\n",
      "Epoch: 6899, Training Loss: 0.068327, Validation Loss: 0.063102\n",
      "Epoch: 6900, Training Loss: 0.065668, Validation Loss: 0.056700\n",
      "Epoch: 6901, Training Loss: 0.065695, Validation Loss: 0.063449\n",
      "Epoch: 6902, Training Loss: 0.062418, Validation Loss: 0.057873\n",
      "Epoch: 6903, Training Loss: 0.064794, Validation Loss: 0.059637\n",
      "Epoch: 6904, Training Loss: 0.065007, Validation Loss: 0.062693\n",
      "Epoch: 6905, Training Loss: 0.060621, Validation Loss: 0.056505\n",
      "Epoch: 6906, Training Loss: 0.063517, Validation Loss: 0.052028\n",
      "Epoch: 6907, Training Loss: 0.061363, Validation Loss: 0.058662\n",
      "Epoch: 6908, Training Loss: 0.061041, Validation Loss: 0.058097\n",
      "Epoch: 6909, Training Loss: 0.063848, Validation Loss: 0.055359\n",
      "Epoch: 6910, Training Loss: 0.061902, Validation Loss: 0.058294\n",
      "Epoch: 6911, Training Loss: 0.059966, Validation Loss: 0.055806\n",
      "Epoch: 6912, Training Loss: 0.061450, Validation Loss: 0.053979\n",
      "Epoch: 6913, Training Loss: 0.060740, Validation Loss: 0.055268\n",
      "Epoch: 6914, Training Loss: 0.059782, Validation Loss: 0.055049\n",
      "Epoch: 6915, Training Loss: 0.060300, Validation Loss: 0.055628\n",
      "Epoch: 6916, Training Loss: 0.059847, Validation Loss: 0.055578\n",
      "Epoch: 6917, Training Loss: 0.060893, Validation Loss: 0.057372\n",
      "Epoch: 6918, Training Loss: 0.059181, Validation Loss: 0.053507\n",
      "Epoch: 6919, Training Loss: 0.058443, Validation Loss: 0.055994\n",
      "Epoch: 6920, Training Loss: 0.057887, Validation Loss: 0.056877\n",
      "Epoch: 6921, Training Loss: 0.060795, Validation Loss: 0.057734\n",
      "Epoch: 6922, Training Loss: 0.060358, Validation Loss: 0.059848\n",
      "Epoch: 6923, Training Loss: 0.061102, Validation Loss: 0.060041\n",
      "Epoch: 6924, Training Loss: 0.060207, Validation Loss: 0.061599\n",
      "Epoch: 6925, Training Loss: 0.059804, Validation Loss: 0.057733\n",
      "Epoch: 6926, Training Loss: 0.060233, Validation Loss: 0.062872\n",
      "Epoch: 6927, Training Loss: 0.061668, Validation Loss: 0.058581\n",
      "Epoch: 6928, Training Loss: 0.061628, Validation Loss: 0.062406\n",
      "Epoch: 6929, Training Loss: 0.061513, Validation Loss: 0.059307\n",
      "Epoch: 6930, Training Loss: 0.057957, Validation Loss: 0.056405\n",
      "Epoch: 6931, Training Loss: 0.057944, Validation Loss: 0.057221\n",
      "Epoch: 6932, Training Loss: 0.061230, Validation Loss: 0.056858\n",
      "Epoch: 6933, Training Loss: 0.061863, Validation Loss: 0.057435\n",
      "Epoch: 6934, Training Loss: 0.058792, Validation Loss: 0.054339\n",
      "Epoch: 6935, Training Loss: 0.057403, Validation Loss: 0.055856\n",
      "Epoch: 6936, Training Loss: 0.057562, Validation Loss: 0.055801\n",
      "Epoch: 6937, Training Loss: 0.057132, Validation Loss: 0.054683\n",
      "Epoch: 6938, Training Loss: 0.058111, Validation Loss: 0.056863\n",
      "Epoch: 6939, Training Loss: 0.059666, Validation Loss: 0.057457\n",
      "Epoch: 6940, Training Loss: 0.058441, Validation Loss: 0.058397\n",
      "Epoch: 6941, Training Loss: 0.058307, Validation Loss: 0.055248\n",
      "Epoch: 6942, Training Loss: 0.058720, Validation Loss: 0.059579\n",
      "Epoch: 6943, Training Loss: 0.058145, Validation Loss: 0.056755\n",
      "Epoch: 6944, Training Loss: 0.058840, Validation Loss: 0.059681\n",
      "Epoch: 6945, Training Loss: 0.059115, Validation Loss: 0.057430\n",
      "Epoch: 6946, Training Loss: 0.061281, Validation Loss: 0.060295\n",
      "Epoch: 6947, Training Loss: 0.064885, Validation Loss: 0.057687\n",
      "Epoch: 6948, Training Loss: 0.058832, Validation Loss: 0.059657\n",
      "Epoch: 6949, Training Loss: 0.057026, Validation Loss: 0.056915\n",
      "Epoch: 6950, Training Loss: 0.057616, Validation Loss: 0.054599\n",
      "Epoch: 6951, Training Loss: 0.058667, Validation Loss: 0.059961\n",
      "Epoch: 6952, Training Loss: 0.061192, Validation Loss: 0.056547\n",
      "Epoch: 6953, Training Loss: 0.060247, Validation Loss: 0.059346\n",
      "Epoch: 6954, Training Loss: 0.057911, Validation Loss: 0.055556\n",
      "Epoch: 6955, Training Loss: 0.058020, Validation Loss: 0.054906\n",
      "Epoch: 6956, Training Loss: 0.058534, Validation Loss: 0.058774\n",
      "Epoch: 6957, Training Loss: 0.060162, Validation Loss: 0.056444\n",
      "Epoch: 6958, Training Loss: 0.058314, Validation Loss: 0.058636\n",
      "Epoch: 6959, Training Loss: 0.056634, Validation Loss: 0.055040\n",
      "Epoch: 6960, Training Loss: 0.056256, Validation Loss: 0.056146\n",
      "Epoch: 6961, Training Loss: 0.058849, Validation Loss: 0.059633\n",
      "Epoch: 6962, Training Loss: 0.063659, Validation Loss: 0.065636\n",
      "Epoch: 6963, Training Loss: 0.062076, Validation Loss: 0.062308\n",
      "Epoch: 6964, Training Loss: 0.056256, Validation Loss: 0.058318\n",
      "Epoch: 6965, Training Loss: 0.059213, Validation Loss: 0.060878\n",
      "Epoch: 6966, Training Loss: 0.063271, Validation Loss: 0.063883\n",
      "Epoch: 6967, Training Loss: 0.056496, Validation Loss: 0.057508\n",
      "Epoch: 6968, Training Loss: 0.062357, Validation Loss: 0.065437\n",
      "Epoch: 6969, Training Loss: 0.070209, Validation Loss: 0.060773\n",
      "Epoch: 6970, Training Loss: 0.074437, Validation Loss: 0.071153\n",
      "Epoch: 6971, Training Loss: 0.090188, Validation Loss: 0.078213\n",
      "Epoch: 6972, Training Loss: 0.083593, Validation Loss: 0.068939\n",
      "Epoch: 6973, Training Loss: 0.086183, Validation Loss: 0.078382\n",
      "Epoch: 6974, Training Loss: 0.082169, Validation Loss: 0.074611\n",
      "Epoch: 6975, Training Loss: 0.076824, Validation Loss: 0.063411\n",
      "Epoch: 6976, Training Loss: 0.073568, Validation Loss: 0.057239\n",
      "Epoch: 6977, Training Loss: 0.077053, Validation Loss: 0.055587\n",
      "Epoch: 6978, Training Loss: 0.078317, Validation Loss: 0.058700\n",
      "Epoch: 6979, Training Loss: 0.067043, Validation Loss: 0.053028\n",
      "Epoch: 6980, Training Loss: 0.076298, Validation Loss: 0.060239\n",
      "Epoch: 6981, Training Loss: 0.077963, Validation Loss: 0.064081\n",
      "Epoch: 6982, Training Loss: 0.072211, Validation Loss: 0.064787\n",
      "Epoch: 6983, Training Loss: 0.066948, Validation Loss: 0.058680\n",
      "Epoch: 6984, Training Loss: 0.069328, Validation Loss: 0.057292\n",
      "Epoch: 6985, Training Loss: 0.065397, Validation Loss: 0.057271\n",
      "Epoch: 6986, Training Loss: 0.070032, Validation Loss: 0.065727\n",
      "Epoch: 6987, Training Loss: 0.067200, Validation Loss: 0.061744\n",
      "Epoch: 6988, Training Loss: 0.066694, Validation Loss: 0.053594\n",
      "Epoch: 6989, Training Loss: 0.068852, Validation Loss: 0.058758\n",
      "Epoch: 6990, Training Loss: 0.063369, Validation Loss: 0.058350\n",
      "Epoch: 6991, Training Loss: 0.065745, Validation Loss: 0.062807\n",
      "Epoch: 6992, Training Loss: 0.065117, Validation Loss: 0.063918\n",
      "Epoch: 6993, Training Loss: 0.067086, Validation Loss: 0.063111\n",
      "Epoch: 6994, Training Loss: 0.064154, Validation Loss: 0.061158\n",
      "Epoch: 6995, Training Loss: 0.066485, Validation Loss: 0.061807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6996, Training Loss: 0.065088, Validation Loss: 0.056671\n",
      "Epoch: 6997, Training Loss: 0.065815, Validation Loss: 0.060166\n",
      "Epoch: 6998, Training Loss: 0.063994, Validation Loss: 0.057692\n",
      "Epoch: 6999, Training Loss: 0.063774, Validation Loss: 0.058612\n",
      "Epoch: 7000, Training Loss: 0.063275, Validation Loss: 0.060656\n",
      "Epoch: 7001, Training Loss: 0.065871, Validation Loss: 0.056304\n",
      "Epoch: 7002, Training Loss: 0.074410, Validation Loss: 0.069863\n",
      "Epoch: 7003, Training Loss: 0.091700, Validation Loss: 0.057876\n",
      "Epoch: 7004, Training Loss: 0.084176, Validation Loss: 0.063547\n",
      "Epoch: 7005, Training Loss: 0.086261, Validation Loss: 0.083263\n",
      "Epoch: 7006, Training Loss: 0.071105, Validation Loss: 0.064475\n",
      "Epoch: 7007, Training Loss: 0.076174, Validation Loss: 0.067567\n",
      "Epoch: 7008, Training Loss: 0.079650, Validation Loss: 0.069873\n",
      "Epoch: 7009, Training Loss: 0.067513, Validation Loss: 0.066114\n",
      "Epoch: 7010, Training Loss: 0.070540, Validation Loss: 0.070701\n",
      "Epoch: 7011, Training Loss: 0.071402, Validation Loss: 0.057406\n",
      "Epoch: 7012, Training Loss: 0.071632, Validation Loss: 0.062010\n",
      "Epoch: 7013, Training Loss: 0.073790, Validation Loss: 0.062480\n",
      "Epoch: 7014, Training Loss: 0.065661, Validation Loss: 0.055077\n",
      "Epoch: 7015, Training Loss: 0.073919, Validation Loss: 0.069794\n",
      "Epoch: 7016, Training Loss: 0.067779, Validation Loss: 0.060794\n",
      "Epoch: 7017, Training Loss: 0.068919, Validation Loss: 0.055449\n",
      "Epoch: 7018, Training Loss: 0.072271, Validation Loss: 0.058287\n",
      "Epoch: 7019, Training Loss: 0.064496, Validation Loss: 0.053525\n",
      "Epoch: 7020, Training Loss: 0.072743, Validation Loss: 0.069397\n",
      "Epoch: 7021, Training Loss: 0.066041, Validation Loss: 0.060447\n",
      "Epoch: 7022, Training Loss: 0.065887, Validation Loss: 0.056463\n",
      "Epoch: 7023, Training Loss: 0.070276, Validation Loss: 0.059259\n",
      "Epoch: 7024, Training Loss: 0.065111, Validation Loss: 0.059972\n",
      "Epoch: 7025, Training Loss: 0.067383, Validation Loss: 0.063872\n",
      "Epoch: 7026, Training Loss: 0.061874, Validation Loss: 0.056121\n",
      "Epoch: 7027, Training Loss: 0.068624, Validation Loss: 0.059869\n",
      "Epoch: 7028, Training Loss: 0.063403, Validation Loss: 0.055953\n",
      "Epoch: 7029, Training Loss: 0.064288, Validation Loss: 0.061448\n",
      "Epoch: 7030, Training Loss: 0.063307, Validation Loss: 0.059695\n",
      "Epoch: 7031, Training Loss: 0.063536, Validation Loss: 0.056920\n",
      "Epoch: 7032, Training Loss: 0.063264, Validation Loss: 0.057048\n",
      "Epoch: 7033, Training Loss: 0.062000, Validation Loss: 0.056864\n",
      "Epoch: 7034, Training Loss: 0.062728, Validation Loss: 0.060864\n",
      "Epoch: 7035, Training Loss: 0.061498, Validation Loss: 0.056251\n",
      "Epoch: 7036, Training Loss: 0.063188, Validation Loss: 0.055129\n",
      "Epoch: 7037, Training Loss: 0.061270, Validation Loss: 0.057781\n",
      "Epoch: 7038, Training Loss: 0.062994, Validation Loss: 0.059982\n",
      "Epoch: 7039, Training Loss: 0.060791, Validation Loss: 0.057056\n",
      "Epoch: 7040, Training Loss: 0.061953, Validation Loss: 0.056364\n",
      "Epoch: 7041, Training Loss: 0.062291, Validation Loss: 0.054547\n",
      "Epoch: 7042, Training Loss: 0.064148, Validation Loss: 0.062581\n",
      "Epoch: 7043, Training Loss: 0.061738, Validation Loss: 0.053325\n",
      "Epoch: 7044, Training Loss: 0.059943, Validation Loss: 0.055245\n",
      "Epoch: 7045, Training Loss: 0.060975, Validation Loss: 0.057224\n",
      "Epoch: 7046, Training Loss: 0.062356, Validation Loss: 0.055986\n",
      "Epoch: 7047, Training Loss: 0.062328, Validation Loss: 0.059208\n",
      "Epoch: 7048, Training Loss: 0.060622, Validation Loss: 0.054388\n",
      "Epoch: 7049, Training Loss: 0.060671, Validation Loss: 0.058296\n",
      "Epoch: 7050, Training Loss: 0.059672, Validation Loss: 0.054133\n",
      "Epoch: 7051, Training Loss: 0.059641, Validation Loss: 0.056238\n",
      "Epoch: 7052, Training Loss: 0.059768, Validation Loss: 0.055004\n",
      "Epoch: 7053, Training Loss: 0.059292, Validation Loss: 0.055100\n",
      "Epoch: 7054, Training Loss: 0.059183, Validation Loss: 0.055461\n",
      "Epoch: 7055, Training Loss: 0.058957, Validation Loss: 0.054202\n",
      "Epoch: 7056, Training Loss: 0.059278, Validation Loss: 0.056066\n",
      "Epoch: 7057, Training Loss: 0.059798, Validation Loss: 0.053696\n",
      "Epoch: 7058, Training Loss: 0.061178, Validation Loss: 0.059655\n",
      "Epoch: 7059, Training Loss: 0.061568, Validation Loss: 0.055484\n",
      "Epoch: 7060, Training Loss: 0.064897, Validation Loss: 0.061216\n",
      "Epoch: 7061, Training Loss: 0.067983, Validation Loss: 0.056025\n",
      "Epoch: 7062, Training Loss: 0.070322, Validation Loss: 0.067777\n",
      "Epoch: 7063, Training Loss: 0.062803, Validation Loss: 0.054130\n",
      "Epoch: 7064, Training Loss: 0.058895, Validation Loss: 0.055328\n",
      "Epoch: 7065, Training Loss: 0.059844, Validation Loss: 0.056623\n",
      "Epoch: 7066, Training Loss: 0.062697, Validation Loss: 0.052382\n",
      "Epoch: 7067, Training Loss: 0.063026, Validation Loss: 0.057666\n",
      "Epoch: 7068, Training Loss: 0.059574, Validation Loss: 0.052060\n",
      "Epoch: 7069, Training Loss: 0.057729, Validation Loss: 0.052661\n",
      "Epoch: 7070, Training Loss: 0.059527, Validation Loss: 0.055000\n",
      "Epoch: 7071, Training Loss: 0.062215, Validation Loss: 0.051897\n",
      "Epoch: 7072, Training Loss: 0.061903, Validation Loss: 0.057870\n",
      "Epoch: 7073, Training Loss: 0.060274, Validation Loss: 0.052893\n",
      "Epoch: 7074, Training Loss: 0.058338, Validation Loss: 0.054394\n",
      "Epoch: 7075, Training Loss: 0.058034, Validation Loss: 0.053746\n",
      "Epoch: 7076, Training Loss: 0.058087, Validation Loss: 0.053010\n",
      "Epoch: 7077, Training Loss: 0.057377, Validation Loss: 0.053755\n",
      "Epoch: 7078, Training Loss: 0.059136, Validation Loss: 0.053897\n",
      "Epoch: 7079, Training Loss: 0.056755, Validation Loss: 0.053716\n",
      "Epoch: 7080, Training Loss: 0.057921, Validation Loss: 0.054555\n",
      "Epoch: 7081, Training Loss: 0.058405, Validation Loss: 0.054284\n",
      "Epoch: 7082, Training Loss: 0.059428, Validation Loss: 0.057335\n",
      "Epoch: 7083, Training Loss: 0.064681, Validation Loss: 0.054646\n",
      "Epoch: 7084, Training Loss: 0.073003, Validation Loss: 0.068522\n",
      "Epoch: 7085, Training Loss: 0.070218, Validation Loss: 0.056870\n",
      "Epoch: 7086, Training Loss: 0.059935, Validation Loss: 0.056432\n",
      "Epoch: 7087, Training Loss: 0.058488, Validation Loss: 0.054288\n",
      "Epoch: 7088, Training Loss: 0.063969, Validation Loss: 0.052312\n",
      "Epoch: 7089, Training Loss: 0.061353, Validation Loss: 0.056187\n",
      "Epoch: 7090, Training Loss: 0.059518, Validation Loss: 0.051511\n",
      "Epoch: 7091, Training Loss: 0.061389, Validation Loss: 0.050773\n",
      "Epoch: 7092, Training Loss: 0.070466, Validation Loss: 0.065144\n",
      "Epoch: 7093, Training Loss: 0.062269, Validation Loss: 0.052273\n",
      "Epoch: 7094, Training Loss: 0.060827, Validation Loss: 0.050545\n",
      "Epoch: 7095, Training Loss: 0.072151, Validation Loss: 0.070240\n",
      "Epoch: 7096, Training Loss: 0.059937, Validation Loss: 0.049299\n",
      "Epoch: 7097, Training Loss: 0.060893, Validation Loss: 0.052391\n",
      "Epoch: 7098, Training Loss: 0.056691, Validation Loss: 0.052142\n",
      "Epoch: 7099, Training Loss: 0.057854, Validation Loss: 0.050496\n",
      "Epoch: 7100, Training Loss: 0.058017, Validation Loss: 0.050706\n",
      "Epoch: 7101, Training Loss: 0.054470, Validation Loss: 0.049432\n",
      "Epoch: 7102, Training Loss: 0.056394, Validation Loss: 0.050752\n",
      "Epoch: 7103, Training Loss: 0.054464, Validation Loss: 0.050647\n",
      "Epoch: 7104, Training Loss: 0.054072, Validation Loss: 0.050348\n",
      "Epoch: 7105, Training Loss: 0.055830, Validation Loss: 0.053980\n",
      "Epoch: 7106, Training Loss: 0.053528, Validation Loss: 0.051291\n",
      "Epoch: 7107, Training Loss: 0.052974, Validation Loss: 0.050726\n",
      "Epoch: 7108, Training Loss: 0.054524, Validation Loss: 0.051509\n",
      "Epoch: 7109, Training Loss: 0.052681, Validation Loss: 0.049167\n",
      "Epoch: 7110, Training Loss: 0.053001, Validation Loss: 0.049919\n",
      "Epoch: 7111, Training Loss: 0.055221, Validation Loss: 0.048332\n",
      "Epoch: 7112, Training Loss: 0.062059, Validation Loss: 0.056369\n",
      "Epoch: 7113, Training Loss: 0.066937, Validation Loss: 0.053996\n",
      "Epoch: 7114, Training Loss: 0.067687, Validation Loss: 0.062715\n",
      "Epoch: 7115, Training Loss: 0.053851, Validation Loss: 0.048212\n",
      "Epoch: 7116, Training Loss: 0.054956, Validation Loss: 0.047001\n",
      "Epoch: 7117, Training Loss: 0.065209, Validation Loss: 0.059568\n",
      "Epoch: 7118, Training Loss: 0.058051, Validation Loss: 0.049565\n",
      "Epoch: 7119, Training Loss: 0.053502, Validation Loss: 0.044182\n",
      "Epoch: 7120, Training Loss: 0.069630, Validation Loss: 0.065667\n",
      "Epoch: 7121, Training Loss: 0.068115, Validation Loss: 0.055397\n",
      "Epoch: 7122, Training Loss: 0.054896, Validation Loss: 0.047040\n",
      "Epoch: 7123, Training Loss: 0.070187, Validation Loss: 0.069026\n",
      "Epoch: 7124, Training Loss: 0.057440, Validation Loss: 0.047914\n",
      "Epoch: 7125, Training Loss: 0.084039, Validation Loss: 0.063241\n",
      "Epoch: 7126, Training Loss: 0.052162, Validation Loss: 0.046935\n",
      "Epoch: 7127, Training Loss: 0.071803, Validation Loss: 0.071624\n",
      "Epoch: 7128, Training Loss: 0.055197, Validation Loss: 0.051877\n",
      "Epoch: 7129, Training Loss: 0.082824, Validation Loss: 0.066156\n",
      "Epoch: 7130, Training Loss: 0.052251, Validation Loss: 0.047920\n",
      "Epoch: 7131, Training Loss: 0.081148, Validation Loss: 0.086724\n",
      "Epoch: 7132, Training Loss: 0.085908, Validation Loss: 0.082254\n",
      "Epoch: 7133, Training Loss: 0.078329, Validation Loss: 0.078122\n",
      "Epoch: 7134, Training Loss: 0.055954, Validation Loss: 0.051795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7135, Training Loss: 0.098522, Validation Loss: 0.084477\n",
      "Epoch: 7136, Training Loss: 0.057911, Validation Loss: 0.049074\n",
      "Epoch: 7137, Training Loss: 0.073499, Validation Loss: 0.076065\n",
      "Epoch: 7138, Training Loss: 0.062148, Validation Loss: 0.060821\n",
      "Epoch: 7139, Training Loss: 0.077206, Validation Loss: 0.063783\n",
      "Epoch: 7140, Training Loss: 0.065650, Validation Loss: 0.058964\n",
      "Epoch: 7141, Training Loss: 0.070486, Validation Loss: 0.070501\n",
      "Epoch: 7142, Training Loss: 0.073020, Validation Loss: 0.073745\n",
      "Epoch: 7143, Training Loss: 0.055609, Validation Loss: 0.048271\n",
      "Epoch: 7144, Training Loss: 0.067694, Validation Loss: 0.057957\n",
      "Epoch: 7145, Training Loss: 0.055010, Validation Loss: 0.047899\n",
      "Epoch: 7146, Training Loss: 0.060828, Validation Loss: 0.052315\n",
      "Epoch: 7147, Training Loss: 0.052719, Validation Loss: 0.046492\n",
      "Epoch: 7148, Training Loss: 0.055674, Validation Loss: 0.048407\n",
      "Epoch: 7149, Training Loss: 0.056840, Validation Loss: 0.054219\n",
      "Epoch: 7150, Training Loss: 0.052716, Validation Loss: 0.048751\n",
      "Epoch: 7151, Training Loss: 0.055512, Validation Loss: 0.049051\n",
      "Epoch: 7152, Training Loss: 0.052072, Validation Loss: 0.047193\n",
      "Epoch: 7153, Training Loss: 0.051677, Validation Loss: 0.044725\n",
      "Epoch: 7154, Training Loss: 0.052442, Validation Loss: 0.046339\n",
      "Epoch: 7155, Training Loss: 0.051039, Validation Loss: 0.045474\n",
      "Epoch: 7156, Training Loss: 0.051626, Validation Loss: 0.046692\n",
      "Epoch: 7157, Training Loss: 0.050018, Validation Loss: 0.045357\n",
      "Epoch: 7158, Training Loss: 0.049486, Validation Loss: 0.045193\n"
     ]
    }
   ],
   "source": [
    "gratingCouplerNet = Network()\n",
    "optimizer = torch.optim.Adam(gratingCouplerNet.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "epoch = 0\n",
    "loss = 100\n",
    "validation_loss = 100\n",
    "\n",
    "while loss > 0.05:\n",
    "    prediction = gratingCouplerNet(X_normed)\n",
    "    loss = loss_function(prediction, y.float())\n",
    "    test_prediction = gratingCouplerNet(X_test_normed)\n",
    "    validation_loss = loss_function(test_prediction, y_test.float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch += 1\n",
    "    print(\"Epoch: {}, Training Loss: {:0.6f}, Validation Loss: {:0.6f}\".format(epoch, loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6576, -6.3010, -1.2430])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gratingCouplerNet(X_test_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0454, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(prediction, y_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8343, -2.2519], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8125, -1.7109])"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01945726046799541"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, y_test[0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005269782584722879"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, gratingCouplerNet(X_test_normed[0]).detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005151365999223099"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, y[0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008357428100215211"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, gratingCouplerNet(X_normed[0]).detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = transform_features(Dataset.get_features(new_dataset))\n",
    "X_new_normed = norm(X_new)\n",
    "y_new = transform_labels(Dataset.get_labels(new_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction = gratingCouplerNet(X_new_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.7959, -0.1303])"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.8410, -8.5823], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9358, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(new_prediction, y_new.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5d7e701477744fca7f01e4ac4ce7a00f852190f16ed8dbc24620b3de422c735"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
