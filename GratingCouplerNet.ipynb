{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"features.csv\"\n",
    "labels_path = \"labels.csv\"\n",
    "dataset_path = \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.input = nn.Linear(3, 100)\n",
    "        self.first_hidden = nn.Linear(100, 100)\n",
    "        self.second_hidden = nn.Linear(100, 100)\n",
    "        self.third_hidden = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x:[])->[]:\n",
    "        # 3 -> 2\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.first_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.second_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 5 -> 5\n",
    "        x = self.third_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        # 2 -> 2\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    \n",
    "    def get_dataset(path_to_dataset:str)->pandas.DataFrame:\n",
    "        return pandas.read_csv(path_to_dataset, header=None)\n",
    "    \n",
    "    \n",
    "    def get_dataset_no_zeroes(dataframe:pandas.DataFrame)->pandas.DataFrame:\n",
    "        return dataframe[dataframe[0] != 0]\n",
    "    \n",
    "    \n",
    "    def get_features(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.Tensor(dataframe.iloc[:,[0,1,2]].values)\n",
    "    \n",
    "    \n",
    "    def get_labels(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.Tensor(dataframe.iloc[:,[3,4]].values)\n",
    "    \n",
    "    \n",
    "    def get_features_no_zeroes(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.tensor(dataframe[dataframe[0] != 0].iloc[:,[0,1,2]].values)\n",
    "    \n",
    "    \n",
    "    def get_labels_no_zeroes(dataframe:pandas.DataFrame)->torch.Tensor:\n",
    "        return torch.tensor(dataframe[dataframe[0] != 0].iloc[:,[3, 4]].values)\n",
    "    \n",
    "\n",
    "def transform_features(tensor:torch.tensor)->torch.tensor:\n",
    "    return np.log10(tensor)\n",
    "\n",
    "\n",
    "def transform_labels(tensor:torch.tensor)->torch.tensor:\n",
    "    tensor[:,0] = np.log10(tensor[:,0])\n",
    "    tensor[:,1] = np.log10(-1*tensor[:,1])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def norm(tensor:torch.tensor)->torch.tensor:\n",
    "    return torch.nn.functional.normalize(tensor).float()\n",
    "    \n",
    "    \n",
    "def magnitude(vector:np.array)->float:\n",
    "    return np.linalg.norm(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.019457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>8.000000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.890000e-07</td>\n",
       "      <td>8.000000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.016767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.010203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.006660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "0    2.200000e-07  5.000000e-07  5.714324e-02  0.000002 -0.019457\n",
       "1    3.140000e-08  8.000000e-07  7.142883e-02  0.000002 -0.000399\n",
       "2    1.890000e-07  8.000000e-07  9.000000e-07  0.000002 -0.003692\n",
       "3    3.140000e-08  6.710000e-07  7.142883e-02  0.000002 -0.016767\n",
       "4    6.290000e-08  7.140000e-07  2.857207e-02  0.000002 -0.008122\n",
       "..            ...           ...           ...       ...       ...\n",
       "507  1.570000e-07  6.290000e-07  2.857207e-02  0.000002 -0.010203\n",
       "508  1.570000e-07  5.430000e-07  2.857207e-02  0.000002 -0.009285\n",
       "509  1.260000e-07  5.000000e-07  5.714324e-02  0.000002 -0.005189\n",
       "510  3.140000e-08  7.570000e-07  2.857207e-02  0.000002 -0.000702\n",
       "511  1.570000e-07  6.710000e-07  1.428649e-02  0.000002 -0.006660\n",
       "\n",
       "[448 rows x 5 columns]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataset from PATH\n",
    "dataset = Dataset.get_dataset_no_zeroes(Dataset.get_dataset(dataset_path))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1.890000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.004594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.002086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.035842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>9.430000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>4.285766e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.006933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.010930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.018188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "211  1.890000e-07  5.860000e-07  1.000000e-01  0.000002 -0.004594\n",
       "490  6.290000e-08  6.710000e-07  9.000000e-07  0.000002 -0.002086\n",
       "24   6.290000e-08  7.140000e-07  8.571441e-02  0.000002 -0.035842\n",
       "255  1.570000e-07  5.860000e-07  7.142883e-02  0.000002 -0.003743\n",
       "160  9.430000e-08  6.290000e-07  4.285766e-02  0.000002 -0.006933\n",
       "..            ...           ...           ...       ...       ...\n",
       "21   1.570000e-07  5.860000e-07  1.428649e-02  0.000002 -0.010930\n",
       "18   6.290000e-08  7.140000e-07  1.428649e-02  0.000002 -0.002851\n",
       "317  2.200000e-07  7.570000e-07  5.714324e-02  0.000002 -0.009943\n",
       "325  1.570000e-07  5.430000e-07  8.571441e-02  0.000002 -0.005662\n",
       "503  1.260000e-07  7.140000e-07  1.000000e-01  0.000002 -0.018188\n",
       "\n",
       "[381 rows x 5 columns]"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET THE TRAINING SET\n",
    "training_set = dataset.sample(frac = 0.85)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transform_features(Dataset.get_features(training_set))\n",
    "X_normed = norm(X)\n",
    "y = transform_labels(Dataset.get_labels(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.019457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>7.142883e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.016767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>5.430000e-07</td>\n",
       "      <td>5.714324e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>1.428649e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.009594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.260000e-07</td>\n",
       "      <td>6.710000e-07</td>\n",
       "      <td>4.285766e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>3.140000e-08</td>\n",
       "      <td>7.570000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>9.430000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>2.857207e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>6.290000e-08</td>\n",
       "      <td>6.290000e-07</td>\n",
       "      <td>8.571441e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.053611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1.570000e-07</td>\n",
       "      <td>7.140000e-07</td>\n",
       "      <td>9.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.003704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3         4\n",
       "0    2.200000e-07  5.000000e-07  5.714324e-02  0.000002 -0.019457\n",
       "3    3.140000e-08  6.710000e-07  7.142883e-02  0.000002 -0.016767\n",
       "4    6.290000e-08  7.140000e-07  2.857207e-02  0.000002 -0.008122\n",
       "15   2.200000e-07  5.430000e-07  5.714324e-02  0.000002 -0.005241\n",
       "17   1.260000e-07  7.140000e-07  1.428649e-02  0.000002 -0.009594\n",
       "..            ...           ...           ...       ...       ...\n",
       "456  1.260000e-07  6.710000e-07  4.285766e-02  0.000002 -0.008698\n",
       "468  3.140000e-08  7.570000e-07  8.571441e-02  0.000002 -0.000279\n",
       "475  9.430000e-08  6.290000e-07  2.857207e-02  0.000002 -0.005730\n",
       "486  6.290000e-08  6.290000e-07  8.571441e-02  0.000002 -0.053611\n",
       "505  1.570000e-07  7.140000e-07  9.000000e-07  0.000002 -0.003704\n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set = dataset.drop(training_set.index)\n",
    "testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_features(Dataset.get_features(testing_set))\n",
    "X_test_normed = norm(X_test)\n",
    "y_test = transform_labels(Dataset.get_labels(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 19.474678, Validation Loss: 19.724630\n",
      "Epoch: 2, Training Loss: 17.623346, Validation Loss: 17.851967\n",
      "Epoch: 3, Training Loss: 14.114474, Validation Loss: 14.295656\n",
      "Epoch: 4, Training Loss: 7.704883, Validation Loss: 7.766698\n",
      "Epoch: 5, Training Loss: 2.456374, Validation Loss: 2.258070\n",
      "Epoch: 6, Training Loss: 8.417977, Validation Loss: 8.053440\n",
      "Epoch: 7, Training Loss: 2.153508, Validation Loss: 1.963870\n",
      "Epoch: 8, Training Loss: 0.263619, Validation Loss: 0.246949\n",
      "Epoch: 9, Training Loss: 1.672676, Validation Loss: 1.763638\n",
      "Epoch: 10, Training Loss: 2.884992, Validation Loss: 3.034387\n",
      "Epoch: 11, Training Loss: 3.122385, Validation Loss: 3.301654\n",
      "Epoch: 12, Training Loss: 2.613750, Validation Loss: 2.806322\n",
      "Epoch: 13, Training Loss: 1.854544, Validation Loss: 2.046989\n",
      "Epoch: 14, Training Loss: 1.479508, Validation Loss: 1.657253\n",
      "Epoch: 15, Training Loss: 1.712294, Validation Loss: 1.859607\n",
      "Epoch: 16, Training Loss: 1.770002, Validation Loss: 1.876900\n",
      "Epoch: 17, Training Loss: 1.120040, Validation Loss: 1.189484\n",
      "Epoch: 18, Training Loss: 0.384870, Validation Loss: 0.427302\n",
      "Epoch: 19, Training Loss: 0.156008, Validation Loss: 0.180782\n",
      "Epoch: 20, Training Loss: 0.326757, Validation Loss: 0.337564\n",
      "Epoch: 21, Training Loss: 0.543267, Validation Loss: 0.538425\n",
      "Epoch: 22, Training Loss: 0.610742, Validation Loss: 0.585344\n",
      "Epoch: 23, Training Loss: 0.547045, Validation Loss: 0.495748\n",
      "Epoch: 24, Training Loss: 0.487475, Validation Loss: 0.406045\n",
      "Epoch: 25, Training Loss: 0.548919, Validation Loss: 0.441672\n",
      "Epoch: 26, Training Loss: 0.657320, Validation Loss: 0.535719\n",
      "Epoch: 27, Training Loss: 0.662528, Validation Loss: 0.541547\n",
      "Epoch: 28, Training Loss: 0.512667, Validation Loss: 0.407973\n",
      "Epoch: 29, Training Loss: 0.343529, Validation Loss: 0.264548\n",
      "Epoch: 30, Training Loss: 0.268121, Validation Loss: 0.216499\n",
      "Epoch: 31, Training Loss: 0.264093, Validation Loss: 0.235828\n",
      "Epoch: 32, Training Loss: 0.252101, Validation Loss: 0.240707\n",
      "Epoch: 33, Training Loss: 0.197321, Validation Loss: 0.196301\n",
      "Epoch: 34, Training Loss: 0.135312, Validation Loss: 0.139714\n",
      "Epoch: 35, Training Loss: 0.125996, Validation Loss: 0.133324\n",
      "Epoch: 36, Training Loss: 0.178565, Validation Loss: 0.189034\n",
      "Epoch: 37, Training Loss: 0.230503, Validation Loss: 0.246307\n",
      "Epoch: 38, Training Loss: 0.229227, Validation Loss: 0.252836\n",
      "Epoch: 39, Training Loss: 0.201001, Validation Loss: 0.233412\n",
      "Epoch: 40, Training Loss: 0.196757, Validation Loss: 0.236676\n",
      "Epoch: 41, Training Loss: 0.217809, Validation Loss: 0.261930\n",
      "Epoch: 42, Training Loss: 0.227505, Validation Loss: 0.271369\n",
      "Epoch: 43, Training Loss: 0.204498, Validation Loss: 0.243479\n",
      "Epoch: 44, Training Loss: 0.165843, Validation Loss: 0.196089\n",
      "Epoch: 45, Training Loss: 0.144013, Validation Loss: 0.163336\n",
      "Epoch: 46, Training Loss: 0.146767, Validation Loss: 0.155263\n",
      "Epoch: 47, Training Loss: 0.149587, Validation Loss: 0.149539\n",
      "Epoch: 48, Training Loss: 0.134911, Validation Loss: 0.129729\n",
      "Epoch: 49, Training Loss: 0.117788, Validation Loss: 0.110446\n",
      "Epoch: 50, Training Loss: 0.118245, Validation Loss: 0.110110\n",
      "Epoch: 51, Training Loss: 0.131718, Validation Loss: 0.122352\n",
      "Epoch: 52, Training Loss: 0.139880, Validation Loss: 0.127669\n",
      "Epoch: 53, Training Loss: 0.136253, Validation Loss: 0.119452\n",
      "Epoch: 54, Training Loss: 0.131208, Validation Loss: 0.108994\n",
      "Epoch: 55, Training Loss: 0.134507, Validation Loss: 0.107685\n",
      "Epoch: 56, Training Loss: 0.140752, Validation Loss: 0.111771\n",
      "Epoch: 57, Training Loss: 0.138780, Validation Loss: 0.110938\n",
      "Epoch: 58, Training Loss: 0.129689, Validation Loss: 0.105881\n",
      "Epoch: 59, Training Loss: 0.123777, Validation Loss: 0.105492\n",
      "Epoch: 60, Training Loss: 0.124071, Validation Loss: 0.111151\n",
      "Epoch: 61, Training Loss: 0.124144, Validation Loss: 0.115249\n",
      "Epoch: 62, Training Loss: 0.119893, Validation Loss: 0.113317\n",
      "Epoch: 63, Training Loss: 0.115413, Validation Loss: 0.109875\n",
      "Epoch: 64, Training Loss: 0.115828, Validation Loss: 0.110952\n",
      "Epoch: 65, Training Loss: 0.119212, Validation Loss: 0.115533\n",
      "Epoch: 66, Training Loss: 0.120170, Validation Loss: 0.118673\n",
      "Epoch: 67, Training Loss: 0.118538, Validation Loss: 0.119969\n",
      "Epoch: 68, Training Loss: 0.118388, Validation Loss: 0.122724\n",
      "Epoch: 69, Training Loss: 0.120465, Validation Loss: 0.126793\n",
      "Epoch: 70, Training Loss: 0.121448, Validation Loss: 0.128265\n",
      "Epoch: 71, Training Loss: 0.119786, Validation Loss: 0.125521\n",
      "Epoch: 72, Training Loss: 0.117831, Validation Loss: 0.121370\n",
      "Epoch: 73, Training Loss: 0.117644, Validation Loss: 0.118648\n",
      "Epoch: 74, Training Loss: 0.117899, Validation Loss: 0.116776\n",
      "Epoch: 75, Training Loss: 0.116773, Validation Loss: 0.114326\n",
      "Epoch: 76, Training Loss: 0.115248, Validation Loss: 0.112170\n",
      "Epoch: 77, Training Loss: 0.115108, Validation Loss: 0.111616\n",
      "Epoch: 78, Training Loss: 0.115883, Validation Loss: 0.111663\n",
      "Epoch: 79, Training Loss: 0.116017, Validation Loss: 0.110478\n",
      "Epoch: 80, Training Loss: 0.115542, Validation Loss: 0.108215\n",
      "Epoch: 81, Training Loss: 0.115632, Validation Loss: 0.106508\n",
      "Epoch: 82, Training Loss: 0.116323, Validation Loss: 0.105956\n",
      "Epoch: 83, Training Loss: 0.116525, Validation Loss: 0.105843\n",
      "Epoch: 84, Training Loss: 0.116026, Validation Loss: 0.105943\n",
      "Epoch: 85, Training Loss: 0.115697, Validation Loss: 0.106760\n",
      "Epoch: 86, Training Loss: 0.115812, Validation Loss: 0.108063\n",
      "Epoch: 87, Training Loss: 0.115740, Validation Loss: 0.108840\n",
      "Epoch: 88, Training Loss: 0.115261, Validation Loss: 0.108784\n",
      "Epoch: 89, Training Loss: 0.114941, Validation Loss: 0.108653\n",
      "Epoch: 90, Training Loss: 0.115051, Validation Loss: 0.109051\n",
      "Epoch: 91, Training Loss: 0.115159, Validation Loss: 0.109791\n",
      "Epoch: 92, Training Loss: 0.115014, Validation Loss: 0.110636\n",
      "Epoch: 93, Training Loss: 0.114945, Validation Loss: 0.111687\n",
      "Epoch: 94, Training Loss: 0.115133, Validation Loss: 0.112785\n",
      "Epoch: 95, Training Loss: 0.115273, Validation Loss: 0.113362\n",
      "Epoch: 96, Training Loss: 0.115179, Validation Loss: 0.113176\n",
      "Epoch: 97, Training Loss: 0.115081, Validation Loss: 0.112616\n",
      "Epoch: 98, Training Loss: 0.115125, Validation Loss: 0.112108\n",
      "Epoch: 99, Training Loss: 0.115128, Validation Loss: 0.111703\n",
      "Epoch: 100, Training Loss: 0.114986, Validation Loss: 0.111374\n",
      "Epoch: 101, Training Loss: 0.114877, Validation Loss: 0.111193\n",
      "Epoch: 102, Training Loss: 0.114896, Validation Loss: 0.111064\n",
      "Epoch: 103, Training Loss: 0.114908, Validation Loss: 0.110706\n",
      "Epoch: 104, Training Loss: 0.114848, Validation Loss: 0.110051\n",
      "Epoch: 105, Training Loss: 0.114828, Validation Loss: 0.109355\n",
      "Epoch: 106, Training Loss: 0.114888, Validation Loss: 0.108869\n",
      "Epoch: 107, Training Loss: 0.114920, Validation Loss: 0.108638\n",
      "Epoch: 108, Training Loss: 0.114889, Validation Loss: 0.108644\n",
      "Epoch: 109, Training Loss: 0.114877, Validation Loss: 0.108852\n",
      "Epoch: 110, Training Loss: 0.114901, Validation Loss: 0.109109\n",
      "Epoch: 111, Training Loss: 0.114890, Validation Loss: 0.109228\n",
      "Epoch: 112, Training Loss: 0.114842, Validation Loss: 0.109203\n",
      "Epoch: 113, Training Loss: 0.114822, Validation Loss: 0.109190\n",
      "Epoch: 114, Training Loss: 0.114829, Validation Loss: 0.109305\n",
      "Epoch: 115, Training Loss: 0.114816, Validation Loss: 0.109560\n",
      "Epoch: 116, Training Loss: 0.114793, Validation Loss: 0.109920\n",
      "Epoch: 117, Training Loss: 0.114798, Validation Loss: 0.110307\n",
      "Epoch: 118, Training Loss: 0.114815, Validation Loss: 0.110585\n",
      "Epoch: 119, Training Loss: 0.114811, Validation Loss: 0.110667\n",
      "Epoch: 120, Training Loss: 0.114803, Validation Loss: 0.110605\n",
      "Epoch: 121, Training Loss: 0.114809, Validation Loss: 0.110510\n",
      "Epoch: 122, Training Loss: 0.114813, Validation Loss: 0.110450\n",
      "Epoch: 123, Training Loss: 0.114800, Validation Loss: 0.110438\n",
      "Epoch: 124, Training Loss: 0.114789, Validation Loss: 0.110451\n",
      "Epoch: 125, Training Loss: 0.114789, Validation Loss: 0.110426\n",
      "Epoch: 126, Training Loss: 0.114784, Validation Loss: 0.110299\n",
      "Epoch: 127, Training Loss: 0.114774, Validation Loss: 0.110078\n",
      "Epoch: 128, Training Loss: 0.114771, Validation Loss: 0.109842\n",
      "Epoch: 129, Training Loss: 0.114775, Validation Loss: 0.109667\n",
      "Epoch: 130, Training Loss: 0.114773, Validation Loss: 0.109582\n",
      "Epoch: 131, Training Loss: 0.114769, Validation Loss: 0.109581\n",
      "Epoch: 132, Training Loss: 0.114770, Validation Loss: 0.109617\n",
      "Epoch: 133, Training Loss: 0.114771, Validation Loss: 0.109633\n",
      "Epoch: 134, Training Loss: 0.114766, Validation Loss: 0.109607\n",
      "Epoch: 135, Training Loss: 0.114762, Validation Loss: 0.109569\n",
      "Epoch: 136, Training Loss: 0.114761, Validation Loss: 0.109568\n",
      "Epoch: 137, Training Loss: 0.114758, Validation Loss: 0.109628\n",
      "Epoch: 138, Training Loss: 0.114753, Validation Loss: 0.109742\n",
      "Epoch: 139, Training Loss: 0.114751, Validation Loss: 0.109873\n",
      "Epoch: 140, Training Loss: 0.114750, Validation Loss: 0.109975\n",
      "Epoch: 141, Training Loss: 0.114748, Validation Loss: 0.110020\n",
      "Epoch: 142, Training Loss: 0.114745, Validation Loss: 0.110021\n",
      "Epoch: 143, Training Loss: 0.114745, Validation Loss: 0.110012\n",
      "Epoch: 144, Training Loss: 0.114744, Validation Loss: 0.110017\n",
      "Epoch: 145, Training Loss: 0.114741, Validation Loss: 0.110042\n",
      "Epoch: 146, Training Loss: 0.114739, Validation Loss: 0.110066\n",
      "Epoch: 147, Training Loss: 0.114737, Validation Loss: 0.110064\n",
      "Epoch: 148, Training Loss: 0.114735, Validation Loss: 0.110019\n",
      "Epoch: 149, Training Loss: 0.114732, Validation Loss: 0.109944\n",
      "Epoch: 150, Training Loss: 0.114730, Validation Loss: 0.109867\n",
      "Epoch: 151, Training Loss: 0.114728, Validation Loss: 0.109813\n",
      "Epoch: 152, Training Loss: 0.114726, Validation Loss: 0.109789\n",
      "Epoch: 153, Training Loss: 0.114724, Validation Loss: 0.109784\n",
      "Epoch: 154, Training Loss: 0.114723, Validation Loss: 0.109778\n",
      "Epoch: 155, Training Loss: 0.114721, Validation Loss: 0.109757\n",
      "Epoch: 156, Training Loss: 0.114719, Validation Loss: 0.109729\n",
      "Epoch: 157, Training Loss: 0.114717, Validation Loss: 0.109709\n",
      "Epoch: 158, Training Loss: 0.114715, Validation Loss: 0.109713\n",
      "Epoch: 159, Training Loss: 0.114713, Validation Loss: 0.109742\n",
      "Epoch: 160, Training Loss: 0.114711, Validation Loss: 0.109782\n",
      "Epoch: 161, Training Loss: 0.114709, Validation Loss: 0.109816\n",
      "Epoch: 162, Training Loss: 0.114707, Validation Loss: 0.109833\n",
      "Epoch: 163, Training Loss: 0.114705, Validation Loss: 0.109836\n",
      "Epoch: 164, Training Loss: 0.114703, Validation Loss: 0.109838\n",
      "Epoch: 165, Training Loss: 0.114701, Validation Loss: 0.109846\n",
      "Epoch: 166, Training Loss: 0.114699, Validation Loss: 0.109862\n",
      "Epoch: 167, Training Loss: 0.114697, Validation Loss: 0.109877\n",
      "Epoch: 168, Training Loss: 0.114695, Validation Loss: 0.109878\n",
      "Epoch: 169, Training Loss: 0.114693, Validation Loss: 0.109864\n",
      "Epoch: 170, Training Loss: 0.114691, Validation Loss: 0.109838\n",
      "Epoch: 171, Training Loss: 0.114689, Validation Loss: 0.109813\n",
      "Epoch: 172, Training Loss: 0.114687, Validation Loss: 0.109797\n",
      "Epoch: 173, Training Loss: 0.114685, Validation Loss: 0.109788\n",
      "Epoch: 174, Training Loss: 0.114683, Validation Loss: 0.109782\n",
      "Epoch: 175, Training Loss: 0.114681, Validation Loss: 0.109772\n",
      "Epoch: 176, Training Loss: 0.114679, Validation Loss: 0.109756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177, Training Loss: 0.114677, Validation Loss: 0.109740\n",
      "Epoch: 178, Training Loss: 0.114675, Validation Loss: 0.109730\n",
      "Epoch: 179, Training Loss: 0.114673, Validation Loss: 0.109731\n",
      "Epoch: 180, Training Loss: 0.114671, Validation Loss: 0.109740\n",
      "Epoch: 181, Training Loss: 0.114669, Validation Loss: 0.109749\n",
      "Epoch: 182, Training Loss: 0.114667, Validation Loss: 0.109754\n",
      "Epoch: 183, Training Loss: 0.114665, Validation Loss: 0.109754\n",
      "Epoch: 184, Training Loss: 0.114663, Validation Loss: 0.109754\n",
      "Epoch: 185, Training Loss: 0.114661, Validation Loss: 0.109756\n",
      "Epoch: 186, Training Loss: 0.114659, Validation Loss: 0.109762\n",
      "Epoch: 187, Training Loss: 0.114657, Validation Loss: 0.109768\n",
      "Epoch: 188, Training Loss: 0.114655, Validation Loss: 0.109770\n",
      "Epoch: 189, Training Loss: 0.114653, Validation Loss: 0.109766\n",
      "Epoch: 190, Training Loss: 0.114651, Validation Loss: 0.109757\n",
      "Epoch: 191, Training Loss: 0.114649, Validation Loss: 0.109748\n",
      "Epoch: 192, Training Loss: 0.114647, Validation Loss: 0.109742\n",
      "Epoch: 193, Training Loss: 0.114645, Validation Loss: 0.109737\n",
      "Epoch: 194, Training Loss: 0.114643, Validation Loss: 0.109733\n",
      "Epoch: 195, Training Loss: 0.114641, Validation Loss: 0.109726\n",
      "Epoch: 196, Training Loss: 0.114639, Validation Loss: 0.109717\n",
      "Epoch: 197, Training Loss: 0.114637, Validation Loss: 0.109708\n",
      "Epoch: 198, Training Loss: 0.114635, Validation Loss: 0.109702\n",
      "Epoch: 199, Training Loss: 0.114632, Validation Loss: 0.109699\n",
      "Epoch: 200, Training Loss: 0.114630, Validation Loss: 0.109699\n",
      "Epoch: 201, Training Loss: 0.114628, Validation Loss: 0.109699\n",
      "Epoch: 202, Training Loss: 0.114626, Validation Loss: 0.109697\n",
      "Epoch: 203, Training Loss: 0.114624, Validation Loss: 0.109695\n",
      "Epoch: 204, Training Loss: 0.114622, Validation Loss: 0.109693\n",
      "Epoch: 205, Training Loss: 0.114620, Validation Loss: 0.109693\n",
      "Epoch: 206, Training Loss: 0.114618, Validation Loss: 0.109694\n",
      "Epoch: 207, Training Loss: 0.114616, Validation Loss: 0.109695\n",
      "Epoch: 208, Training Loss: 0.114614, Validation Loss: 0.109694\n",
      "Epoch: 209, Training Loss: 0.114612, Validation Loss: 0.109690\n",
      "Epoch: 210, Training Loss: 0.114610, Validation Loss: 0.109685\n",
      "Epoch: 211, Training Loss: 0.114608, Validation Loss: 0.109681\n",
      "Epoch: 212, Training Loss: 0.114605, Validation Loss: 0.109678\n",
      "Epoch: 213, Training Loss: 0.114603, Validation Loss: 0.109675\n",
      "Epoch: 214, Training Loss: 0.114601, Validation Loss: 0.109671\n",
      "Epoch: 215, Training Loss: 0.114599, Validation Loss: 0.109666\n",
      "Epoch: 216, Training Loss: 0.114597, Validation Loss: 0.109660\n",
      "Epoch: 217, Training Loss: 0.114595, Validation Loss: 0.109655\n",
      "Epoch: 218, Training Loss: 0.114593, Validation Loss: 0.109652\n",
      "Epoch: 219, Training Loss: 0.114591, Validation Loss: 0.109649\n",
      "Epoch: 220, Training Loss: 0.114589, Validation Loss: 0.109646\n",
      "Epoch: 221, Training Loss: 0.114586, Validation Loss: 0.109643\n",
      "Epoch: 222, Training Loss: 0.114584, Validation Loss: 0.109640\n",
      "Epoch: 223, Training Loss: 0.114582, Validation Loss: 0.109637\n",
      "Epoch: 224, Training Loss: 0.114580, Validation Loss: 0.109635\n",
      "Epoch: 225, Training Loss: 0.114578, Validation Loss: 0.109634\n",
      "Epoch: 226, Training Loss: 0.114576, Validation Loss: 0.109632\n",
      "Epoch: 227, Training Loss: 0.114574, Validation Loss: 0.109630\n",
      "Epoch: 228, Training Loss: 0.114571, Validation Loss: 0.109627\n",
      "Epoch: 229, Training Loss: 0.114569, Validation Loss: 0.109624\n",
      "Epoch: 230, Training Loss: 0.114567, Validation Loss: 0.109621\n",
      "Epoch: 231, Training Loss: 0.114565, Validation Loss: 0.109618\n",
      "Epoch: 232, Training Loss: 0.114563, Validation Loss: 0.109615\n",
      "Epoch: 233, Training Loss: 0.114561, Validation Loss: 0.109612\n",
      "Epoch: 234, Training Loss: 0.114559, Validation Loss: 0.109608\n",
      "Epoch: 235, Training Loss: 0.114556, Validation Loss: 0.109604\n",
      "Epoch: 236, Training Loss: 0.114554, Validation Loss: 0.109600\n",
      "Epoch: 237, Training Loss: 0.114552, Validation Loss: 0.109597\n",
      "Epoch: 238, Training Loss: 0.114550, Validation Loss: 0.109594\n",
      "Epoch: 239, Training Loss: 0.114548, Validation Loss: 0.109590\n",
      "Epoch: 240, Training Loss: 0.114546, Validation Loss: 0.109587\n",
      "Epoch: 241, Training Loss: 0.114543, Validation Loss: 0.109584\n",
      "Epoch: 242, Training Loss: 0.114541, Validation Loss: 0.109581\n",
      "Epoch: 243, Training Loss: 0.114539, Validation Loss: 0.109578\n",
      "Epoch: 244, Training Loss: 0.114537, Validation Loss: 0.109576\n",
      "Epoch: 245, Training Loss: 0.114535, Validation Loss: 0.109573\n",
      "Epoch: 246, Training Loss: 0.114532, Validation Loss: 0.109570\n",
      "Epoch: 247, Training Loss: 0.114530, Validation Loss: 0.109567\n",
      "Epoch: 248, Training Loss: 0.114528, Validation Loss: 0.109564\n",
      "Epoch: 249, Training Loss: 0.114526, Validation Loss: 0.109561\n",
      "Epoch: 250, Training Loss: 0.114524, Validation Loss: 0.109559\n",
      "Epoch: 251, Training Loss: 0.114521, Validation Loss: 0.109556\n",
      "Epoch: 252, Training Loss: 0.114519, Validation Loss: 0.109552\n",
      "Epoch: 253, Training Loss: 0.114517, Validation Loss: 0.109549\n",
      "Epoch: 254, Training Loss: 0.114515, Validation Loss: 0.109545\n",
      "Epoch: 255, Training Loss: 0.114513, Validation Loss: 0.109542\n",
      "Epoch: 256, Training Loss: 0.114510, Validation Loss: 0.109539\n",
      "Epoch: 257, Training Loss: 0.114508, Validation Loss: 0.109536\n",
      "Epoch: 258, Training Loss: 0.114506, Validation Loss: 0.109533\n",
      "Epoch: 259, Training Loss: 0.114504, Validation Loss: 0.109529\n",
      "Epoch: 260, Training Loss: 0.114502, Validation Loss: 0.109526\n",
      "Epoch: 261, Training Loss: 0.114499, Validation Loss: 0.109523\n",
      "Epoch: 262, Training Loss: 0.114497, Validation Loss: 0.109520\n",
      "Epoch: 263, Training Loss: 0.114495, Validation Loss: 0.109517\n",
      "Epoch: 264, Training Loss: 0.114493, Validation Loss: 0.109514\n",
      "Epoch: 265, Training Loss: 0.114490, Validation Loss: 0.109511\n",
      "Epoch: 266, Training Loss: 0.114488, Validation Loss: 0.109508\n",
      "Epoch: 267, Training Loss: 0.114486, Validation Loss: 0.109505\n",
      "Epoch: 268, Training Loss: 0.114484, Validation Loss: 0.109502\n",
      "Epoch: 269, Training Loss: 0.114481, Validation Loss: 0.109499\n",
      "Epoch: 270, Training Loss: 0.114479, Validation Loss: 0.109496\n",
      "Epoch: 271, Training Loss: 0.114477, Validation Loss: 0.109493\n",
      "Epoch: 272, Training Loss: 0.114475, Validation Loss: 0.109490\n",
      "Epoch: 273, Training Loss: 0.114472, Validation Loss: 0.109487\n",
      "Epoch: 274, Training Loss: 0.114470, Validation Loss: 0.109484\n",
      "Epoch: 275, Training Loss: 0.114468, Validation Loss: 0.109481\n",
      "Epoch: 276, Training Loss: 0.114466, Validation Loss: 0.109477\n",
      "Epoch: 277, Training Loss: 0.114463, Validation Loss: 0.109474\n",
      "Epoch: 278, Training Loss: 0.114461, Validation Loss: 0.109471\n",
      "Epoch: 279, Training Loss: 0.114459, Validation Loss: 0.109468\n",
      "Epoch: 280, Training Loss: 0.114457, Validation Loss: 0.109465\n",
      "Epoch: 281, Training Loss: 0.114454, Validation Loss: 0.109462\n",
      "Epoch: 282, Training Loss: 0.114452, Validation Loss: 0.109459\n",
      "Epoch: 283, Training Loss: 0.114450, Validation Loss: 0.109456\n",
      "Epoch: 284, Training Loss: 0.114447, Validation Loss: 0.109453\n",
      "Epoch: 285, Training Loss: 0.114445, Validation Loss: 0.109450\n",
      "Epoch: 286, Training Loss: 0.114443, Validation Loss: 0.109447\n",
      "Epoch: 287, Training Loss: 0.114441, Validation Loss: 0.109444\n",
      "Epoch: 288, Training Loss: 0.114438, Validation Loss: 0.109440\n",
      "Epoch: 289, Training Loss: 0.114436, Validation Loss: 0.109437\n",
      "Epoch: 290, Training Loss: 0.114434, Validation Loss: 0.109434\n",
      "Epoch: 291, Training Loss: 0.114431, Validation Loss: 0.109431\n",
      "Epoch: 292, Training Loss: 0.114429, Validation Loss: 0.109428\n",
      "Epoch: 293, Training Loss: 0.114427, Validation Loss: 0.109425\n",
      "Epoch: 294, Training Loss: 0.114425, Validation Loss: 0.109422\n",
      "Epoch: 295, Training Loss: 0.114422, Validation Loss: 0.109419\n",
      "Epoch: 296, Training Loss: 0.114420, Validation Loss: 0.109416\n",
      "Epoch: 297, Training Loss: 0.114418, Validation Loss: 0.109413\n",
      "Epoch: 298, Training Loss: 0.114415, Validation Loss: 0.109410\n",
      "Epoch: 299, Training Loss: 0.114413, Validation Loss: 0.109407\n",
      "Epoch: 300, Training Loss: 0.114411, Validation Loss: 0.109403\n",
      "Epoch: 301, Training Loss: 0.114408, Validation Loss: 0.109400\n",
      "Epoch: 302, Training Loss: 0.114406, Validation Loss: 0.109397\n",
      "Epoch: 303, Training Loss: 0.114404, Validation Loss: 0.109394\n",
      "Epoch: 304, Training Loss: 0.114401, Validation Loss: 0.109391\n",
      "Epoch: 305, Training Loss: 0.114399, Validation Loss: 0.109388\n",
      "Epoch: 306, Training Loss: 0.114397, Validation Loss: 0.109385\n",
      "Epoch: 307, Training Loss: 0.114394, Validation Loss: 0.109382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308, Training Loss: 0.114392, Validation Loss: 0.109379\n",
      "Epoch: 309, Training Loss: 0.114390, Validation Loss: 0.109376\n",
      "Epoch: 310, Training Loss: 0.114387, Validation Loss: 0.109373\n",
      "Epoch: 311, Training Loss: 0.114385, Validation Loss: 0.109370\n",
      "Epoch: 312, Training Loss: 0.114383, Validation Loss: 0.109366\n",
      "Epoch: 313, Training Loss: 0.114380, Validation Loss: 0.109363\n",
      "Epoch: 314, Training Loss: 0.114378, Validation Loss: 0.109360\n",
      "Epoch: 315, Training Loss: 0.114376, Validation Loss: 0.109357\n",
      "Epoch: 316, Training Loss: 0.114373, Validation Loss: 0.109354\n",
      "Epoch: 317, Training Loss: 0.114371, Validation Loss: 0.109351\n",
      "Epoch: 318, Training Loss: 0.114369, Validation Loss: 0.109348\n",
      "Epoch: 319, Training Loss: 0.114366, Validation Loss: 0.109345\n",
      "Epoch: 320, Training Loss: 0.114364, Validation Loss: 0.109342\n",
      "Epoch: 321, Training Loss: 0.114362, Validation Loss: 0.109339\n",
      "Epoch: 322, Training Loss: 0.114359, Validation Loss: 0.109336\n",
      "Epoch: 323, Training Loss: 0.114357, Validation Loss: 0.109333\n",
      "Epoch: 324, Training Loss: 0.114354, Validation Loss: 0.109330\n",
      "Epoch: 325, Training Loss: 0.114352, Validation Loss: 0.109327\n",
      "Epoch: 326, Training Loss: 0.114350, Validation Loss: 0.109324\n",
      "Epoch: 327, Training Loss: 0.114347, Validation Loss: 0.109320\n",
      "Epoch: 328, Training Loss: 0.114345, Validation Loss: 0.109317\n",
      "Epoch: 329, Training Loss: 0.114343, Validation Loss: 0.109314\n",
      "Epoch: 330, Training Loss: 0.114340, Validation Loss: 0.109311\n",
      "Epoch: 331, Training Loss: 0.114338, Validation Loss: 0.109308\n",
      "Epoch: 332, Training Loss: 0.114335, Validation Loss: 0.109305\n",
      "Epoch: 333, Training Loss: 0.114333, Validation Loss: 0.109302\n",
      "Epoch: 334, Training Loss: 0.114331, Validation Loss: 0.109299\n",
      "Epoch: 335, Training Loss: 0.114328, Validation Loss: 0.109296\n",
      "Epoch: 336, Training Loss: 0.114326, Validation Loss: 0.109293\n",
      "Epoch: 337, Training Loss: 0.114323, Validation Loss: 0.109290\n",
      "Epoch: 338, Training Loss: 0.114321, Validation Loss: 0.109287\n",
      "Epoch: 339, Training Loss: 0.114319, Validation Loss: 0.109284\n",
      "Epoch: 340, Training Loss: 0.114316, Validation Loss: 0.109281\n",
      "Epoch: 341, Training Loss: 0.114314, Validation Loss: 0.109278\n",
      "Epoch: 342, Training Loss: 0.114312, Validation Loss: 0.109274\n",
      "Epoch: 343, Training Loss: 0.114309, Validation Loss: 0.109271\n",
      "Epoch: 344, Training Loss: 0.114307, Validation Loss: 0.109268\n",
      "Epoch: 345, Training Loss: 0.114304, Validation Loss: 0.109265\n",
      "Epoch: 346, Training Loss: 0.114302, Validation Loss: 0.109262\n",
      "Epoch: 347, Training Loss: 0.114299, Validation Loss: 0.109259\n",
      "Epoch: 348, Training Loss: 0.114297, Validation Loss: 0.109256\n",
      "Epoch: 349, Training Loss: 0.114295, Validation Loss: 0.109253\n",
      "Epoch: 350, Training Loss: 0.114292, Validation Loss: 0.109250\n",
      "Epoch: 351, Training Loss: 0.114290, Validation Loss: 0.109247\n",
      "Epoch: 352, Training Loss: 0.114287, Validation Loss: 0.109244\n",
      "Epoch: 353, Training Loss: 0.114285, Validation Loss: 0.109241\n",
      "Epoch: 354, Training Loss: 0.114282, Validation Loss: 0.109238\n",
      "Epoch: 355, Training Loss: 0.114280, Validation Loss: 0.109235\n",
      "Epoch: 356, Training Loss: 0.114278, Validation Loss: 0.109232\n",
      "Epoch: 357, Training Loss: 0.114275, Validation Loss: 0.109229\n",
      "Epoch: 358, Training Loss: 0.114273, Validation Loss: 0.109226\n",
      "Epoch: 359, Training Loss: 0.114270, Validation Loss: 0.109223\n",
      "Epoch: 360, Training Loss: 0.114268, Validation Loss: 0.109220\n",
      "Epoch: 361, Training Loss: 0.114265, Validation Loss: 0.109217\n",
      "Epoch: 362, Training Loss: 0.114263, Validation Loss: 0.109213\n",
      "Epoch: 363, Training Loss: 0.114261, Validation Loss: 0.109210\n",
      "Epoch: 364, Training Loss: 0.114258, Validation Loss: 0.109207\n",
      "Epoch: 365, Training Loss: 0.114256, Validation Loss: 0.109204\n",
      "Epoch: 366, Training Loss: 0.114253, Validation Loss: 0.109201\n",
      "Epoch: 367, Training Loss: 0.114251, Validation Loss: 0.109198\n",
      "Epoch: 368, Training Loss: 0.114248, Validation Loss: 0.109195\n",
      "Epoch: 369, Training Loss: 0.114246, Validation Loss: 0.109192\n",
      "Epoch: 370, Training Loss: 0.114243, Validation Loss: 0.109189\n",
      "Epoch: 371, Training Loss: 0.114241, Validation Loss: 0.109186\n",
      "Epoch: 372, Training Loss: 0.114238, Validation Loss: 0.109183\n",
      "Epoch: 373, Training Loss: 0.114236, Validation Loss: 0.109180\n",
      "Epoch: 374, Training Loss: 0.114233, Validation Loss: 0.109177\n",
      "Epoch: 375, Training Loss: 0.114231, Validation Loss: 0.109174\n",
      "Epoch: 376, Training Loss: 0.114228, Validation Loss: 0.109171\n",
      "Epoch: 377, Training Loss: 0.114226, Validation Loss: 0.109168\n",
      "Epoch: 378, Training Loss: 0.114223, Validation Loss: 0.109165\n",
      "Epoch: 379, Training Loss: 0.114221, Validation Loss: 0.109162\n",
      "Epoch: 380, Training Loss: 0.114218, Validation Loss: 0.109158\n",
      "Epoch: 381, Training Loss: 0.114216, Validation Loss: 0.109155\n",
      "Epoch: 382, Training Loss: 0.114213, Validation Loss: 0.109152\n",
      "Epoch: 383, Training Loss: 0.114211, Validation Loss: 0.109149\n",
      "Epoch: 384, Training Loss: 0.114208, Validation Loss: 0.109146\n",
      "Epoch: 385, Training Loss: 0.114206, Validation Loss: 0.109143\n",
      "Epoch: 386, Training Loss: 0.114203, Validation Loss: 0.109140\n",
      "Epoch: 387, Training Loss: 0.114201, Validation Loss: 0.109137\n",
      "Epoch: 388, Training Loss: 0.114198, Validation Loss: 0.109134\n",
      "Epoch: 389, Training Loss: 0.114196, Validation Loss: 0.109131\n",
      "Epoch: 390, Training Loss: 0.114193, Validation Loss: 0.109128\n",
      "Epoch: 391, Training Loss: 0.114191, Validation Loss: 0.109125\n",
      "Epoch: 392, Training Loss: 0.114188, Validation Loss: 0.109122\n",
      "Epoch: 393, Training Loss: 0.114186, Validation Loss: 0.109119\n",
      "Epoch: 394, Training Loss: 0.114183, Validation Loss: 0.109116\n",
      "Epoch: 395, Training Loss: 0.114181, Validation Loss: 0.109112\n",
      "Epoch: 396, Training Loss: 0.114178, Validation Loss: 0.109109\n",
      "Epoch: 397, Training Loss: 0.114176, Validation Loss: 0.109106\n",
      "Epoch: 398, Training Loss: 0.114173, Validation Loss: 0.109103\n",
      "Epoch: 399, Training Loss: 0.114170, Validation Loss: 0.109100\n",
      "Epoch: 400, Training Loss: 0.114168, Validation Loss: 0.109097\n",
      "Epoch: 401, Training Loss: 0.114165, Validation Loss: 0.109094\n",
      "Epoch: 402, Training Loss: 0.114163, Validation Loss: 0.109091\n",
      "Epoch: 403, Training Loss: 0.114160, Validation Loss: 0.109088\n",
      "Epoch: 404, Training Loss: 0.114158, Validation Loss: 0.109084\n",
      "Epoch: 405, Training Loss: 0.114155, Validation Loss: 0.109081\n",
      "Epoch: 406, Training Loss: 0.114152, Validation Loss: 0.109078\n",
      "Epoch: 407, Training Loss: 0.114150, Validation Loss: 0.109076\n",
      "Epoch: 408, Training Loss: 0.114147, Validation Loss: 0.109072\n",
      "Epoch: 409, Training Loss: 0.114145, Validation Loss: 0.109069\n",
      "Epoch: 410, Training Loss: 0.114142, Validation Loss: 0.109066\n",
      "Epoch: 411, Training Loss: 0.114139, Validation Loss: 0.109063\n",
      "Epoch: 412, Training Loss: 0.114137, Validation Loss: 0.109060\n",
      "Epoch: 413, Training Loss: 0.114134, Validation Loss: 0.109057\n",
      "Epoch: 414, Training Loss: 0.114132, Validation Loss: 0.109054\n",
      "Epoch: 415, Training Loss: 0.114129, Validation Loss: 0.109051\n",
      "Epoch: 416, Training Loss: 0.114126, Validation Loss: 0.109047\n",
      "Epoch: 417, Training Loss: 0.114124, Validation Loss: 0.109044\n",
      "Epoch: 418, Training Loss: 0.114121, Validation Loss: 0.109041\n",
      "Epoch: 419, Training Loss: 0.114118, Validation Loss: 0.109038\n",
      "Epoch: 420, Training Loss: 0.114116, Validation Loss: 0.109035\n",
      "Epoch: 421, Training Loss: 0.114113, Validation Loss: 0.109032\n",
      "Epoch: 422, Training Loss: 0.114111, Validation Loss: 0.109029\n",
      "Epoch: 423, Training Loss: 0.114108, Validation Loss: 0.109025\n",
      "Epoch: 424, Training Loss: 0.114105, Validation Loss: 0.109022\n",
      "Epoch: 425, Training Loss: 0.114103, Validation Loss: 0.109019\n",
      "Epoch: 426, Training Loss: 0.114100, Validation Loss: 0.109016\n",
      "Epoch: 427, Training Loss: 0.114097, Validation Loss: 0.109013\n",
      "Epoch: 428, Training Loss: 0.114095, Validation Loss: 0.109010\n",
      "Epoch: 429, Training Loss: 0.114092, Validation Loss: 0.109006\n",
      "Epoch: 430, Training Loss: 0.114089, Validation Loss: 0.109003\n",
      "Epoch: 431, Training Loss: 0.114086, Validation Loss: 0.109000\n",
      "Epoch: 432, Training Loss: 0.114084, Validation Loss: 0.108997\n",
      "Epoch: 433, Training Loss: 0.114081, Validation Loss: 0.108994\n",
      "Epoch: 434, Training Loss: 0.114078, Validation Loss: 0.108991\n",
      "Epoch: 435, Training Loss: 0.114076, Validation Loss: 0.108987\n",
      "Epoch: 436, Training Loss: 0.114073, Validation Loss: 0.108985\n",
      "Epoch: 437, Training Loss: 0.114070, Validation Loss: 0.108981\n",
      "Epoch: 438, Training Loss: 0.114067, Validation Loss: 0.108978\n",
      "Epoch: 439, Training Loss: 0.114065, Validation Loss: 0.108975\n",
      "Epoch: 440, Training Loss: 0.114062, Validation Loss: 0.108972\n",
      "Epoch: 441, Training Loss: 0.114059, Validation Loss: 0.108969\n",
      "Epoch: 442, Training Loss: 0.114056, Validation Loss: 0.108965\n",
      "Epoch: 443, Training Loss: 0.114054, Validation Loss: 0.108962\n",
      "Epoch: 444, Training Loss: 0.114051, Validation Loss: 0.108959\n",
      "Epoch: 445, Training Loss: 0.114048, Validation Loss: 0.108955\n",
      "Epoch: 446, Training Loss: 0.114045, Validation Loss: 0.108952\n",
      "Epoch: 447, Training Loss: 0.114043, Validation Loss: 0.108949\n",
      "Epoch: 448, Training Loss: 0.114040, Validation Loss: 0.108946\n",
      "Epoch: 449, Training Loss: 0.114037, Validation Loss: 0.108942\n",
      "Epoch: 450, Training Loss: 0.114034, Validation Loss: 0.108939\n",
      "Epoch: 451, Training Loss: 0.114031, Validation Loss: 0.108936\n",
      "Epoch: 452, Training Loss: 0.114028, Validation Loss: 0.108933\n",
      "Epoch: 453, Training Loss: 0.114026, Validation Loss: 0.108929\n",
      "Epoch: 454, Training Loss: 0.114023, Validation Loss: 0.108926\n",
      "Epoch: 455, Training Loss: 0.114020, Validation Loss: 0.108923\n",
      "Epoch: 456, Training Loss: 0.114017, Validation Loss: 0.108920\n",
      "Epoch: 457, Training Loss: 0.114014, Validation Loss: 0.108916\n",
      "Epoch: 458, Training Loss: 0.114011, Validation Loss: 0.108913\n",
      "Epoch: 459, Training Loss: 0.114009, Validation Loss: 0.108909\n",
      "Epoch: 460, Training Loss: 0.114006, Validation Loss: 0.108906\n",
      "Epoch: 461, Training Loss: 0.114003, Validation Loss: 0.108903\n",
      "Epoch: 462, Training Loss: 0.114000, Validation Loss: 0.108900\n",
      "Epoch: 463, Training Loss: 0.113997, Validation Loss: 0.108896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 464, Training Loss: 0.113994, Validation Loss: 0.108893\n",
      "Epoch: 465, Training Loss: 0.113991, Validation Loss: 0.108890\n",
      "Epoch: 466, Training Loss: 0.113988, Validation Loss: 0.108886\n",
      "Epoch: 467, Training Loss: 0.113985, Validation Loss: 0.108883\n",
      "Epoch: 468, Training Loss: 0.113982, Validation Loss: 0.108879\n",
      "Epoch: 469, Training Loss: 0.113979, Validation Loss: 0.108876\n",
      "Epoch: 470, Training Loss: 0.113976, Validation Loss: 0.108873\n",
      "Epoch: 471, Training Loss: 0.113973, Validation Loss: 0.108869\n",
      "Epoch: 472, Training Loss: 0.113970, Validation Loss: 0.108866\n",
      "Epoch: 473, Training Loss: 0.113967, Validation Loss: 0.108862\n",
      "Epoch: 474, Training Loss: 0.113964, Validation Loss: 0.108859\n",
      "Epoch: 475, Training Loss: 0.113961, Validation Loss: 0.108855\n",
      "Epoch: 476, Training Loss: 0.113958, Validation Loss: 0.108852\n",
      "Epoch: 477, Training Loss: 0.113955, Validation Loss: 0.108848\n",
      "Epoch: 478, Training Loss: 0.113952, Validation Loss: 0.108844\n",
      "Epoch: 479, Training Loss: 0.113949, Validation Loss: 0.108841\n",
      "Epoch: 480, Training Loss: 0.113946, Validation Loss: 0.108838\n",
      "Epoch: 481, Training Loss: 0.113943, Validation Loss: 0.108834\n",
      "Epoch: 482, Training Loss: 0.113940, Validation Loss: 0.108830\n",
      "Epoch: 483, Training Loss: 0.113937, Validation Loss: 0.108827\n",
      "Epoch: 484, Training Loss: 0.113933, Validation Loss: 0.108823\n",
      "Epoch: 485, Training Loss: 0.113930, Validation Loss: 0.108820\n",
      "Epoch: 486, Training Loss: 0.113927, Validation Loss: 0.108816\n",
      "Epoch: 487, Training Loss: 0.113924, Validation Loss: 0.108813\n",
      "Epoch: 488, Training Loss: 0.113921, Validation Loss: 0.108809\n",
      "Epoch: 489, Training Loss: 0.113918, Validation Loss: 0.108806\n",
      "Epoch: 490, Training Loss: 0.113914, Validation Loss: 0.108802\n",
      "Epoch: 491, Training Loss: 0.113911, Validation Loss: 0.108798\n",
      "Epoch: 492, Training Loss: 0.113908, Validation Loss: 0.108795\n",
      "Epoch: 493, Training Loss: 0.113905, Validation Loss: 0.108791\n",
      "Epoch: 494, Training Loss: 0.113902, Validation Loss: 0.108788\n",
      "Epoch: 495, Training Loss: 0.113898, Validation Loss: 0.108785\n",
      "Epoch: 496, Training Loss: 0.113895, Validation Loss: 0.108781\n",
      "Epoch: 497, Training Loss: 0.113892, Validation Loss: 0.108778\n",
      "Epoch: 498, Training Loss: 0.113889, Validation Loss: 0.108775\n",
      "Epoch: 499, Training Loss: 0.113885, Validation Loss: 0.108771\n",
      "Epoch: 500, Training Loss: 0.113882, Validation Loss: 0.108768\n",
      "Epoch: 501, Training Loss: 0.113879, Validation Loss: 0.108764\n",
      "Epoch: 502, Training Loss: 0.113875, Validation Loss: 0.108761\n",
      "Epoch: 503, Training Loss: 0.113872, Validation Loss: 0.108758\n",
      "Epoch: 504, Training Loss: 0.113869, Validation Loss: 0.108754\n",
      "Epoch: 505, Training Loss: 0.113866, Validation Loss: 0.108751\n",
      "Epoch: 506, Training Loss: 0.113862, Validation Loss: 0.108747\n",
      "Epoch: 507, Training Loss: 0.113859, Validation Loss: 0.108744\n",
      "Epoch: 508, Training Loss: 0.113856, Validation Loss: 0.108740\n",
      "Epoch: 509, Training Loss: 0.113852, Validation Loss: 0.108737\n",
      "Epoch: 510, Training Loss: 0.113849, Validation Loss: 0.108733\n",
      "Epoch: 511, Training Loss: 0.113846, Validation Loss: 0.108730\n",
      "Epoch: 512, Training Loss: 0.113842, Validation Loss: 0.108726\n",
      "Epoch: 513, Training Loss: 0.113839, Validation Loss: 0.108723\n",
      "Epoch: 514, Training Loss: 0.113835, Validation Loss: 0.108719\n",
      "Epoch: 515, Training Loss: 0.113832, Validation Loss: 0.108716\n",
      "Epoch: 516, Training Loss: 0.113828, Validation Loss: 0.108712\n",
      "Epoch: 517, Training Loss: 0.113825, Validation Loss: 0.108708\n",
      "Epoch: 518, Training Loss: 0.113822, Validation Loss: 0.108704\n",
      "Epoch: 519, Training Loss: 0.113818, Validation Loss: 0.108701\n",
      "Epoch: 520, Training Loss: 0.113814, Validation Loss: 0.108698\n",
      "Epoch: 521, Training Loss: 0.113811, Validation Loss: 0.108694\n",
      "Epoch: 522, Training Loss: 0.113807, Validation Loss: 0.108690\n",
      "Epoch: 523, Training Loss: 0.113804, Validation Loss: 0.108686\n",
      "Epoch: 524, Training Loss: 0.113800, Validation Loss: 0.108683\n",
      "Epoch: 525, Training Loss: 0.113797, Validation Loss: 0.108679\n",
      "Epoch: 526, Training Loss: 0.113793, Validation Loss: 0.108675\n",
      "Epoch: 527, Training Loss: 0.113789, Validation Loss: 0.108671\n",
      "Epoch: 528, Training Loss: 0.113786, Validation Loss: 0.108667\n",
      "Epoch: 529, Training Loss: 0.113782, Validation Loss: 0.108664\n",
      "Epoch: 530, Training Loss: 0.113778, Validation Loss: 0.108660\n",
      "Epoch: 531, Training Loss: 0.113774, Validation Loss: 0.108655\n",
      "Epoch: 532, Training Loss: 0.113770, Validation Loss: 0.108652\n",
      "Epoch: 533, Training Loss: 0.113766, Validation Loss: 0.108649\n",
      "Epoch: 534, Training Loss: 0.113762, Validation Loss: 0.108645\n",
      "Epoch: 535, Training Loss: 0.113758, Validation Loss: 0.108641\n",
      "Epoch: 536, Training Loss: 0.113754, Validation Loss: 0.108637\n",
      "Epoch: 537, Training Loss: 0.113750, Validation Loss: 0.108633\n",
      "Epoch: 538, Training Loss: 0.113745, Validation Loss: 0.108630\n",
      "Epoch: 539, Training Loss: 0.113741, Validation Loss: 0.108627\n",
      "Epoch: 540, Training Loss: 0.113737, Validation Loss: 0.108623\n",
      "Epoch: 541, Training Loss: 0.113732, Validation Loss: 0.108619\n",
      "Epoch: 542, Training Loss: 0.113728, Validation Loss: 0.108615\n",
      "Epoch: 543, Training Loss: 0.113724, Validation Loss: 0.108612\n",
      "Epoch: 544, Training Loss: 0.113720, Validation Loss: 0.108609\n",
      "Epoch: 545, Training Loss: 0.113715, Validation Loss: 0.108606\n",
      "Epoch: 546, Training Loss: 0.113711, Validation Loss: 0.108603\n",
      "Epoch: 547, Training Loss: 0.113707, Validation Loss: 0.108600\n",
      "Epoch: 548, Training Loss: 0.113703, Validation Loss: 0.108598\n",
      "Epoch: 549, Training Loss: 0.113698, Validation Loss: 0.108595\n",
      "Epoch: 550, Training Loss: 0.113694, Validation Loss: 0.108593\n",
      "Epoch: 551, Training Loss: 0.113689, Validation Loss: 0.108590\n",
      "Epoch: 552, Training Loss: 0.113685, Validation Loss: 0.108586\n",
      "Epoch: 553, Training Loss: 0.113680, Validation Loss: 0.108583\n",
      "Epoch: 554, Training Loss: 0.113675, Validation Loss: 0.108579\n",
      "Epoch: 555, Training Loss: 0.113671, Validation Loss: 0.108576\n",
      "Epoch: 556, Training Loss: 0.113666, Validation Loss: 0.108573\n",
      "Epoch: 557, Training Loss: 0.113661, Validation Loss: 0.108571\n",
      "Epoch: 558, Training Loss: 0.113656, Validation Loss: 0.108568\n",
      "Epoch: 559, Training Loss: 0.113651, Validation Loss: 0.108565\n",
      "Epoch: 560, Training Loss: 0.113647, Validation Loss: 0.108563\n",
      "Epoch: 561, Training Loss: 0.113642, Validation Loss: 0.108560\n",
      "Epoch: 562, Training Loss: 0.113636, Validation Loss: 0.108556\n",
      "Epoch: 563, Training Loss: 0.113631, Validation Loss: 0.108553\n",
      "Epoch: 564, Training Loss: 0.113626, Validation Loss: 0.108550\n",
      "Epoch: 565, Training Loss: 0.113621, Validation Loss: 0.108548\n",
      "Epoch: 566, Training Loss: 0.113616, Validation Loss: 0.108544\n",
      "Epoch: 567, Training Loss: 0.113610, Validation Loss: 0.108541\n",
      "Epoch: 568, Training Loss: 0.113605, Validation Loss: 0.108537\n",
      "Epoch: 569, Training Loss: 0.113599, Validation Loss: 0.108534\n",
      "Epoch: 570, Training Loss: 0.113594, Validation Loss: 0.108531\n",
      "Epoch: 571, Training Loss: 0.113589, Validation Loss: 0.108526\n",
      "Epoch: 572, Training Loss: 0.113583, Validation Loss: 0.108522\n",
      "Epoch: 573, Training Loss: 0.113578, Validation Loss: 0.108518\n",
      "Epoch: 574, Training Loss: 0.113572, Validation Loss: 0.108514\n",
      "Epoch: 575, Training Loss: 0.113567, Validation Loss: 0.108511\n",
      "Epoch: 576, Training Loss: 0.113561, Validation Loss: 0.108506\n",
      "Epoch: 577, Training Loss: 0.113556, Validation Loss: 0.108502\n",
      "Epoch: 578, Training Loss: 0.113551, Validation Loss: 0.108498\n",
      "Epoch: 579, Training Loss: 0.113546, Validation Loss: 0.108494\n",
      "Epoch: 580, Training Loss: 0.113540, Validation Loss: 0.108488\n",
      "Epoch: 581, Training Loss: 0.113535, Validation Loss: 0.108483\n",
      "Epoch: 582, Training Loss: 0.113530, Validation Loss: 0.108478\n",
      "Epoch: 583, Training Loss: 0.113524, Validation Loss: 0.108474\n",
      "Epoch: 584, Training Loss: 0.113519, Validation Loss: 0.108468\n",
      "Epoch: 585, Training Loss: 0.113513, Validation Loss: 0.108462\n",
      "Epoch: 586, Training Loss: 0.113508, Validation Loss: 0.108456\n",
      "Epoch: 587, Training Loss: 0.113502, Validation Loss: 0.108451\n",
      "Epoch: 588, Training Loss: 0.113496, Validation Loss: 0.108445\n",
      "Epoch: 589, Training Loss: 0.113491, Validation Loss: 0.108440\n",
      "Epoch: 590, Training Loss: 0.113485, Validation Loss: 0.108435\n",
      "Epoch: 591, Training Loss: 0.113479, Validation Loss: 0.108430\n",
      "Epoch: 592, Training Loss: 0.113473, Validation Loss: 0.108426\n",
      "Epoch: 593, Training Loss: 0.113467, Validation Loss: 0.108421\n",
      "Epoch: 594, Training Loss: 0.113461, Validation Loss: 0.108416\n",
      "Epoch: 595, Training Loss: 0.113455, Validation Loss: 0.108411\n",
      "Epoch: 596, Training Loss: 0.113449, Validation Loss: 0.108406\n",
      "Epoch: 597, Training Loss: 0.113443, Validation Loss: 0.108399\n",
      "Epoch: 598, Training Loss: 0.113437, Validation Loss: 0.108392\n",
      "Epoch: 599, Training Loss: 0.113431, Validation Loss: 0.108384\n",
      "Epoch: 600, Training Loss: 0.113424, Validation Loss: 0.108376\n",
      "Epoch: 601, Training Loss: 0.113418, Validation Loss: 0.108369\n",
      "Epoch: 602, Training Loss: 0.113412, Validation Loss: 0.108362\n",
      "Epoch: 603, Training Loss: 0.113405, Validation Loss: 0.108355\n",
      "Epoch: 604, Training Loss: 0.113399, Validation Loss: 0.108348\n",
      "Epoch: 605, Training Loss: 0.113392, Validation Loss: 0.108342\n",
      "Epoch: 606, Training Loss: 0.113385, Validation Loss: 0.108337\n",
      "Epoch: 607, Training Loss: 0.113379, Validation Loss: 0.108331\n",
      "Epoch: 608, Training Loss: 0.113372, Validation Loss: 0.108325\n",
      "Epoch: 609, Training Loss: 0.113365, Validation Loss: 0.108319\n",
      "Epoch: 610, Training Loss: 0.113358, Validation Loss: 0.108313\n",
      "Epoch: 611, Training Loss: 0.113351, Validation Loss: 0.108306\n",
      "Epoch: 612, Training Loss: 0.113344, Validation Loss: 0.108299\n",
      "Epoch: 613, Training Loss: 0.113337, Validation Loss: 0.108292\n",
      "Epoch: 614, Training Loss: 0.113329, Validation Loss: 0.108285\n",
      "Epoch: 615, Training Loss: 0.113322, Validation Loss: 0.108277\n",
      "Epoch: 616, Training Loss: 0.113314, Validation Loss: 0.108270\n",
      "Epoch: 617, Training Loss: 0.113307, Validation Loss: 0.108262\n",
      "Epoch: 618, Training Loss: 0.113299, Validation Loss: 0.108255\n",
      "Epoch: 619, Training Loss: 0.113291, Validation Loss: 0.108248\n",
      "Epoch: 620, Training Loss: 0.113284, Validation Loss: 0.108241\n",
      "Epoch: 621, Training Loss: 0.113276, Validation Loss: 0.108233\n",
      "Epoch: 622, Training Loss: 0.113268, Validation Loss: 0.108227\n",
      "Epoch: 623, Training Loss: 0.113259, Validation Loss: 0.108219\n",
      "Epoch: 624, Training Loss: 0.113251, Validation Loss: 0.108211\n",
      "Epoch: 625, Training Loss: 0.113243, Validation Loss: 0.108203\n",
      "Epoch: 626, Training Loss: 0.113234, Validation Loss: 0.108194\n",
      "Epoch: 627, Training Loss: 0.113226, Validation Loss: 0.108186\n",
      "Epoch: 628, Training Loss: 0.113217, Validation Loss: 0.108177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 629, Training Loss: 0.113208, Validation Loss: 0.108168\n",
      "Epoch: 630, Training Loss: 0.113199, Validation Loss: 0.108159\n",
      "Epoch: 631, Training Loss: 0.113190, Validation Loss: 0.108150\n",
      "Epoch: 632, Training Loss: 0.113181, Validation Loss: 0.108141\n",
      "Epoch: 633, Training Loss: 0.113172, Validation Loss: 0.108132\n",
      "Epoch: 634, Training Loss: 0.113162, Validation Loss: 0.108123\n",
      "Epoch: 635, Training Loss: 0.113153, Validation Loss: 0.108114\n",
      "Epoch: 636, Training Loss: 0.113143, Validation Loss: 0.108100\n",
      "Epoch: 637, Training Loss: 0.113132, Validation Loss: 0.108083\n",
      "Epoch: 638, Training Loss: 0.113120, Validation Loss: 0.108061\n",
      "Epoch: 639, Training Loss: 0.113102, Validation Loss: 0.108027\n",
      "Epoch: 640, Training Loss: 0.113081, Validation Loss: 0.107978\n",
      "Epoch: 641, Training Loss: 0.113065, Validation Loss: 0.107925\n",
      "Epoch: 642, Training Loss: 0.113052, Validation Loss: 0.107872\n",
      "Epoch: 643, Training Loss: 0.113032, Validation Loss: 0.107806\n",
      "Epoch: 644, Training Loss: 0.113015, Validation Loss: 0.107721\n",
      "Epoch: 645, Training Loss: 0.113005, Validation Loss: 0.107654\n",
      "Epoch: 646, Training Loss: 0.112992, Validation Loss: 0.107589\n",
      "Epoch: 647, Training Loss: 0.112983, Validation Loss: 0.107551\n",
      "Epoch: 648, Training Loss: 0.112973, Validation Loss: 0.107536\n",
      "Epoch: 649, Training Loss: 0.112963, Validation Loss: 0.107521\n",
      "Epoch: 650, Training Loss: 0.112953, Validation Loss: 0.107508\n",
      "Epoch: 651, Training Loss: 0.112941, Validation Loss: 0.107512\n",
      "Epoch: 652, Training Loss: 0.112929, Validation Loss: 0.107537\n",
      "Epoch: 653, Training Loss: 0.112917, Validation Loss: 0.107553\n",
      "Epoch: 654, Training Loss: 0.112905, Validation Loss: 0.107546\n",
      "Epoch: 655, Training Loss: 0.112891, Validation Loss: 0.107536\n",
      "Epoch: 656, Training Loss: 0.112876, Validation Loss: 0.107535\n",
      "Epoch: 657, Training Loss: 0.112862, Validation Loss: 0.107534\n",
      "Epoch: 658, Training Loss: 0.112848, Validation Loss: 0.107514\n",
      "Epoch: 659, Training Loss: 0.112833, Validation Loss: 0.107476\n",
      "Epoch: 660, Training Loss: 0.112819, Validation Loss: 0.107444\n",
      "Epoch: 661, Training Loss: 0.112805, Validation Loss: 0.107426\n",
      "Epoch: 662, Training Loss: 0.112792, Validation Loss: 0.107406\n",
      "Epoch: 663, Training Loss: 0.112779, Validation Loss: 0.107382\n",
      "Epoch: 664, Training Loss: 0.112766, Validation Loss: 0.107376\n",
      "Epoch: 665, Training Loss: 0.112752, Validation Loss: 0.107388\n",
      "Epoch: 666, Training Loss: 0.112738, Validation Loss: 0.107395\n",
      "Epoch: 667, Training Loss: 0.112723, Validation Loss: 0.107380\n",
      "Epoch: 668, Training Loss: 0.112708, Validation Loss: 0.107358\n",
      "Epoch: 669, Training Loss: 0.112692, Validation Loss: 0.107341\n",
      "Epoch: 670, Training Loss: 0.112677, Validation Loss: 0.107322\n",
      "Epoch: 671, Training Loss: 0.112662, Validation Loss: 0.107295\n",
      "Epoch: 672, Training Loss: 0.112646, Validation Loss: 0.107276\n",
      "Epoch: 673, Training Loss: 0.112631, Validation Loss: 0.107270\n",
      "Epoch: 674, Training Loss: 0.112616, Validation Loss: 0.107263\n",
      "Epoch: 675, Training Loss: 0.112600, Validation Loss: 0.107247\n",
      "Epoch: 676, Training Loss: 0.112584, Validation Loss: 0.107232\n",
      "Epoch: 677, Training Loss: 0.112567, Validation Loss: 0.107223\n",
      "Epoch: 678, Training Loss: 0.112551, Validation Loss: 0.107210\n",
      "Epoch: 679, Training Loss: 0.112534, Validation Loss: 0.107186\n",
      "Epoch: 680, Training Loss: 0.112517, Validation Loss: 0.107165\n",
      "Epoch: 681, Training Loss: 0.112500, Validation Loss: 0.107151\n",
      "Epoch: 682, Training Loss: 0.112482, Validation Loss: 0.107136\n",
      "Epoch: 683, Training Loss: 0.112465, Validation Loss: 0.107114\n",
      "Epoch: 684, Training Loss: 0.112447, Validation Loss: 0.107094\n",
      "Epoch: 685, Training Loss: 0.112429, Validation Loss: 0.107080\n",
      "Epoch: 686, Training Loss: 0.112411, Validation Loss: 0.107062\n",
      "Epoch: 687, Training Loss: 0.112393, Validation Loss: 0.107037\n",
      "Epoch: 688, Training Loss: 0.112374, Validation Loss: 0.107015\n",
      "Epoch: 689, Training Loss: 0.112355, Validation Loss: 0.107000\n",
      "Epoch: 690, Training Loss: 0.112336, Validation Loss: 0.106985\n",
      "Epoch: 691, Training Loss: 0.112317, Validation Loss: 0.106966\n",
      "Epoch: 692, Training Loss: 0.112297, Validation Loss: 0.106949\n",
      "Epoch: 693, Training Loss: 0.112277, Validation Loss: 0.106937\n",
      "Epoch: 694, Training Loss: 0.112257, Validation Loss: 0.106926\n",
      "Epoch: 695, Training Loss: 0.112237, Validation Loss: 0.106908\n",
      "Epoch: 696, Training Loss: 0.112216, Validation Loss: 0.106888\n",
      "Epoch: 697, Training Loss: 0.112196, Validation Loss: 0.106868\n",
      "Epoch: 698, Training Loss: 0.112175, Validation Loss: 0.106848\n",
      "Epoch: 699, Training Loss: 0.112153, Validation Loss: 0.106824\n",
      "Epoch: 700, Training Loss: 0.112132, Validation Loss: 0.106799\n",
      "Epoch: 701, Training Loss: 0.112110, Validation Loss: 0.106778\n",
      "Epoch: 702, Training Loss: 0.112088, Validation Loss: 0.106756\n",
      "Epoch: 703, Training Loss: 0.112065, Validation Loss: 0.106730\n",
      "Epoch: 704, Training Loss: 0.112043, Validation Loss: 0.106704\n",
      "Epoch: 705, Training Loss: 0.112020, Validation Loss: 0.106681\n",
      "Epoch: 706, Training Loss: 0.112401, Validation Loss: 0.108541\n",
      "Epoch: 707, Training Loss: 0.117616, Validation Loss: 0.118129\n",
      "Epoch: 708, Training Loss: 0.113462, Validation Loss: 0.108288\n",
      "Epoch: 709, Training Loss: 0.113471, Validation Loss: 0.106134\n",
      "Epoch: 710, Training Loss: 0.113934, Validation Loss: 0.103883\n",
      "Epoch: 711, Training Loss: 0.113236, Validation Loss: 0.103844\n",
      "Epoch: 712, Training Loss: 0.113302, Validation Loss: 0.106862\n",
      "Epoch: 713, Training Loss: 0.112598, Validation Loss: 0.107242\n",
      "Epoch: 714, Training Loss: 0.112756, Validation Loss: 0.107074\n",
      "Epoch: 715, Training Loss: 0.112457, Validation Loss: 0.108141\n",
      "Epoch: 716, Training Loss: 0.112781, Validation Loss: 0.110750\n",
      "Epoch: 717, Training Loss: 0.112470, Validation Loss: 0.110383\n",
      "Epoch: 718, Training Loss: 0.112569, Validation Loss: 0.108616\n",
      "Epoch: 719, Training Loss: 0.112077, Validation Loss: 0.107536\n",
      "Epoch: 720, Training Loss: 0.112280, Validation Loss: 0.108136\n",
      "Epoch: 721, Training Loss: 0.111804, Validation Loss: 0.106304\n",
      "Epoch: 722, Training Loss: 0.112251, Validation Loss: 0.104822\n",
      "Epoch: 723, Training Loss: 0.111772, Validation Loss: 0.104494\n",
      "Epoch: 724, Training Loss: 0.112178, Validation Loss: 0.105972\n",
      "Epoch: 725, Training Loss: 0.111617, Validation Loss: 0.105226\n",
      "Epoch: 726, Training Loss: 0.111931, Validation Loss: 0.105353\n",
      "Epoch: 727, Training Loss: 0.111477, Validation Loss: 0.106165\n",
      "Epoch: 728, Training Loss: 0.111805, Validation Loss: 0.107809\n",
      "Epoch: 729, Training Loss: 0.111468, Validation Loss: 0.107361\n",
      "Epoch: 730, Training Loss: 0.111727, Validation Loss: 0.107176\n",
      "Epoch: 731, Training Loss: 0.111395, Validation Loss: 0.107199\n",
      "Epoch: 732, Training Loss: 0.111842, Validation Loss: 0.107903\n",
      "Epoch: 733, Training Loss: 0.111547, Validation Loss: 0.106017\n",
      "Epoch: 734, Training Loss: 0.111383, Validation Loss: 0.105521\n",
      "Epoch: 735, Training Loss: 0.111498, Validation Loss: 0.106208\n",
      "Epoch: 736, Training Loss: 0.111274, Validation Loss: 0.105503\n",
      "Epoch: 737, Training Loss: 0.111436, Validation Loss: 0.104842\n",
      "Epoch: 738, Training Loss: 0.111152, Validation Loss: 0.105097\n",
      "Epoch: 739, Training Loss: 0.111318, Validation Loss: 0.106225\n",
      "Epoch: 740, Training Loss: 0.111049, Validation Loss: 0.105863\n",
      "Epoch: 741, Training Loss: 0.111210, Validation Loss: 0.105829\n",
      "Epoch: 742, Training Loss: 0.110992, Validation Loss: 0.106280\n",
      "Epoch: 743, Training Loss: 0.111118, Validation Loss: 0.106907\n",
      "Epoch: 744, Training Loss: 0.110928, Validation Loss: 0.106222\n",
      "Epoch: 745, Training Loss: 0.111008, Validation Loss: 0.105831\n",
      "Epoch: 746, Training Loss: 0.110852, Validation Loss: 0.105895\n",
      "Epoch: 747, Training Loss: 0.110903, Validation Loss: 0.105945\n",
      "Epoch: 748, Training Loss: 0.110792, Validation Loss: 0.105205\n",
      "Epoch: 749, Training Loss: 0.110811, Validation Loss: 0.104984\n",
      "Epoch: 750, Training Loss: 0.110737, Validation Loss: 0.105331\n",
      "Epoch: 751, Training Loss: 0.110715, Validation Loss: 0.105449\n",
      "Epoch: 752, Training Loss: 0.110668, Validation Loss: 0.105168\n",
      "Epoch: 753, Training Loss: 0.110621, Validation Loss: 0.105338\n",
      "Epoch: 754, Training Loss: 0.110600, Validation Loss: 0.105831\n",
      "Epoch: 755, Training Loss: 0.110542, Validation Loss: 0.105802\n",
      "Epoch: 756, Training Loss: 0.110535, Validation Loss: 0.105560\n",
      "Epoch: 757, Training Loss: 0.110467, Validation Loss: 0.105609\n",
      "Epoch: 758, Training Loss: 0.110461, Validation Loss: 0.105760\n",
      "Epoch: 759, Training Loss: 0.110393, Validation Loss: 0.105415\n",
      "Epoch: 760, Training Loss: 0.110386, Validation Loss: 0.105122\n",
      "Epoch: 761, Training Loss: 0.110326, Validation Loss: 0.105169\n",
      "Epoch: 762, Training Loss: 0.110312, Validation Loss: 0.105250\n",
      "Epoch: 763, Training Loss: 0.110259, Validation Loss: 0.105019\n",
      "Epoch: 764, Training Loss: 0.110237, Validation Loss: 0.104981\n",
      "Epoch: 765, Training Loss: 0.110192, Validation Loss: 0.105213\n",
      "Epoch: 766, Training Loss: 0.110162, Validation Loss: 0.105301\n",
      "Epoch: 767, Training Loss: 0.110126, Validation Loss: 0.105159\n",
      "Epoch: 768, Training Loss: 0.110092, Validation Loss: 0.105182\n",
      "Epoch: 769, Training Loss: 0.110061, Validation Loss: 0.105335\n",
      "Epoch: 770, Training Loss: 0.110022, Validation Loss: 0.105249\n",
      "Epoch: 771, Training Loss: 0.109994, Validation Loss: 0.105059\n",
      "Epoch: 772, Training Loss: 0.109954, Validation Loss: 0.105056\n",
      "Epoch: 773, Training Loss: 0.109928, Validation Loss: 0.105115\n",
      "Epoch: 774, Training Loss: 0.109889, Validation Loss: 0.104981\n",
      "Epoch: 775, Training Loss: 0.109864, Validation Loss: 0.104885\n",
      "Epoch: 776, Training Loss: 0.109829, Validation Loss: 0.104967\n",
      "Epoch: 777, Training Loss: 0.109803, Validation Loss: 0.105029\n",
      "Epoch: 778, Training Loss: 0.109771, Validation Loss: 0.104945\n",
      "Epoch: 779, Training Loss: 0.109744, Validation Loss: 0.104935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 780, Training Loss: 0.109715, Validation Loss: 0.105023\n",
      "Epoch: 781, Training Loss: 0.109687, Validation Loss: 0.105009\n",
      "Epoch: 782, Training Loss: 0.109660, Validation Loss: 0.104911\n",
      "Epoch: 783, Training Loss: 0.109632, Validation Loss: 0.104910\n",
      "Epoch: 784, Training Loss: 0.109607, Validation Loss: 0.104956\n",
      "Epoch: 785, Training Loss: 0.109579, Validation Loss: 0.104897\n",
      "Epoch: 786, Training Loss: 0.109556, Validation Loss: 0.104832\n",
      "Epoch: 787, Training Loss: 0.109838, Validation Loss: 0.107050\n",
      "Epoch: 788, Training Loss: 0.110062, Validation Loss: 0.104332\n",
      "Epoch: 789, Training Loss: 0.109547, Validation Loss: 0.104499\n",
      "Epoch: 790, Training Loss: 0.109837, Validation Loss: 0.105145\n",
      "Epoch: 791, Training Loss: 0.109572, Validation Loss: 0.104088\n",
      "Epoch: 792, Training Loss: 0.109616, Validation Loss: 0.104317\n",
      "Epoch: 793, Training Loss: 0.109581, Validation Loss: 0.105458\n",
      "Epoch: 794, Training Loss: 0.109465, Validation Loss: 0.105462\n",
      "Epoch: 795, Training Loss: 0.109577, Validation Loss: 0.104979\n",
      "Epoch: 796, Training Loss: 0.109362, Validation Loss: 0.105102\n",
      "Epoch: 797, Training Loss: 0.109523, Validation Loss: 0.105773\n",
      "Epoch: 798, Training Loss: 0.109296, Validation Loss: 0.104947\n",
      "Epoch: 799, Training Loss: 0.109446, Validation Loss: 0.104508\n",
      "Epoch: 800, Training Loss: 0.109280, Validation Loss: 0.104734\n",
      "Epoch: 801, Training Loss: 0.109361, Validation Loss: 0.104994\n",
      "Epoch: 802, Training Loss: 0.109276, Validation Loss: 0.104450\n",
      "Epoch: 803, Training Loss: 0.109266, Validation Loss: 0.104557\n",
      "Epoch: 804, Training Loss: 0.109265, Validation Loss: 0.105193\n",
      "Epoch: 805, Training Loss: 0.109201, Validation Loss: 0.105126\n",
      "Epoch: 806, Training Loss: 0.109245, Validation Loss: 0.104841\n",
      "Epoch: 807, Training Loss: 0.109159, Validation Loss: 0.104974\n",
      "Epoch: 808, Training Loss: 0.109209, Validation Loss: 0.105233\n",
      "Epoch: 809, Training Loss: 0.109132, Validation Loss: 0.104750\n",
      "Epoch: 810, Training Loss: 0.109166, Validation Loss: 0.104517\n",
      "Epoch: 811, Training Loss: 0.109123, Validation Loss: 0.104750\n",
      "Epoch: 812, Training Loss: 0.109123, Validation Loss: 0.104819\n",
      "Epoch: 813, Training Loss: 0.109112, Validation Loss: 0.104605\n",
      "Epoch: 814, Training Loss: 0.109083, Validation Loss: 0.104774\n",
      "Epoch: 815, Training Loss: 0.109097, Validation Loss: 0.105148\n",
      "Epoch: 816, Training Loss: 0.109061, Validation Loss: 0.105034\n",
      "Epoch: 817, Training Loss: 0.109077, Validation Loss: 0.104901\n",
      "Epoch: 818, Training Loss: 0.109045, Validation Loss: 0.105030\n",
      "Epoch: 819, Training Loss: 0.109051, Validation Loss: 0.105052\n",
      "Epoch: 820, Training Loss: 0.109033, Validation Loss: 0.104752\n",
      "Epoch: 821, Training Loss: 0.109027, Validation Loss: 0.104684\n",
      "Epoch: 822, Training Loss: 0.109025, Validation Loss: 0.104850\n",
      "Epoch: 823, Training Loss: 0.109008, Validation Loss: 0.104794\n",
      "Epoch: 824, Training Loss: 0.109012, Validation Loss: 0.104719\n",
      "Epoch: 825, Training Loss: 0.108993, Validation Loss: 0.104894\n",
      "Epoch: 826, Training Loss: 0.108997, Validation Loss: 0.105059\n",
      "Epoch: 827, Training Loss: 0.108984, Validation Loss: 0.104956\n",
      "Epoch: 828, Training Loss: 0.108982, Validation Loss: 0.104929\n",
      "Epoch: 829, Training Loss: 0.108975, Validation Loss: 0.105031\n",
      "Epoch: 830, Training Loss: 0.108967, Validation Loss: 0.104960\n",
      "Epoch: 831, Training Loss: 0.108965, Validation Loss: 0.104800\n",
      "Epoch: 832, Training Loss: 0.108956, Validation Loss: 0.104816\n",
      "Epoch: 833, Training Loss: 0.108956, Validation Loss: 0.104889\n",
      "Epoch: 834, Training Loss: 0.108947, Validation Loss: 0.104834\n",
      "Epoch: 835, Training Loss: 0.108944, Validation Loss: 0.104859\n",
      "Epoch: 836, Training Loss: 0.108939, Validation Loss: 0.104992\n",
      "Epoch: 837, Training Loss: 0.108933, Validation Loss: 0.105004\n",
      "Epoch: 838, Training Loss: 0.108930, Validation Loss: 0.104930\n",
      "Epoch: 839, Training Loss: 0.108923, Validation Loss: 0.104954\n",
      "Epoch: 840, Training Loss: 0.108922, Validation Loss: 0.104981\n",
      "Epoch: 841, Training Loss: 0.108915, Validation Loss: 0.104905\n",
      "Epoch: 842, Training Loss: 0.108912, Validation Loss: 0.104888\n",
      "Epoch: 843, Training Loss: 0.108908, Validation Loss: 0.104953\n",
      "Epoch: 844, Training Loss: 0.108903, Validation Loss: 0.104937\n",
      "Epoch: 845, Training Loss: 0.108899, Validation Loss: 0.104877\n",
      "Epoch: 846, Training Loss: 0.108894, Validation Loss: 0.104897\n",
      "Epoch: 847, Training Loss: 0.108891, Validation Loss: 0.104924\n",
      "Epoch: 848, Training Loss: 0.108886, Validation Loss: 0.104884\n",
      "Epoch: 849, Training Loss: 0.108882, Validation Loss: 0.104895\n",
      "Epoch: 850, Training Loss: 0.108878, Validation Loss: 0.104957\n",
      "Epoch: 851, Training Loss: 0.108873, Validation Loss: 0.104954\n",
      "Epoch: 852, Training Loss: 0.108869, Validation Loss: 0.104922\n",
      "Epoch: 853, Training Loss: 0.108865, Validation Loss: 0.104929\n",
      "Epoch: 854, Training Loss: 0.108861, Validation Loss: 0.104927\n",
      "Epoch: 855, Training Loss: 0.108857, Validation Loss: 0.104899\n",
      "Epoch: 856, Training Loss: 0.108852, Validation Loss: 0.104901\n",
      "Epoch: 857, Training Loss: 0.108848, Validation Loss: 0.104910\n",
      "Epoch: 858, Training Loss: 0.108844, Validation Loss: 0.104884\n",
      "Epoch: 859, Training Loss: 0.108840, Validation Loss: 0.104870\n",
      "Epoch: 860, Training Loss: 0.108835, Validation Loss: 0.104887\n",
      "Epoch: 861, Training Loss: 0.108831, Validation Loss: 0.104892\n",
      "Epoch: 862, Training Loss: 0.108827, Validation Loss: 0.104887\n",
      "Epoch: 863, Training Loss: 0.108822, Validation Loss: 0.104898\n",
      "Epoch: 864, Training Loss: 0.108818, Validation Loss: 0.104894\n",
      "Epoch: 865, Training Loss: 0.108814, Validation Loss: 0.104869\n",
      "Epoch: 866, Training Loss: 0.108809, Validation Loss: 0.104872\n",
      "Epoch: 867, Training Loss: 0.108805, Validation Loss: 0.104891\n",
      "Epoch: 868, Training Loss: 0.108801, Validation Loss: 0.104878\n",
      "Epoch: 869, Training Loss: 0.108796, Validation Loss: 0.104855\n",
      "Epoch: 870, Training Loss: 0.108792, Validation Loss: 0.104852\n",
      "Epoch: 871, Training Loss: 0.108787, Validation Loss: 0.104849\n",
      "Epoch: 872, Training Loss: 0.108783, Validation Loss: 0.104837\n",
      "Epoch: 873, Training Loss: 0.108778, Validation Loss: 0.104839\n",
      "Epoch: 874, Training Loss: 0.108774, Validation Loss: 0.104844\n",
      "Epoch: 875, Training Loss: 0.108769, Validation Loss: 0.104840\n",
      "Epoch: 876, Training Loss: 0.108765, Validation Loss: 0.104838\n",
      "Epoch: 877, Training Loss: 0.108760, Validation Loss: 0.104841\n",
      "Epoch: 878, Training Loss: 0.108755, Validation Loss: 0.104834\n",
      "Epoch: 879, Training Loss: 0.108751, Validation Loss: 0.104821\n",
      "Epoch: 880, Training Loss: 0.108746, Validation Loss: 0.104815\n",
      "Epoch: 881, Training Loss: 0.108742, Validation Loss: 0.104815\n",
      "Epoch: 882, Training Loss: 0.108737, Validation Loss: 0.104812\n",
      "Epoch: 883, Training Loss: 0.108732, Validation Loss: 0.104810\n",
      "Epoch: 884, Training Loss: 0.108728, Validation Loss: 0.104801\n",
      "Epoch: 885, Training Loss: 0.108723, Validation Loss: 0.104789\n",
      "Epoch: 886, Training Loss: 0.108718, Validation Loss: 0.104782\n",
      "Epoch: 887, Training Loss: 0.108713, Validation Loss: 0.104781\n",
      "Epoch: 888, Training Loss: 0.108708, Validation Loss: 0.104781\n",
      "Epoch: 889, Training Loss: 0.108704, Validation Loss: 0.104783\n",
      "Epoch: 890, Training Loss: 0.108699, Validation Loss: 0.104783\n",
      "Epoch: 891, Training Loss: 0.108694, Validation Loss: 0.104769\n",
      "Epoch: 892, Training Loss: 0.108689, Validation Loss: 0.104760\n",
      "Epoch: 893, Training Loss: 0.108684, Validation Loss: 0.104760\n",
      "Epoch: 894, Training Loss: 0.108679, Validation Loss: 0.104757\n",
      "Epoch: 895, Training Loss: 0.108674, Validation Loss: 0.104759\n",
      "Epoch: 896, Training Loss: 0.108669, Validation Loss: 0.104768\n",
      "Epoch: 897, Training Loss: 0.108664, Validation Loss: 0.104762\n",
      "Epoch: 898, Training Loss: 0.108659, Validation Loss: 0.104744\n",
      "Epoch: 899, Training Loss: 0.108654, Validation Loss: 0.104732\n",
      "Epoch: 900, Training Loss: 0.108649, Validation Loss: 0.104717\n",
      "Epoch: 901, Training Loss: 0.108644, Validation Loss: 0.104702\n",
      "Epoch: 902, Training Loss: 0.108638, Validation Loss: 0.104707\n",
      "Epoch: 903, Training Loss: 0.108633, Validation Loss: 0.104713\n",
      "Epoch: 904, Training Loss: 0.108628, Validation Loss: 0.104707\n",
      "Epoch: 905, Training Loss: 0.108622, Validation Loss: 0.104713\n",
      "Epoch: 906, Training Loss: 0.108617, Validation Loss: 0.104727\n",
      "Epoch: 907, Training Loss: 0.108612, Validation Loss: 0.104713\n",
      "Epoch: 908, Training Loss: 0.108606, Validation Loss: 0.104692\n",
      "Epoch: 909, Training Loss: 0.108601, Validation Loss: 0.104688\n",
      "Epoch: 910, Training Loss: 0.108595, Validation Loss: 0.104684\n",
      "Epoch: 911, Training Loss: 0.108590, Validation Loss: 0.104676\n",
      "Epoch: 912, Training Loss: 0.108584, Validation Loss: 0.104666\n",
      "Epoch: 913, Training Loss: 0.108579, Validation Loss: 0.104648\n",
      "Epoch: 914, Training Loss: 0.108573, Validation Loss: 0.104640\n",
      "Epoch: 915, Training Loss: 0.108567, Validation Loss: 0.104656\n",
      "Epoch: 916, Training Loss: 0.108562, Validation Loss: 0.104665\n",
      "Epoch: 917, Training Loss: 0.108556, Validation Loss: 0.104654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 918, Training Loss: 0.108551, Validation Loss: 0.104641\n",
      "Epoch: 919, Training Loss: 0.108545, Validation Loss: 0.104639\n",
      "Epoch: 920, Training Loss: 0.108539, Validation Loss: 0.104641\n",
      "Epoch: 921, Training Loss: 0.108533, Validation Loss: 0.104635\n",
      "Epoch: 922, Training Loss: 0.108528, Validation Loss: 0.104618\n",
      "Epoch: 923, Training Loss: 0.108522, Validation Loss: 0.104611\n",
      "Epoch: 924, Training Loss: 0.108516, Validation Loss: 0.104610\n",
      "Epoch: 925, Training Loss: 0.108510, Validation Loss: 0.104604\n",
      "Epoch: 926, Training Loss: 0.108504, Validation Loss: 0.104600\n",
      "Epoch: 927, Training Loss: 0.108498, Validation Loss: 0.104597\n",
      "Epoch: 928, Training Loss: 0.108492, Validation Loss: 0.104585\n",
      "Epoch: 929, Training Loss: 0.108486, Validation Loss: 0.104582\n",
      "Epoch: 930, Training Loss: 0.108480, Validation Loss: 0.104583\n",
      "Epoch: 931, Training Loss: 0.108474, Validation Loss: 0.104580\n",
      "Epoch: 932, Training Loss: 0.108468, Validation Loss: 0.104583\n",
      "Epoch: 933, Training Loss: 0.108462, Validation Loss: 0.104571\n",
      "Epoch: 934, Training Loss: 0.108456, Validation Loss: 0.104544\n",
      "Epoch: 935, Training Loss: 0.108450, Validation Loss: 0.104556\n",
      "Epoch: 936, Training Loss: 0.108444, Validation Loss: 0.104571\n",
      "Epoch: 937, Training Loss: 0.108438, Validation Loss: 0.104549\n",
      "Epoch: 938, Training Loss: 0.108432, Validation Loss: 0.104529\n",
      "Epoch: 939, Training Loss: 0.108425, Validation Loss: 0.104520\n",
      "Epoch: 940, Training Loss: 0.108419, Validation Loss: 0.104511\n",
      "Epoch: 941, Training Loss: 0.108412, Validation Loss: 0.104512\n",
      "Epoch: 942, Training Loss: 0.108406, Validation Loss: 0.104523\n",
      "Epoch: 943, Training Loss: 0.108400, Validation Loss: 0.104527\n",
      "Epoch: 944, Training Loss: 0.108393, Validation Loss: 0.104523\n",
      "Epoch: 945, Training Loss: 0.108386, Validation Loss: 0.104523\n",
      "Epoch: 946, Training Loss: 0.108380, Validation Loss: 0.104496\n",
      "Epoch: 947, Training Loss: 0.108373, Validation Loss: 0.104454\n",
      "Epoch: 948, Training Loss: 0.108366, Validation Loss: 0.104443\n",
      "Epoch: 949, Training Loss: 0.108359, Validation Loss: 0.104455\n",
      "Epoch: 950, Training Loss: 0.108352, Validation Loss: 0.104462\n",
      "Epoch: 951, Training Loss: 0.108345, Validation Loss: 0.104474\n",
      "Epoch: 952, Training Loss: 0.108339, Validation Loss: 0.104488\n",
      "Epoch: 953, Training Loss: 0.108332, Validation Loss: 0.104481\n",
      "Epoch: 954, Training Loss: 0.108325, Validation Loss: 0.104460\n",
      "Epoch: 955, Training Loss: 0.108318, Validation Loss: 0.104436\n",
      "Epoch: 956, Training Loss: 0.108310, Validation Loss: 0.104413\n",
      "Epoch: 957, Training Loss: 0.108303, Validation Loss: 0.104400\n",
      "Epoch: 958, Training Loss: 0.108296, Validation Loss: 0.104401\n",
      "Epoch: 959, Training Loss: 0.108289, Validation Loss: 0.104407\n",
      "Epoch: 960, Training Loss: 0.108282, Validation Loss: 0.104411\n",
      "Epoch: 961, Training Loss: 0.108274, Validation Loss: 0.104417\n",
      "Epoch: 962, Training Loss: 0.108267, Validation Loss: 0.104417\n",
      "Epoch: 963, Training Loss: 0.108260, Validation Loss: 0.104408\n",
      "Epoch: 964, Training Loss: 0.108252, Validation Loss: 0.104394\n",
      "Epoch: 965, Training Loss: 0.108245, Validation Loss: 0.104384\n",
      "Epoch: 966, Training Loss: 0.108237, Validation Loss: 0.104364\n",
      "Epoch: 967, Training Loss: 0.108230, Validation Loss: 0.104348\n",
      "Epoch: 968, Training Loss: 0.108222, Validation Loss: 0.104352\n",
      "Epoch: 969, Training Loss: 0.108214, Validation Loss: 0.104361\n",
      "Epoch: 970, Training Loss: 0.108206, Validation Loss: 0.104356\n",
      "Epoch: 971, Training Loss: 0.108199, Validation Loss: 0.104349\n",
      "Epoch: 972, Training Loss: 0.108191, Validation Loss: 0.104338\n",
      "Epoch: 973, Training Loss: 0.108183, Validation Loss: 0.104324\n",
      "Epoch: 974, Training Loss: 0.108175, Validation Loss: 0.104312\n",
      "Epoch: 975, Training Loss: 0.108167, Validation Loss: 0.104300\n",
      "Epoch: 976, Training Loss: 0.108159, Validation Loss: 0.104285\n",
      "Epoch: 977, Training Loss: 0.108151, Validation Loss: 0.104300\n",
      "Epoch: 978, Training Loss: 0.108143, Validation Loss: 0.104314\n",
      "Epoch: 979, Training Loss: 0.108134, Validation Loss: 0.104290\n",
      "Epoch: 980, Training Loss: 0.108126, Validation Loss: 0.104277\n",
      "Epoch: 981, Training Loss: 0.108118, Validation Loss: 0.104267\n",
      "Epoch: 982, Training Loss: 0.108109, Validation Loss: 0.104261\n",
      "Epoch: 983, Training Loss: 0.108101, Validation Loss: 0.104262\n",
      "Epoch: 984, Training Loss: 0.108092, Validation Loss: 0.104261\n",
      "Epoch: 985, Training Loss: 0.108084, Validation Loss: 0.104255\n",
      "Epoch: 986, Training Loss: 0.108075, Validation Loss: 0.104256\n",
      "Epoch: 987, Training Loss: 0.108066, Validation Loss: 0.104243\n",
      "Epoch: 988, Training Loss: 0.108058, Validation Loss: 0.104217\n",
      "Epoch: 989, Training Loss: 0.108049, Validation Loss: 0.104203\n",
      "Epoch: 990, Training Loss: 0.108040, Validation Loss: 0.104200\n",
      "Epoch: 991, Training Loss: 0.108031, Validation Loss: 0.104196\n",
      "Epoch: 992, Training Loss: 0.108022, Validation Loss: 0.104194\n",
      "Epoch: 993, Training Loss: 0.108013, Validation Loss: 0.104192\n",
      "Epoch: 994, Training Loss: 0.108004, Validation Loss: 0.104184\n",
      "Epoch: 995, Training Loss: 0.107995, Validation Loss: 0.104172\n",
      "Epoch: 996, Training Loss: 0.107985, Validation Loss: 0.104153\n",
      "Epoch: 997, Training Loss: 0.107976, Validation Loss: 0.104144\n",
      "Epoch: 998, Training Loss: 0.107966, Validation Loss: 0.104146\n",
      "Epoch: 999, Training Loss: 0.107957, Validation Loss: 0.104145\n",
      "Epoch: 1000, Training Loss: 0.107947, Validation Loss: 0.104130\n",
      "Epoch: 1001, Training Loss: 0.107938, Validation Loss: 0.104129\n",
      "Epoch: 1002, Training Loss: 0.107928, Validation Loss: 0.104128\n",
      "Epoch: 1003, Training Loss: 0.107918, Validation Loss: 0.104123\n",
      "Epoch: 1004, Training Loss: 0.107908, Validation Loss: 0.104097\n",
      "Epoch: 1005, Training Loss: 0.107898, Validation Loss: 0.104077\n",
      "Epoch: 1006, Training Loss: 0.107889, Validation Loss: 0.104070\n",
      "Epoch: 1007, Training Loss: 0.107878, Validation Loss: 0.104074\n",
      "Epoch: 1008, Training Loss: 0.107868, Validation Loss: 0.104076\n",
      "Epoch: 1009, Training Loss: 0.107858, Validation Loss: 0.104059\n",
      "Epoch: 1010, Training Loss: 0.107848, Validation Loss: 0.104041\n",
      "Epoch: 1011, Training Loss: 0.107837, Validation Loss: 0.104040\n",
      "Epoch: 1012, Training Loss: 0.107827, Validation Loss: 0.104040\n",
      "Epoch: 1013, Training Loss: 0.107816, Validation Loss: 0.104033\n",
      "Epoch: 1014, Training Loss: 0.107806, Validation Loss: 0.104017\n",
      "Epoch: 1015, Training Loss: 0.107795, Validation Loss: 0.104003\n",
      "Epoch: 1016, Training Loss: 0.107784, Validation Loss: 0.103988\n",
      "Epoch: 1017, Training Loss: 0.107773, Validation Loss: 0.103979\n",
      "Epoch: 1018, Training Loss: 0.107762, Validation Loss: 0.103979\n",
      "Epoch: 1019, Training Loss: 0.107751, Validation Loss: 0.103968\n",
      "Epoch: 1020, Training Loss: 0.107740, Validation Loss: 0.103962\n",
      "Epoch: 1021, Training Loss: 0.107729, Validation Loss: 0.103958\n",
      "Epoch: 1022, Training Loss: 0.107718, Validation Loss: 0.103940\n",
      "Epoch: 1023, Training Loss: 0.107706, Validation Loss: 0.103915\n",
      "Epoch: 1024, Training Loss: 0.107695, Validation Loss: 0.103914\n",
      "Epoch: 1025, Training Loss: 0.107683, Validation Loss: 0.103922\n",
      "Epoch: 1026, Training Loss: 0.107671, Validation Loss: 0.103921\n",
      "Epoch: 1027, Training Loss: 0.107659, Validation Loss: 0.103907\n",
      "Epoch: 1028, Training Loss: 0.107647, Validation Loss: 0.103876\n",
      "Epoch: 1029, Training Loss: 0.107635, Validation Loss: 0.103835\n",
      "Epoch: 1030, Training Loss: 0.107623, Validation Loss: 0.103821\n",
      "Epoch: 1031, Training Loss: 0.107611, Validation Loss: 0.103838\n",
      "Epoch: 1032, Training Loss: 0.107599, Validation Loss: 0.103862\n",
      "Epoch: 1033, Training Loss: 0.107586, Validation Loss: 0.103853\n",
      "Epoch: 1034, Training Loss: 0.107574, Validation Loss: 0.103834\n",
      "Epoch: 1035, Training Loss: 0.107561, Validation Loss: 0.103814\n",
      "Epoch: 1036, Training Loss: 0.107548, Validation Loss: 0.103799\n",
      "Epoch: 1037, Training Loss: 0.107535, Validation Loss: 0.103797\n",
      "Epoch: 1038, Training Loss: 0.107522, Validation Loss: 0.103770\n",
      "Epoch: 1039, Training Loss: 0.107509, Validation Loss: 0.103740\n",
      "Epoch: 1040, Training Loss: 0.107496, Validation Loss: 0.103737\n",
      "Epoch: 1041, Training Loss: 0.107482, Validation Loss: 0.103739\n",
      "Epoch: 1042, Training Loss: 0.107469, Validation Loss: 0.103741\n",
      "Epoch: 1043, Training Loss: 0.107455, Validation Loss: 0.103736\n",
      "Epoch: 1044, Training Loss: 0.107442, Validation Loss: 0.103712\n",
      "Epoch: 1045, Training Loss: 0.107428, Validation Loss: 0.103686\n",
      "Epoch: 1046, Training Loss: 0.107414, Validation Loss: 0.103673\n",
      "Epoch: 1047, Training Loss: 0.107400, Validation Loss: 0.103652\n",
      "Epoch: 1048, Training Loss: 0.107386, Validation Loss: 0.103639\n",
      "Epoch: 1049, Training Loss: 0.107372, Validation Loss: 0.103646\n",
      "Epoch: 1050, Training Loss: 0.107357, Validation Loss: 0.103649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1051, Training Loss: 0.107342, Validation Loss: 0.103648\n",
      "Epoch: 1052, Training Loss: 0.107328, Validation Loss: 0.103630\n",
      "Epoch: 1053, Training Loss: 0.107313, Validation Loss: 0.103593\n",
      "Epoch: 1054, Training Loss: 0.107298, Validation Loss: 0.103564\n",
      "Epoch: 1055, Training Loss: 0.107282, Validation Loss: 0.103561\n",
      "Epoch: 1056, Training Loss: 0.107268, Validation Loss: 0.103559\n",
      "Epoch: 1057, Training Loss: 0.107255, Validation Loss: 0.103576\n",
      "Epoch: 1058, Training Loss: 0.107242, Validation Loss: 0.103602\n",
      "Epoch: 1059, Training Loss: 0.107229, Validation Loss: 0.103635\n",
      "Epoch: 1060, Training Loss: 0.107216, Validation Loss: 0.103656\n",
      "Epoch: 1061, Training Loss: 0.107202, Validation Loss: 0.103656\n",
      "Epoch: 1062, Training Loss: 0.107187, Validation Loss: 0.103617\n",
      "Epoch: 1063, Training Loss: 0.107172, Validation Loss: 0.103557\n",
      "Epoch: 1064, Training Loss: 0.107157, Validation Loss: 0.103509\n",
      "Epoch: 1065, Training Loss: 0.107141, Validation Loss: 0.103476\n",
      "Epoch: 1066, Training Loss: 0.107125, Validation Loss: 0.103472\n",
      "Epoch: 1067, Training Loss: 0.107111, Validation Loss: 0.103492\n",
      "Epoch: 1068, Training Loss: 0.107096, Validation Loss: 0.103491\n",
      "Epoch: 1069, Training Loss: 0.107082, Validation Loss: 0.103449\n",
      "Epoch: 1070, Training Loss: 0.107067, Validation Loss: 0.103384\n",
      "Epoch: 1071, Training Loss: 0.107054, Validation Loss: 0.103347\n",
      "Epoch: 1072, Training Loss: 0.107039, Validation Loss: 0.103376\n",
      "Epoch: 1073, Training Loss: 0.107025, Validation Loss: 0.103448\n",
      "Epoch: 1074, Training Loss: 0.107011, Validation Loss: 0.103488\n",
      "Epoch: 1075, Training Loss: 0.106996, Validation Loss: 0.103457\n",
      "Epoch: 1076, Training Loss: 0.106981, Validation Loss: 0.103409\n",
      "Epoch: 1077, Training Loss: 0.106966, Validation Loss: 0.103370\n",
      "Epoch: 1078, Training Loss: 0.106951, Validation Loss: 0.103380\n",
      "Epoch: 1079, Training Loss: 0.106936, Validation Loss: 0.103398\n",
      "Epoch: 1080, Training Loss: 0.106920, Validation Loss: 0.103337\n",
      "Epoch: 1081, Training Loss: 0.106905, Validation Loss: 0.103255\n",
      "Epoch: 1082, Training Loss: 0.106889, Validation Loss: 0.103219\n",
      "Epoch: 1083, Training Loss: 0.106874, Validation Loss: 0.103207\n",
      "Epoch: 1084, Training Loss: 0.106859, Validation Loss: 0.103217\n",
      "Epoch: 1085, Training Loss: 0.106843, Validation Loss: 0.103249\n",
      "Epoch: 1086, Training Loss: 0.106827, Validation Loss: 0.103253\n",
      "Epoch: 1087, Training Loss: 0.106812, Validation Loss: 0.103234\n",
      "Epoch: 1088, Training Loss: 0.106796, Validation Loss: 0.103214\n",
      "Epoch: 1089, Training Loss: 0.106779, Validation Loss: 0.103178\n",
      "Epoch: 1090, Training Loss: 0.106763, Validation Loss: 0.103134\n",
      "Epoch: 1091, Training Loss: 0.106747, Validation Loss: 0.103115\n",
      "Epoch: 1092, Training Loss: 0.106730, Validation Loss: 0.103113\n",
      "Epoch: 1093, Training Loss: 0.106713, Validation Loss: 0.103089\n",
      "Epoch: 1094, Training Loss: 0.106696, Validation Loss: 0.103033\n",
      "Epoch: 1095, Training Loss: 0.106679, Validation Loss: 0.102996\n",
      "Epoch: 1096, Training Loss: 0.106662, Validation Loss: 0.102986\n",
      "Epoch: 1097, Training Loss: 0.106645, Validation Loss: 0.102942\n",
      "Epoch: 1098, Training Loss: 0.106628, Validation Loss: 0.102953\n",
      "Epoch: 1099, Training Loss: 0.106611, Validation Loss: 0.102969\n",
      "Epoch: 1100, Training Loss: 0.106593, Validation Loss: 0.102910\n",
      "Epoch: 1101, Training Loss: 0.106575, Validation Loss: 0.102892\n",
      "Epoch: 1102, Training Loss: 0.106557, Validation Loss: 0.102942\n",
      "Epoch: 1103, Training Loss: 0.106539, Validation Loss: 0.102883\n",
      "Epoch: 1104, Training Loss: 0.106522, Validation Loss: 0.102759\n",
      "Epoch: 1105, Training Loss: 0.106503, Validation Loss: 0.102746\n",
      "Epoch: 1106, Training Loss: 0.106484, Validation Loss: 0.102798\n",
      "Epoch: 1107, Training Loss: 0.106465, Validation Loss: 0.102801\n",
      "Epoch: 1108, Training Loss: 0.106447, Validation Loss: 0.102791\n",
      "Epoch: 1109, Training Loss: 0.106427, Validation Loss: 0.102698\n",
      "Epoch: 1110, Training Loss: 0.106408, Validation Loss: 0.102594\n",
      "Epoch: 1111, Training Loss: 0.106388, Validation Loss: 0.102603\n",
      "Epoch: 1112, Training Loss: 0.106370, Validation Loss: 0.102598\n",
      "Epoch: 1113, Training Loss: 0.106349, Validation Loss: 0.102525\n",
      "Epoch: 1114, Training Loss: 0.106329, Validation Loss: 0.102498\n",
      "Epoch: 1115, Training Loss: 0.106308, Validation Loss: 0.102547\n",
      "Epoch: 1116, Training Loss: 0.106288, Validation Loss: 0.102552\n",
      "Epoch: 1117, Training Loss: 0.106267, Validation Loss: 0.102501\n",
      "Epoch: 1118, Training Loss: 0.106245, Validation Loss: 0.102473\n",
      "Epoch: 1119, Training Loss: 0.106225, Validation Loss: 0.102375\n",
      "Epoch: 1120, Training Loss: 0.106203, Validation Loss: 0.102362\n",
      "Epoch: 1121, Training Loss: 0.106181, Validation Loss: 0.102341\n",
      "Epoch: 1122, Training Loss: 0.106160, Validation Loss: 0.102252\n",
      "Epoch: 1123, Training Loss: 0.106140, Validation Loss: 0.102283\n",
      "Epoch: 1124, Training Loss: 0.106117, Validation Loss: 0.102302\n",
      "Epoch: 1125, Training Loss: 0.106094, Validation Loss: 0.102159\n",
      "Epoch: 1126, Training Loss: 0.106073, Validation Loss: 0.102149\n",
      "Epoch: 1127, Training Loss: 0.106050, Validation Loss: 0.102224\n",
      "Epoch: 1128, Training Loss: 0.106025, Validation Loss: 0.102116\n",
      "Epoch: 1129, Training Loss: 0.106005, Validation Loss: 0.101983\n",
      "Epoch: 1130, Training Loss: 0.105979, Validation Loss: 0.102042\n",
      "Epoch: 1131, Training Loss: 0.105956, Validation Loss: 0.102067\n",
      "Epoch: 1132, Training Loss: 0.105932, Validation Loss: 0.101970\n",
      "Epoch: 1133, Training Loss: 0.105909, Validation Loss: 0.101873\n",
      "Epoch: 1134, Training Loss: 0.105882, Validation Loss: 0.101877\n",
      "Epoch: 1135, Training Loss: 0.105858, Validation Loss: 0.101882\n",
      "Epoch: 1136, Training Loss: 0.105833, Validation Loss: 0.101845\n",
      "Epoch: 1137, Training Loss: 0.105807, Validation Loss: 0.101754\n",
      "Epoch: 1138, Training Loss: 0.105781, Validation Loss: 0.101729\n",
      "Epoch: 1139, Training Loss: 0.105756, Validation Loss: 0.101703\n",
      "Epoch: 1140, Training Loss: 0.105730, Validation Loss: 0.101643\n",
      "Epoch: 1141, Training Loss: 0.105703, Validation Loss: 0.101599\n",
      "Epoch: 1142, Training Loss: 0.105676, Validation Loss: 0.101559\n",
      "Epoch: 1143, Training Loss: 0.105649, Validation Loss: 0.101512\n",
      "Epoch: 1144, Training Loss: 0.105622, Validation Loss: 0.101512\n",
      "Epoch: 1145, Training Loss: 0.105594, Validation Loss: 0.101468\n",
      "Epoch: 1146, Training Loss: 0.105566, Validation Loss: 0.101360\n",
      "Epoch: 1147, Training Loss: 0.105537, Validation Loss: 0.101328\n",
      "Epoch: 1148, Training Loss: 0.105510, Validation Loss: 0.101359\n",
      "Epoch: 1149, Training Loss: 0.105480, Validation Loss: 0.101255\n",
      "Epoch: 1150, Training Loss: 0.105452, Validation Loss: 0.101121\n",
      "Epoch: 1151, Training Loss: 0.105421, Validation Loss: 0.101135\n",
      "Epoch: 1152, Training Loss: 0.105392, Validation Loss: 0.101129\n",
      "Epoch: 1153, Training Loss: 0.105361, Validation Loss: 0.101093\n",
      "Epoch: 1154, Training Loss: 0.105331, Validation Loss: 0.101074\n",
      "Epoch: 1155, Training Loss: 0.105300, Validation Loss: 0.101041\n",
      "Epoch: 1156, Training Loss: 0.105268, Validation Loss: 0.100960\n",
      "Epoch: 1157, Training Loss: 0.105236, Validation Loss: 0.100874\n",
      "Epoch: 1158, Training Loss: 0.105204, Validation Loss: 0.100711\n",
      "Epoch: 1159, Training Loss: 0.105172, Validation Loss: 0.100632\n",
      "Epoch: 1160, Training Loss: 0.105139, Validation Loss: 0.100696\n",
      "Epoch: 1161, Training Loss: 0.105105, Validation Loss: 0.100608\n",
      "Epoch: 1162, Training Loss: 0.105071, Validation Loss: 0.100527\n",
      "Epoch: 1163, Training Loss: 0.105036, Validation Loss: 0.100570\n",
      "Epoch: 1164, Training Loss: 0.105003, Validation Loss: 0.100508\n",
      "Epoch: 1165, Training Loss: 0.104968, Validation Loss: 0.100401\n",
      "Epoch: 1166, Training Loss: 0.104932, Validation Loss: 0.100376\n",
      "Epoch: 1167, Training Loss: 0.104897, Validation Loss: 0.100270\n",
      "Epoch: 1168, Training Loss: 0.104861, Validation Loss: 0.100088\n",
      "Epoch: 1169, Training Loss: 0.104823, Validation Loss: 0.100122\n",
      "Epoch: 1170, Training Loss: 0.104787, Validation Loss: 0.100133\n",
      "Epoch: 1171, Training Loss: 0.104749, Validation Loss: 0.099934\n",
      "Epoch: 1172, Training Loss: 0.104711, Validation Loss: 0.099812\n",
      "Epoch: 1173, Training Loss: 0.104672, Validation Loss: 0.099886\n",
      "Epoch: 1174, Training Loss: 0.104635, Validation Loss: 0.099860\n",
      "Epoch: 1175, Training Loss: 0.104595, Validation Loss: 0.099562\n",
      "Epoch: 1176, Training Loss: 0.104554, Validation Loss: 0.099500\n",
      "Epoch: 1177, Training Loss: 0.104515, Validation Loss: 0.099658\n",
      "Epoch: 1178, Training Loss: 0.104469, Validation Loss: 0.099497\n",
      "Epoch: 1179, Training Loss: 0.104429, Validation Loss: 0.099272\n",
      "Epoch: 1180, Training Loss: 0.104385, Validation Loss: 0.099212\n",
      "Epoch: 1181, Training Loss: 0.104342, Validation Loss: 0.099261\n",
      "Epoch: 1182, Training Loss: 0.104298, Validation Loss: 0.099194\n",
      "Epoch: 1183, Training Loss: 0.104256, Validation Loss: 0.098875\n",
      "Epoch: 1184, Training Loss: 0.104213, Validation Loss: 0.098746\n",
      "Epoch: 1185, Training Loss: 0.104168, Validation Loss: 0.098988\n",
      "Epoch: 1186, Training Loss: 0.104115, Validation Loss: 0.098842\n",
      "Epoch: 1187, Training Loss: 0.104081, Validation Loss: 0.098383\n",
      "Epoch: 1188, Training Loss: 0.104016, Validation Loss: 0.098442\n",
      "Epoch: 1189, Training Loss: 0.103997, Validation Loss: 0.098768\n",
      "Epoch: 1190, Training Loss: 0.103914, Validation Loss: 0.098285\n",
      "Epoch: 1191, Training Loss: 0.103899, Validation Loss: 0.097837\n",
      "Epoch: 1192, Training Loss: 0.103813, Validation Loss: 0.098182\n",
      "Epoch: 1193, Training Loss: 0.103774, Validation Loss: 0.098244\n",
      "Epoch: 1194, Training Loss: 0.103704, Validation Loss: 0.097844\n",
      "Epoch: 1195, Training Loss: 0.103683, Validation Loss: 0.097442\n",
      "Epoch: 1196, Training Loss: 0.103607, Validation Loss: 0.097827\n",
      "Epoch: 1197, Training Loss: 0.103558, Validation Loss: 0.097694\n",
      "Epoch: 1198, Training Loss: 0.103554, Validation Loss: 0.096948\n",
      "Epoch: 1199, Training Loss: 0.103429, Validation Loss: 0.097359\n",
      "Epoch: 1200, Training Loss: 0.103419, Validation Loss: 0.097517\n",
      "Epoch: 1201, Training Loss: 0.103317, Validation Loss: 0.096895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1202, Training Loss: 0.103271, Validation Loss: 0.096685\n",
      "Epoch: 1203, Training Loss: 0.103227, Validation Loss: 0.096992\n",
      "Epoch: 1204, Training Loss: 0.103134, Validation Loss: 0.096563\n",
      "Epoch: 1205, Training Loss: 0.103103, Validation Loss: 0.096197\n",
      "Epoch: 1206, Training Loss: 0.103051, Validation Loss: 0.096612\n",
      "Epoch: 1207, Training Loss: 0.102953, Validation Loss: 0.096121\n",
      "Epoch: 1208, Training Loss: 0.102926, Validation Loss: 0.095795\n",
      "Epoch: 1209, Training Loss: 0.102851, Validation Loss: 0.096217\n",
      "Epoch: 1210, Training Loss: 0.102776, Validation Loss: 0.095829\n",
      "Epoch: 1211, Training Loss: 0.102722, Validation Loss: 0.095388\n",
      "Epoch: 1212, Training Loss: 0.102628, Validation Loss: 0.095439\n",
      "Epoch: 1213, Training Loss: 0.102604, Validation Loss: 0.095539\n",
      "Epoch: 1214, Training Loss: 0.102491, Validation Loss: 0.095118\n",
      "Epoch: 1215, Training Loss: 0.102475, Validation Loss: 0.094613\n",
      "Epoch: 1216, Training Loss: 0.102383, Validation Loss: 0.095091\n",
      "Epoch: 1217, Training Loss: 0.102334, Validation Loss: 0.095015\n",
      "Epoch: 1218, Training Loss: 0.102228, Validation Loss: 0.094151\n",
      "Epoch: 1219, Training Loss: 0.102142, Validation Loss: 0.094026\n",
      "Epoch: 1220, Training Loss: 0.102078, Validation Loss: 0.094312\n",
      "Epoch: 1221, Training Loss: 0.101984, Validation Loss: 0.094077\n",
      "Epoch: 1222, Training Loss: 0.101919, Validation Loss: 0.093503\n",
      "Epoch: 1223, Training Loss: 0.101809, Validation Loss: 0.093592\n",
      "Epoch: 1224, Training Loss: 0.101773, Validation Loss: 0.093757\n",
      "Epoch: 1225, Training Loss: 0.101654, Validation Loss: 0.092971\n",
      "Epoch: 1226, Training Loss: 0.101568, Validation Loss: 0.092913\n",
      "Epoch: 1227, Training Loss: 0.101507, Validation Loss: 0.092948\n",
      "Epoch: 1228, Training Loss: 0.101413, Validation Loss: 0.092188\n",
      "Epoch: 1229, Training Loss: 0.101332, Validation Loss: 0.092587\n",
      "Epoch: 1230, Training Loss: 0.101222, Validation Loss: 0.091934\n",
      "Epoch: 1231, Training Loss: 0.101117, Validation Loss: 0.092039\n",
      "Epoch: 1232, Training Loss: 0.101036, Validation Loss: 0.091675\n",
      "Epoch: 1233, Training Loss: 0.100970, Validation Loss: 0.091728\n",
      "Epoch: 1234, Training Loss: 0.100864, Validation Loss: 0.090858\n",
      "Epoch: 1235, Training Loss: 0.100818, Validation Loss: 0.091371\n",
      "Epoch: 1236, Training Loss: 0.100753, Validation Loss: 0.090153\n",
      "Epoch: 1237, Training Loss: 0.100643, Validation Loss: 0.091072\n",
      "Epoch: 1238, Training Loss: 0.100453, Validation Loss: 0.090195\n",
      "Epoch: 1239, Training Loss: 0.100379, Validation Loss: 0.089763\n",
      "Epoch: 1240, Training Loss: 0.100303, Validation Loss: 0.090118\n",
      "Epoch: 1241, Training Loss: 0.100222, Validation Loss: 0.089138\n",
      "Epoch: 1242, Training Loss: 0.100113, Validation Loss: 0.089629\n",
      "Epoch: 1243, Training Loss: 0.100016, Validation Loss: 0.088627\n",
      "Epoch: 1244, Training Loss: 0.099907, Validation Loss: 0.089046\n",
      "Epoch: 1245, Training Loss: 0.099784, Validation Loss: 0.088299\n",
      "Epoch: 1246, Training Loss: 0.099695, Validation Loss: 0.088404\n",
      "Epoch: 1247, Training Loss: 0.099596, Validation Loss: 0.087632\n",
      "Epoch: 1248, Training Loss: 0.099522, Validation Loss: 0.088115\n",
      "Epoch: 1249, Training Loss: 0.099429, Validation Loss: 0.086955\n",
      "Epoch: 1250, Training Loss: 0.099361, Validation Loss: 0.087840\n",
      "Epoch: 1251, Training Loss: 0.099216, Validation Loss: 0.086516\n",
      "Epoch: 1252, Training Loss: 0.099101, Validation Loss: 0.087059\n",
      "Epoch: 1253, Training Loss: 0.098947, Validation Loss: 0.086235\n",
      "Epoch: 1254, Training Loss: 0.098898, Validation Loss: 0.085568\n",
      "Epoch: 1255, Training Loss: 0.098964, Validation Loss: 0.087045\n",
      "Epoch: 1256, Training Loss: 0.098870, Validation Loss: 0.084811\n",
      "Epoch: 1257, Training Loss: 0.098755, Validation Loss: 0.086392\n",
      "Epoch: 1258, Training Loss: 0.098550, Validation Loss: 0.084282\n",
      "Epoch: 1259, Training Loss: 0.098341, Validation Loss: 0.085261\n",
      "Epoch: 1260, Training Loss: 0.098241, Validation Loss: 0.084039\n",
      "Epoch: 1261, Training Loss: 0.098100, Validation Loss: 0.084613\n",
      "Epoch: 1262, Training Loss: 0.097989, Validation Loss: 0.083915\n",
      "Epoch: 1263, Training Loss: 0.097877, Validation Loss: 0.083396\n",
      "Epoch: 1264, Training Loss: 0.097765, Validation Loss: 0.083483\n",
      "Epoch: 1265, Training Loss: 0.097685, Validation Loss: 0.082945\n",
      "Epoch: 1266, Training Loss: 0.097548, Validation Loss: 0.082875\n",
      "Epoch: 1267, Training Loss: 0.097450, Validation Loss: 0.082238\n",
      "Epoch: 1268, Training Loss: 0.097396, Validation Loss: 0.083208\n",
      "Epoch: 1269, Training Loss: 0.097219, Validation Loss: 0.081733\n",
      "Epoch: 1270, Training Loss: 0.097140, Validation Loss: 0.081637\n",
      "Epoch: 1271, Training Loss: 0.096971, Validation Loss: 0.081156\n",
      "Epoch: 1272, Training Loss: 0.096852, Validation Loss: 0.081202\n",
      "Epoch: 1273, Training Loss: 0.096764, Validation Loss: 0.080704\n",
      "Epoch: 1274, Training Loss: 0.096627, Validation Loss: 0.081075\n",
      "Epoch: 1275, Training Loss: 0.096552, Validation Loss: 0.079937\n",
      "Epoch: 1276, Training Loss: 0.096646, Validation Loss: 0.081405\n",
      "Epoch: 1277, Training Loss: 0.097324, Validation Loss: 0.078269\n",
      "Epoch: 1278, Training Loss: 0.098621, Validation Loss: 0.086775\n",
      "Epoch: 1279, Training Loss: 0.097226, Validation Loss: 0.079317\n",
      "Epoch: 1280, Training Loss: 0.096921, Validation Loss: 0.078157\n",
      "Epoch: 1281, Training Loss: 0.096745, Validation Loss: 0.081439\n",
      "Epoch: 1282, Training Loss: 0.098387, Validation Loss: 0.080537\n",
      "Epoch: 1283, Training Loss: 0.096814, Validation Loss: 0.080283\n",
      "Epoch: 1284, Training Loss: 0.096198, Validation Loss: 0.076905\n",
      "Epoch: 1285, Training Loss: 0.096428, Validation Loss: 0.079590\n",
      "Epoch: 1286, Training Loss: 0.096252, Validation Loss: 0.080798\n",
      "Epoch: 1287, Training Loss: 0.095844, Validation Loss: 0.075662\n",
      "Epoch: 1288, Training Loss: 0.095217, Validation Loss: 0.077006\n",
      "Epoch: 1289, Training Loss: 0.095354, Validation Loss: 0.078263\n",
      "Epoch: 1290, Training Loss: 0.095085, Validation Loss: 0.076098\n",
      "Epoch: 1291, Training Loss: 0.095132, Validation Loss: 0.076351\n",
      "Epoch: 1292, Training Loss: 0.094837, Validation Loss: 0.076114\n",
      "Epoch: 1293, Training Loss: 0.094763, Validation Loss: 0.075663\n",
      "Epoch: 1294, Training Loss: 0.094803, Validation Loss: 0.075533\n",
      "Epoch: 1295, Training Loss: 0.094638, Validation Loss: 0.074609\n",
      "Epoch: 1296, Training Loss: 0.094762, Validation Loss: 0.076837\n",
      "Epoch: 1297, Training Loss: 0.094379, Validation Loss: 0.074988\n",
      "Epoch: 1298, Training Loss: 0.094601, Validation Loss: 0.073582\n",
      "Epoch: 1299, Training Loss: 0.094609, Validation Loss: 0.076291\n",
      "Epoch: 1300, Training Loss: 0.094660, Validation Loss: 0.075010\n",
      "Epoch: 1301, Training Loss: 0.094164, Validation Loss: 0.073589\n",
      "Epoch: 1302, Training Loss: 0.094151, Validation Loss: 0.073514\n",
      "Epoch: 1303, Training Loss: 0.094248, Validation Loss: 0.074331\n",
      "Epoch: 1304, Training Loss: 0.093909, Validation Loss: 0.074312\n",
      "Epoch: 1305, Training Loss: 0.094083, Validation Loss: 0.072382\n",
      "Epoch: 1306, Training Loss: 0.093879, Validation Loss: 0.074357\n",
      "Epoch: 1307, Training Loss: 0.093849, Validation Loss: 0.073423\n",
      "Epoch: 1308, Training Loss: 0.093688, Validation Loss: 0.071873\n",
      "Epoch: 1309, Training Loss: 0.093478, Validation Loss: 0.071907\n",
      "Epoch: 1310, Training Loss: 0.093646, Validation Loss: 0.073087\n",
      "Epoch: 1311, Training Loss: 0.093372, Validation Loss: 0.072577\n",
      "Epoch: 1312, Training Loss: 0.093653, Validation Loss: 0.070875\n",
      "Epoch: 1313, Training Loss: 0.093437, Validation Loss: 0.073473\n",
      "Epoch: 1314, Training Loss: 0.093530, Validation Loss: 0.072873\n",
      "Epoch: 1315, Training Loss: 0.093219, Validation Loss: 0.071080\n",
      "Epoch: 1316, Training Loss: 0.093173, Validation Loss: 0.070201\n",
      "Epoch: 1317, Training Loss: 0.093118, Validation Loss: 0.072100\n",
      "Epoch: 1318, Training Loss: 0.092998, Validation Loss: 0.071465\n",
      "Epoch: 1319, Training Loss: 0.093177, Validation Loss: 0.069732\n",
      "Epoch: 1320, Training Loss: 0.092885, Validation Loss: 0.071649\n",
      "Epoch: 1321, Training Loss: 0.093246, Validation Loss: 0.073454\n",
      "Epoch: 1322, Training Loss: 0.094043, Validation Loss: 0.068832\n",
      "Epoch: 1323, Training Loss: 0.105172, Validation Loss: 0.087619\n",
      "Epoch: 1324, Training Loss: 0.165776, Validation Loss: 0.136849\n",
      "Epoch: 1325, Training Loss: 0.219057, Validation Loss: 0.222386\n",
      "Epoch: 1326, Training Loss: 0.102721, Validation Loss: 0.083362\n",
      "Epoch: 1327, Training Loss: 0.148989, Validation Loss: 0.115786\n",
      "Epoch: 1328, Training Loss: 0.224618, Validation Loss: 0.213594\n",
      "Epoch: 1329, Training Loss: 0.100829, Validation Loss: 0.075030\n",
      "Epoch: 1330, Training Loss: 0.152433, Validation Loss: 0.124287\n",
      "Epoch: 1331, Training Loss: 0.219554, Validation Loss: 0.213433\n",
      "Epoch: 1332, Training Loss: 0.095595, Validation Loss: 0.070146\n",
      "Epoch: 1333, Training Loss: 0.175404, Validation Loss: 0.142602\n",
      "Epoch: 1334, Training Loss: 0.220540, Validation Loss: 0.216259\n",
      "Epoch: 1335, Training Loss: 0.094321, Validation Loss: 0.078376\n",
      "Epoch: 1336, Training Loss: 0.216698, Validation Loss: 0.184695\n",
      "Epoch: 1337, Training Loss: 0.225618, Validation Loss: 0.216000\n",
      "Epoch: 1338, Training Loss: 0.102730, Validation Loss: 0.082413\n",
      "Epoch: 1339, Training Loss: 0.272686, Validation Loss: 0.237799\n",
      "Epoch: 1340, Training Loss: 0.216797, Validation Loss: 0.211385\n",
      "Epoch: 1341, Training Loss: 0.127382, Validation Loss: 0.114698\n",
      "Epoch: 1342, Training Loss: 0.297328, Validation Loss: 0.259467\n",
      "Epoch: 1343, Training Loss: 0.164897, Validation Loss: 0.151910\n",
      "Epoch: 1344, Training Loss: 0.158052, Validation Loss: 0.147695\n",
      "Epoch: 1345, Training Loss: 0.233929, Validation Loss: 0.201592\n",
      "Epoch: 1346, Training Loss: 0.099582, Validation Loss: 0.081948\n",
      "Epoch: 1347, Training Loss: 0.166597, Validation Loss: 0.153275\n",
      "Epoch: 1348, Training Loss: 0.138094, Validation Loss: 0.105976\n"
     ]
    }
   ],
   "source": [
    "gratingCouplerNet = Network()\n",
    "optimizer = torch.optim.Adam(gratingCouplerNet.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "epoch = 0\n",
    "loss = 100\n",
    "validation_loss = 100\n",
    "\n",
    "while loss > 0.05:\n",
    "    prediction = gratingCouplerNet(X_normed)\n",
    "    loss = loss_function(prediction, y.float())\n",
    "    test_prediction = gratingCouplerNet(X_test_normed)\n",
    "    validation_loss = loss_function(test_prediction, y_test.float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch += 1\n",
    "    print(\"Epoch: {}, Training Loss: {:0.6f}, Validation Loss: {:0.6f}\".format(epoch, loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6576, -6.3010, -1.2430])"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gratingCouplerNet(X_test_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0402, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(prediction, y_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8148, -2.2623], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8125, -1.7109])"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01945726046799541"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, y_test[0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005269782584722879"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, gratingCouplerNet(X_test_normed[0]).detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005151365999223099"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, y[0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008357428100215211"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.power(10, gratingCouplerNet(X_normed[0]).detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5d7e701477744fca7f01e4ac4ce7a00f852190f16ed8dbc24620b3de422c735"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
